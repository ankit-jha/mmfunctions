{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real life data\n",
    "\n",
    "import logging\n",
    "import threading\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import seaborn as seabornInstance\n",
    "from sqlalchemy import Column, Integer, String, Float, DateTime, Boolean, func\n",
    "from iotfunctions import base\n",
    "from iotfunctions import bif\n",
    "from iotfunctions import entity\n",
    "from iotfunctions import metadata\n",
    "from iotfunctions.metadata import EntityType\n",
    "from iotfunctions.db import Database\n",
    "from iotfunctions.enginelog import EngineLogging\n",
    "from iotfunctions import estimator\n",
    "from iotfunctions.ui import (UISingle, UIMultiItem, UIFunctionOutSingle,\n",
    "                 UISingleItem, UIFunctionOutMulti, UIMulti, UIExpression,\n",
    "                 UIText, UIStatusFlag, UIParameters)\n",
    "from mmfunctions.anomaly import (SaliencybasedGeneralizedAnomalyScore, SpectralAnomalyScore,\n",
    "                 FFTbasedGeneralizedAnomalyScore, KMeansAnomalyScore)\n",
    "import datetime as dt\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, r2_score\n",
    "\n",
    "import scipy as sp\n",
    "import scipy.fftpack\n",
    "import skimage as ski\n",
    "\n",
    "from skimage import util as skiutil # for nifty windowing\n",
    "import pyod as pyod\n",
    "from pyod.utils.data import generate_data\n",
    "from pyod.utils.data import evaluate_print\n",
    "from pyod.utils.example import visualize\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.iforest import IForest\n",
    "%matplotlib inline\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "EngineLogging.configure_console_logging(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a 2-layered LSTM in Watson Machine Learning\n",
    "\n",
    " \n",
    "Telemanom ([Detecting Spacecraft Anomalies Using LSTMs and Nonparametric Dynamic Thresholding](https://arxiv.org/pdf/1802.04431.pdf)\n",
    "\n",
    "\n",
    "Let's find out what ML libraries are supported in WML.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------  --------------------------  ------------------------  --------\n",
      "GUID                        NAME                        CREATED                   PLATFORM\n",
      "do_12.10                    do_12.10                    2020-03-20T04:19:17.471Z  do\n",
      "xgboost_0.90-py3.6          xgboost_0.90-py3.6          2020-03-20T04:19:03.205Z  python\n",
      "scikit-learn_0.22-py3.6     scikit-learn_0.22-py3.6     2020-03-20T04:18:53.589Z  python\n",
      "spark-mllib_2.4             spark-mllib_2.4             2020-02-06T09:30:35.538Z  spark\n",
      "tensorflow_1.15-py3.6       tensorflow_1.15-py3.6       2020-02-06T09:30:30.574Z  python\n",
      "pytorch-onnx_1.2-py3.6      pytorch-onnx_1.2-py3.6      2020-02-06T09:29:58.456Z  python\n",
      "pytorch-onnx_1.2-py3.6-edt  pytorch-onnx_1.2-py3.6-edt  2020-02-06T09:29:54.031Z  python\n",
      "tensorflow_1.14-py3.6       tensorflow_1.14-py3.6       2019-10-23T09:54:32.847Z  python\n",
      "pytorch-onnx_1.1-py3.6      pytorch-onnx_1.1-py3.6      2019-10-23T09:54:02.251Z  python\n",
      "pytorch-onnx_1.1-py3.6-edt  pytorch-onnx_1.1-py3.6-edt  2019-10-23T09:53:58.775Z  python\n",
      "pytorch_1.1-py3.6           pytorch_1.1-py3.6           2019-10-23T09:53:56.218Z  python\n",
      "pytorch_1.1-py3             pytorch_1.1-py3             2019-10-23T09:53:53.712Z  python\n",
      "xgboost_0.82-py3.6          xgboost_0.82-py3.6          2019-08-02T06:02:55.531Z  python\n",
      "xgboost_0.80-py3.6          xgboost_0.80-py3.6          2019-08-02T06:02:52.157Z  python\n",
      "scikit-learn_0.20-py3.6     scikit-learn_0.20-py3.6     2019-08-02T06:02:45.108Z  python\n",
      "scikit-learn_0.19-py3.6     scikit-learn_0.19-py3.6     2019-08-02T06:02:41.790Z  python\n",
      "tensorflow_1.13-py3.6       tensorflow_1.13-py3.6       2019-08-02T06:02:34.144Z  python\n",
      "tensorflow_1.11-py3.6       tensorflow_1.11-py3.6       2019-08-02T06:02:31.678Z  python\n",
      "tensorflow_1.5-py3.6        tensorflow_1.5-py3.6        2019-08-02T06:02:12.855Z  python\n",
      "ai-function_0.1-py3.6       ai-function_0.1-py3.6       2019-07-05T12:02:38.754Z  python\n",
      "xgboost_0.82-py3            xgboost_0.82-py3            2019-07-05T12:02:33.447Z  python\n",
      "spss-modeler_18.2           spss-modeler_18.2           2019-07-05T12:02:29.092Z  spss\n",
      "scikit-learn_0.20-py3       scikit-learn_0.20-py3       2019-07-05T12:02:24.736Z  python\n",
      "pmml_4.3                    pmml_4.3                    2019-04-15T09:59:34.202Z  pmml\n",
      "pmml_4.2                    pmml_4.2                    2019-04-15T09:59:30.855Z  pmml\n",
      "pmml_4.1                    pmml_4.1                    2019-04-15T09:59:28.401Z  pmml\n",
      "pmml_4.0                    pmml_4.0                    2019-04-15T09:59:25.948Z  pmml\n",
      "pmml_3.2                    pmml_3.2                    2019-04-15T09:59:23.480Z  pmml\n",
      "pmml_3.1                    pmml_3.1                    2019-04-15T09:59:20.942Z  pmml\n",
      "pmml_3.0                    pmml_3.0                    2019-04-15T09:59:18.466Z  pmml\n",
      "do_12.9                     do_12.9                     2019-04-04T10:55:24.375Z  do\n",
      "pmml_4.2.1                  pmml_4.2.1                  2019-04-04T10:55:21.561Z  pmml\n",
      "ai-function_0.1-py3         ai-function_0.1-py3         2019-04-04T10:55:18.663Z  python\n",
      "hybrid_0.2                  hybrid_0.2                  2019-04-04T10:55:15.872Z  hybrid\n",
      "hybrid_0.1                  hybrid_0.1                  2019-04-04T10:55:13.073Z  hybrid\n",
      "xgboost_0.80-py3            xgboost_0.80-py3            2019-04-04T10:55:10.280Z  python\n",
      "xgboost_0.6-py3             xgboost_0.6-py3             2019-04-04T10:55:07.493Z  python\n",
      "spss-modeler_18.1           spss-modeler_18.1           2019-04-04T10:55:04.706Z  spss\n",
      "spss-modeler_17.1           spss-modeler_17.1           2019-04-04T10:55:01.919Z  spss\n",
      "scikit-learn_0.19-py3       scikit-learn_0.19-py3       2019-04-04T10:54:59.060Z  python\n",
      "scikit-learn_0.17-py3       scikit-learn_0.17-py3       2019-04-04T10:54:56.269Z  python\n",
      "spark-mllib_2.3             spark-mllib_2.3             2019-04-04T10:54:53.422Z  spark\n",
      "spark-mllib_2.2             spark-mllib_2.2             2019-04-04T10:54:50.637Z  spark\n",
      "spark-mllib_2.1             spark-mllib_2.1             2019-04-04T10:54:47.851Z  spark\n",
      "tensorflow_1.13-py3         tensorflow_1.13-py3         2019-04-04T10:54:45.039Z  python\n",
      "tensorflow_1.13-py2         tensorflow_1.13-py2         2019-04-04T10:54:42.256Z  python\n",
      "tensorflow_0.11-horovod     tensorflow_0.11-horovod     2019-04-04T10:54:39.420Z  native\n",
      "tensorflow_1.11-py3         tensorflow_1.11-py3         2019-04-04T10:54:36.622Z  python\n",
      "tensorflow_1.10-py3         tensorflow_1.10-py3         2019-04-04T10:54:33.741Z  python\n",
      "tensorflow_1.10-py2         tensorflow_1.10-py2         2019-04-04T10:54:30.915Z  python\n",
      "tensorflow_1.9-py3          tensorflow_1.9-py3          2019-04-04T10:54:27.968Z  python\n",
      "tensorflow_1.9-py2          tensorflow_1.9-py2          2019-04-04T10:54:25.170Z  python\n",
      "tensorflow_1.8-py3          tensorflow_1.8-py3          2019-04-04T10:54:22.330Z  python\n",
      "tensorflow_1.8-py2          tensorflow_1.8-py2          2019-04-04T10:54:19.534Z  python\n",
      "tensorflow_1.7-py3          tensorflow_1.7-py3          2019-04-04T10:54:16.741Z  python\n",
      "tensorflow_1.7-py2          tensorflow_1.7-py2          2019-04-04T10:54:13.963Z  python\n",
      "tensorflow_1.6-py3          tensorflow_1.6-py3          2019-04-04T10:54:11.163Z  python\n",
      "tensorflow_1.6-py2          tensorflow_1.6-py2          2019-04-04T10:54:08.371Z  python\n",
      "tensorflow_1.5-py2-ddl      tensorflow_1.5-py2-ddl      2019-04-04T10:54:05.592Z  python\n",
      "tensorflow_1.5-py3-horovod  tensorflow_1.5-py3-horovod  2019-04-04T10:54:02.749Z  python\n",
      "tensorflow_1.5-py3          tensorflow_1.5-py3          2019-04-04T10:53:59.968Z  python\n",
      "tensorflow_1.5-py2          tensorflow_1.5-py2          2019-04-04T10:53:57.099Z  python\n",
      "tensorflow_1.4-py2-ddl      tensorflow_1.4-py2-ddl      2019-04-04T10:53:54.318Z  python\n",
      "tensorflow_1.4-py3-horovod  tensorflow_1.4-py3-horovod  2019-04-04T10:53:51.540Z  python\n",
      "tensorflow_1.4-py3          tensorflow_1.4-py3          2019-04-04T10:53:48.740Z  python\n",
      "tensorflow_1.4-py2          tensorflow_1.4-py2          2019-04-04T10:53:45.962Z  python\n",
      "tensorflow_1.3-py2-ddl      tensorflow_1.3-py2-ddl      2019-04-04T10:53:43.176Z  python\n",
      "tensorflow_1.3-py3          tensorflow_1.3-py3          2019-04-04T10:53:40.401Z  python\n",
      "tensorflow_1.3-py2          tensorflow_1.3-py2          2019-04-04T10:53:37.610Z  python\n",
      "tensorflow_1.2-py3          tensorflow_1.2-py3          2019-04-04T10:53:34.841Z  python\n",
      "tensorflow_1.2-py2          tensorflow_1.2-py2          2019-04-04T10:53:32.032Z  python\n",
      "pytorch-onnx_1.0-py3        pytorch-onnx_1.0-py3        2019-04-04T10:53:29.246Z  python\n",
      "pytorch_1.0-py3-edt         pytorch_1.0-py3-edt         2019-04-04T10:53:26.273Z  python\n",
      "pytorch_1.0-py2-edt         pytorch_1.0-py2-edt         2019-04-04T10:53:23.495Z  python\n",
      "pytorch_1.0-py3             pytorch_1.0-py3             2019-04-04T10:53:20.707Z  python\n",
      "pytorch_1.0-py2             pytorch_1.0-py2             2019-04-04T10:53:17.933Z  python\n",
      "pytorch_0.4-py3-horovod     pytorch_0.4-py3-horovod     2019-04-04T10:53:15.163Z  python\n",
      "pytorch_0.4-py3             pytorch_0.4-py3             2019-04-04T10:53:12.392Z  python\n",
      "pytorch_0.4-py2             pytorch_0.4-py2             2019-04-04T10:53:09.623Z  python\n",
      "pytorch_0.3-py3             pytorch_0.3-py3             2019-04-04T10:53:06.785Z  python\n",
      "pytorch_0.3-py2             pytorch_0.3-py2             2019-04-04T10:53:04.005Z  python\n",
      "torch_lua52                 torch_lua52                 2019-04-04T10:53:01.226Z  lua\n",
      "torch_luajit                torch_luajit                2019-04-04T10:52:58.465Z  lua\n",
      "caffe-ibm_1.0-py3           caffe-ibm_1.0-py3           2019-04-04T10:52:55.622Z  python\n",
      "caffe-ibm_1.0-py2           caffe-ibm_1.0-py2           2019-04-04T10:52:52.862Z  python\n",
      "caffe_1.0-py3               caffe_1.0-py3               2019-04-04T10:52:50.101Z  python\n",
      "caffe_1.0-py2               caffe_1.0-py2               2019-04-04T10:52:47.261Z  python\n",
      "caffe_frcnn                 caffe_frcnn                 2019-04-04T10:52:44.492Z  Python\n",
      "caffe_1.0-ddl               caffe_1.0-ddl               2019-04-04T10:52:41.727Z  native\n",
      "caffe2_0.8                  caffe2_0.8                  2019-04-04T10:52:38.971Z  Python\n",
      "darknet_0                   darknet_0                   2019-04-04T10:52:36.028Z  native\n",
      "theano_1.0                  theano_1.0                  2019-04-04T10:52:33.192Z  Python\n",
      "mxnet_1.2-py2               mxnet_1.2-py2               2019-04-04T10:52:30.427Z  python\n",
      "mxnet_1.1-py2               mxnet_1.1-py2               2019-04-04T10:52:27.598Z  python\n",
      "--------------------------  --------------------------  ------------------------  --------\n"
     ]
    }
   ],
   "source": [
    "# make sure to downgrade to sklearn 0.22.2 (no >= 0.23)\n",
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n",
    "\n",
    "with open('credentials_wml.json', encoding='utf-8') as F:\n",
    "    wml_credentials = json.loads(F.read())\n",
    "    \n",
    "with open('credentials_cos.json', encoding='utf-8') as F:\n",
    "    cos_credentials = json.loads(F.read())\n",
    "\n",
    "wml_url=wml_credentials['url']\n",
    "wml_instance_id=wml_credentials['instance_id']\n",
    "wml_apikey=wml_credentials['apikey']\n",
    "\n",
    "wml_data_source_type= 's3'\n",
    "\n",
    "\n",
    "cos_endpoint = cos_credentials['endpoints']\n",
    "cos_apikey = cos_credentials['apikey']\n",
    "cos_access_key = cos_credentials['cos_hmac_keys']['access_key_id']\n",
    "cos_secret_key = cos_credentials['cos_hmac_keys']['secret_access_key']\n",
    "# 'https://s3.eu.cloud-object-storage.appdomain.cloud'\n",
    "\n",
    "cos_input_bucket = 'githubanalyzer-donotdelete-pr-b9xa3kxotzh5in'\n",
    "cos_output_bucket = 'githubanalyzer-donotdelete-pr-b9xa3kxotzh5in'\n",
    "\n",
    "client = WatsonMachineLearningAPIClient(wml_credentials)\n",
    "rep_list = client.runtimes.list(limit=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First step \n",
    "\n",
    "Apparently Keras is missing out. However, Telemanom is built on Keras, so we have to port it to either Tensorflow, Pytorch, Mxnet, Caffe or Theano.\n",
    "\n",
    "I opted for Pytorch for skill building purposes and ported Telemanom to Pytorch.\n",
    "\n",
    "\n",
    "\n",
    "<small>\n",
    "    \n",
    "```\n",
    "    \n",
    "class LSTM_2L(nn.Module):\n",
    "    def __init__(self, n_features = 1, hidden_dims = [80,80], seq_length = 250,\n",
    "                 batch_size = 64, n_predictions = 10, dropout = 0.3):\n",
    "        super(LSTM_2L, self).__init__()\n",
    "        print ('LSTM_2L', n_features, hidden_dims, seq_length, batch_size, n_predictions, dropout)\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.seq_length = seq_length\n",
    "        self.num_layers = len(self.hidden_dims)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size = self.n_features,\n",
    "            hidden_size = self.hidden_dims[0],\n",
    "            batch_first = True,\n",
    "            dropout = dropout,\n",
    "            num_layers = 2)\n",
    "\n",
    "        self.linear = nn.Linear(self.hidden_dims[1], n_predictions)\n",
    "        self.init_hidden_state()\n",
    "        \n",
    "    def init_hidden_state(self):\n",
    "\n",
    "        self.hidden = (\n",
    "            torch.randn(self.num_layers, self.batch_size, self.hidden_dims[0]), #.to(self.device),\n",
    "            torch.randn(self.num_layers, self.batch_size, self.hidden_dims[0]), #.to(self.device),\n",
    "            )\n",
    "\n",
    "    def forward(self, sequences):\n",
    "\n",
    "        batch_size, seq_len, n_features = sequences.size()  # batch first\n",
    "\n",
    "        lstm1_out , (h1_n, c1_n) = self.lstm1(sequences, (self.hidden[0], self.hidden[1]))\n",
    "\n",
    "        last_time_step = lstm1_out[:,-1,:]\n",
    "\n",
    "        y_pred = self.linear(last_time_step)\n",
    "\n",
    "        return y_pred\n",
    " ```\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of mmfunctions\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import telemanom\n",
    "from telemanom.helpers import Config\n",
    "from telemanom.errors import Errors\n",
    "import telemanom.helpers as helpers\n",
    "from telemanom.channel import Channel\n",
    "from telemanom.modeling import Model\n",
    "\n",
    "conf = Config(\"./telemanom/config.yaml\")\n",
    "\n",
    "conf.dictionary['l_s'] = 250\n",
    "conf.dictionary['epochs'] = 80\n",
    "conf.dictionary['dropout'] = 0.2\n",
    "conf.batch_size = 512\n",
    "conf.l_s = 250\n",
    "conf.epochs = 80    # max\n",
    "conf.dropout = 0.2\n",
    "conf.lstm_batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Channel:Channel\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Define structure for local data\n",
    "#              telemanom supports multiple channels to reflect spacecraft sensors, we only need a single one now\n",
    "#\n",
    "device=\"Armstarknew\"\n",
    "chan = Channel(conf, device)\n",
    "helpers.make_dirs(conf.use_id, conf, \"./telemanom\")\n",
    "print(chan)\n",
    "conf\n",
    "\n",
    "# load data\n",
    "\n",
    "chan.train = np.loadtxt('./telemanom/wml_train.csv')\n",
    "chan.test = np.loadtxt('./telemanom/wml_test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following steps replay the code in wml_telemanom.py\n",
    "\n",
    "We jump over the next few cells unless we want to initiate a local training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-27 12:28:39,117 - telemanom - INFO - FFT channel: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-27T12:28:39.117 INFO telemanom.shape_data FFT channel: False\n",
      "(129300, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-27 12:28:39,563 - telemanom - INFO - FFT channel: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-27T12:28:39.563 INFO telemanom.shape_data FFT channel: False\n",
      "(129195, 2)\n"
     ]
    }
   ],
   "source": [
    "# producing overlapping windows of length 260 for lookback (250) and prediction (10)\n",
    "chan.shape_data(chan.train, train=True)\n",
    "chan.shape_data(chan.test, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_2L 2 [80, 80] 250 64 10 0.2\n",
      "init hidden state\n",
      "Hidden dimension 0:  2 64 80\n",
      "Hidden dimension 1:  2 64 80\n",
      "input shape:  (None, 2)\n"
     ]
    }
   ],
   "source": [
    "# init the Python double stacked LSTM model\n",
    "model = Model(conf, conf.use_id, chan, \"./telemanom\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "try:\n",
    "    model.model.load_state_dict(torch.load(\"./mytrainedpytorchmodel\"))\n",
    "    model.model.eval()\n",
    "except Exception:\n",
    "    # drink a coffee - training takes roughly 30 minutes\n",
    "    model.train_new(chan)\n",
    "    torch.save(model.model.state_dict(), \"./mytrainedpytorchmodel\")\n",
    "\n",
    "# no training run - we've already spent CPU cycles last week\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of LSTM_2L(\n",
       "  (lstm1): LSTM(2, 80, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (linear): Linear(in_features=80, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<strong>Make sure you have uploaded the code in mmfunctions/telemanom as zip file to COS bucket</strong><br/>githubanalyzer-donotdelete-pr-b9xa3kxotzh5in"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "Markdown('<strong>{}</strong><br/>{}'.format('Make sure you have uploaded the code in mmfunctions/telemanom as zip file to COS bucket', cos_input_bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<strong>./telemanom/wml_model.zip\n",
       "found - good</strong><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zip the code in the ./telemanom subdirectory first\n",
    "\n",
    "import subprocess\n",
    "output = None\n",
    "try:\n",
    "    output = subprocess.check_output(\"ls ./telemanom/wml_model.zip\", shell=True).decode('ascii')  + 'found - good'\n",
    "except Exception:\n",
    "    output = 'Not found - do it now'\n",
    "\n",
    "Markdown('<strong>{}</strong><br/>'.format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          DIR  s3://githubanalyzer-donotdelete-pr-b9xa3kxotzh5in/_wml_checkpoints/\r\n",
      "                          DIR  s3://githubanalyzer-donotdelete-pr-b9xa3kxotzh5in/bf25a013-c8e9-4501-a8af-bd5a3bbc22a6/\r\n",
      "                          DIR  s3://githubanalyzer-donotdelete-pr-b9xa3kxotzh5in/notebook/\r\n",
      "                          DIR  s3://githubanalyzer-donotdelete-pr-b9xa3kxotzh5in/training-h9VOfZVMR/\r\n",
      "2020-07-24 16:54      2661699  s3://githubanalyzer-donotdelete-pr-b9xa3kxotzh5in/wml_model.zip\r\n"
     ]
    }
   ],
   "source": [
    "# check whether we have uploaded the code\n",
    "!s3cmd --access_key {cos_access_key} --secret_key {cos_secret_key} \\\n",
    "--access_token {cos_apikey} --host s3.eu.cloud-object-storage.appdomain.cloud --host-bucket=s3.eu.cloud-object-storage.appdomain.cloud \\\n",
    "ls s3://githubanalyzer-donotdelete-pr-b9xa3kxotzh5in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wml_train_code='./telemanom/wml_model.zip' # where this notebook finds the code\n",
    "\n",
    "wml_execution_command='python3 wml_telemanom.py' # command to start training\n",
    "\n",
    "wml_framework_name='pytorch'\n",
    "wml_framework_version='1.1' # we run on pytorch 1.1\n",
    "wml_runtime = 'python'\n",
    "wml_runtime_version='3.6' # and python 3.6\n",
    "\n",
    "wml_run_definition = 'wml-pytorch-definition' # dummy name\n",
    "wml_run_name = 'wml-pytorch-run' # more dummy\n",
    "wml_model_name='wml-tensorflow-miregal' # even more dummy\n",
    "\n",
    "wml_compute_name='k80'  # free tier machine type\n",
    "wml_compute_nodes='1'   # free tier\n",
    "\n",
    "wml_runtime_version_v4 = wml_framework_version + '-py' + wml_runtime_version   # sdk level\n",
    "wml_compute_nodes_v4 = int(wml_compute_nodes)\n",
    "\n",
    "model_code = wml_train_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./telemanom/wml_model.zip'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wml_train_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# define library meta data for our training code\n",
    "#\n",
    "lib_meta = {\n",
    "    client.runtimes.LibraryMetaNames.NAME: wml_run_definition,\n",
    "    client.runtimes.LibraryMetaNames.VERSION: wml_framework_version,\n",
    "    client.runtimes.LibraryMetaNames.FILEPATH: model_code,\n",
    "    client.runtimes.LibraryMetaNames.PLATFORM: {\"name\": wml_framework_name, \"versions\": [wml_framework_version]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# do we have a library with that name defined ?\n",
    "#   delete it first and then store the new updated library\n",
    "#\n",
    "library_details = client.runtimes.get_library_details()\n",
    "for library_detail in library_details['resources']:\n",
    "    if library_detail['entity']['name'] == wml_run_definition:\n",
    "        # Delete library if exist because we cannot update model_code\n",
    "        uid = client.runtimes.get_library_uid(library_detail)\n",
    "        print ('delete ', library_detail)\n",
    "        client.repository.delete(uid)\n",
    "        break\n",
    "\n",
    "custom_library_details = client.runtimes.store_library(lib_meta)\n",
    "custom_library_uid = client.runtimes.get_library_uid(custom_library_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# define a pipeline with a single entry (node) for the training run\n",
    "#  we could add more node for scaling/normalizing, imputation, feature extraction, \"you name it\"\n",
    "#\n",
    "doc = {\n",
    "    \"doc_type\": \"pipeline\",\n",
    "    \"version\": \"2.0\",\n",
    "    \"primary_pipeline\": wml_framework_name,\n",
    "    \"pipelines\": [{\n",
    "        \"id\": wml_framework_name,\n",
    "        \"runtime_ref\": \"hybrid\",\n",
    "        \"nodes\": [{\n",
    "            \"id\": \"training\",\n",
    "            \"type\": \"model_node\",\n",
    "            \"op\": \"dl_train\",\n",
    "            \"runtime_ref\": wml_run_name,\n",
    "            \"inputs\": [],\n",
    "            \"outputs\": [],\n",
    "            \"parameters\": {\n",
    "                \"name\": \"tf-mnist\",\n",
    "                \"description\": wml_run_definition,\n",
    "                \"command\": wml_execution_command,\n",
    "                \"training_lib_href\": \"/v4/libraries/\"+custom_library_uid,\n",
    "                \"compute\": {\n",
    "                    \"name\": wml_compute_name,            # specify where to run it (not that I have a choice)\n",
    "                    \"nodes\": wml_compute_nodes_v4\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "    }],\n",
    "    \"runtimes\": [{\n",
    "        \"id\": wml_run_name,\n",
    "        \"name\": wml_framework_name,         # run it on a pytorch image\n",
    "        \"version\": wml_runtime_version_v4\n",
    "    }]\n",
    "}\n",
    "\n",
    "# put it in metadata object\n",
    "metadata = {\n",
    "    client.repository.PipelineMetaNames.NAME: wml_run_name,\n",
    "    client.repository.PipelineMetaNames.DOCUMENT: doc\n",
    "}\n",
    "\n",
    "# and create the pipeline\n",
    "pipeline_id = client.pipelines.get_uid(client.repository.store_pipeline(meta_props=metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metadata': {'name': 'wml-pytorch-run',\n",
       "  'guid': '871b686e-b077-4005-a0ee-28e87f3facab',\n",
       "  'rev': '00d0f17c-cb83-4401-9d66-585c106562e0',\n",
       "  'id': '871b686e-b077-4005-a0ee-28e87f3facab',\n",
       "  'modified_at': '2020-07-24T16:37:37.663Z',\n",
       "  'created_at': '2020-07-24T16:37:37.596Z',\n",
       "  'href': '/v4/pipelines/871b686e-b077-4005-a0ee-28e87f3facab?rev=00d0f17c-cb83-4401-9d66-585c106562e0'},\n",
       " 'entity': {'space': {'id': '88740b60-6b2f-4f74-b6d8-20528d14db8b',\n",
       "   'href': '/v4/spaces/88740b60-6b2f-4f74-b6d8-20528d14db8b'},\n",
       "  'name': 'wml-pytorch-run',\n",
       "  'document': {'doc_type': 'pipeline',\n",
       "   'version': '2.0',\n",
       "   'pipelines': [{'id': 'pytorch',\n",
       "     'runtime_ref': 'hybrid',\n",
       "     'nodes': [{'outputs': [],\n",
       "       'id': 'training',\n",
       "       'inputs': [],\n",
       "       'type': 'model_node',\n",
       "       'parameters': {'name': 'tf-mnist',\n",
       "        'description': 'wml-pytorch-definition',\n",
       "        'compute': {'name': 'k80', 'nodes': 1},\n",
       "        'command': 'python3 wml_telemanom.py',\n",
       "        'training_lib_href': '/v4/libraries/a38ad1e9-3d2b-45c6-a596-340170884653'},\n",
       "       'runtime_ref': 'wml-pytorch-run',\n",
       "       'op': 'dl_train'}]}],\n",
       "   'runtimes': [{'id': 'wml-pytorch-run',\n",
       "     'name': 'pytorch',\n",
       "     'version': '1.1-py3.6'}],\n",
       "   'primary_pipeline': 'pytorch'}}}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is my pipeline now\n",
    "client.pipelines.get_details(pipeline_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_id {'metadata': {'created_at': '2020-07-24T16:55:55.904Z', 'guid': 'bf25a013-c8e9-4501-a8af-bd5a3bbc22a6', 'href': '/v4/trainings/bf25a013-c8e9-4501-a8af-bd5a3bbc22a6', 'id': 'bf25a013-c8e9-4501-a8af-bd5a3bbc22a6', 'space_id': '88740b60-6b2f-4f74-b6d8-20528d14db8b'}, 'entity': {'pipeline': {'href': '/v4/pipelines/871b686e-b077-4005-a0ee-28e87f3facab', 'id': '871b686e-b077-4005-a0ee-28e87f3facab'}, 'results_reference': {'location': {'training': 'bf25a013-c8e9-4501-a8af-bd5a3bbc22a6', 'pipeline_model': 'bf25a013-c8e9-4501-a8af-bd5a3bbc22a6/pipeline-model.json', 'training_status': 'bf25a013-c8e9-4501-a8af-bd5a3bbc22a6/training-status.json', 'pipeline': 'bf25a013-c8e9-4501-a8af-bd5a3bbc22a6/pipeline.json', 'bucket': 'githubanalyzer-donotdelete-pr-b9xa3kxotzh5in', 'assets_path': 'bf25a013-c8e9-4501-a8af-bd5a3bbc22a6/assets'}, 'type': 's3', 'connection': {'access_key_id': 'cc04444c99374c9e9589b8f85e931323', 'secret_access_key': '1a5062d937b09507a05b521a41b8baf6848c0cd6936e2864', 'endpoint_url': 'https://s3.eu.cloud-object-storage.appdomain.cloud'}}, 'space': {'href': '/v4/spaces/88740b60-6b2f-4f74-b6d8-20528d14db8b', 'id': '88740b60-6b2f-4f74-b6d8-20528d14db8b'}, 'space_id': '88740b60-6b2f-4f74-b6d8-20528d14db8b', 'status': {'state': 'pending'}, 'training_data_references': [{'location': {'bucket': 'githubanalyzer-donotdelete-pr-b9xa3kxotzh5in'}, 'type': 's3', 'connection': {'access_key_id': 'cc04444c99374c9e9589b8f85e931323', 'secret_access_key': '1a5062d937b09507a05b521a41b8baf6848c0cd6936e2864', 'endpoint_url': 'https://s3.eu.cloud-object-storage.appdomain.cloud'}}]}}\n",
      "get status {'state': 'pending'}\n"
     ]
    }
   ],
   "source": [
    "# start the training run for v4\n",
    "metadata = {\n",
    "    client.training.ConfigurationMetaNames.TRAINING_RESULTS_REFERENCE: {\n",
    "        \"name\": \"training-results-reference_name\",\n",
    "        \"connection\": {\n",
    "            \"endpoint_url\": cos_endpoint,\n",
    "            \"access_key_id\": cos_access_key,\n",
    "            \"secret_access_key\": cos_secret_key\n",
    "        },\n",
    "        \"location\": {\n",
    "            \"bucket\": cos_output_bucket\n",
    "        },\n",
    "        \"type\": wml_data_source_type\n",
    "    },\n",
    "    client.training.ConfigurationMetaNames.TRAINING_DATA_REFERENCES:[{\n",
    "        \"name\": \"training_input_data\",\n",
    "        \"type\": wml_data_source_type,\n",
    "        \"connection\": {\n",
    "            \"endpoint_url\": cos_endpoint,\n",
    "            \"access_key_id\": cos_access_key,\n",
    "            \"secret_access_key\": cos_secret_key\n",
    "        },\n",
    "        \"location\": {\n",
    "            \"bucket\": cos_input_bucket\n",
    "        }\n",
    "    }],\n",
    "    client.training.ConfigurationMetaNames.PIPELINE_UID: pipeline_id\n",
    "}\n",
    "\n",
    "training_id = client.training.get_uid(client.training.run(meta_props=metadata))\n",
    "print(\"training_id\", client.training.get_details(training_id))\n",
    "print(\"get status\", client.training.get_status(training_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Log monitor started for training run: bf25a013-c8e9-4501-a8af-bd5a3bbc22a6\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "\n",
      "ERROR - Cannot find pipeline_model.json in the bucket bf25a013-c8e9-4501-a8af-bd5a3bbc22a6\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'pipeline_model' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-c3632d1f42af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# print logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_uid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_uid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/watson_machine_learning_client/training.py\u001b[0m in \u001b[0;36mmonitor_logs\u001b[0;34m(self, training_uid)\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0mTraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_uid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'training_uid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTR_TYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simple_monitor_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_uid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_text_header_h1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'Log monitor started for training run: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_uid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0mprint_text_header_h2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Log monitor done.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/watson_machine_learning_client/training.py\u001b[0m in \u001b[0;36m_simple_monitor_logs\u001b[0;34m(self, training_uid, on_start)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"completed\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"failed\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"canceled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             self._COS_logs(training_uid,\n\u001b[0m\u001b[1;32m    755\u001b[0m                            lambda: print_text_header_h1(u'Log monitor started for training run: ' + str(training_uid)))\n\u001b[1;32m    756\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/watson_machine_learning_client/training.py\u001b[0m in \u001b[0;36m_COS_logs\u001b[0;34m(self, run_uid, on_start)\u001b[0m\n\u001b[1;32m    651\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pipelines\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"nodes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"parameters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/learner-1/training-log.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient_cos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'pipeline_model' referenced before assignment"
     ]
    }
   ],
   "source": [
    "run_details = client.training.get_details(training_id)\n",
    "run_uid = training_id\n",
    "\n",
    "# print logs\n",
    "\n",
    "client.training.monitor_logs(run_uid)\n",
    "client.training.monitor_metrics(run_uid)\n",
    "\n",
    "# should not have run after restarting the notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'failure': {'trace': '328d08cef1309a8c129566b260007dab',\n",
       "  'errors': [{'code': 'dl_job_failed (C201)',\n",
       "    'message': 'Learner process crashed (C201) with exit code (1), please check the job logs for more information',\n",
       "    'more_info': 'http://watson-ml-api.mybluemix.net/'}]},\n",
       " 'message': {'text': 'Node training: 1', 'level': 'info'},\n",
       " 'running_at': '2020-07-24T16:56:26.203Z',\n",
       " 'state': 'failed'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status = client.training.get_status(run_uid)\n",
    "status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading the logs from COS\n",
    "\n",
    "Dumb me, forgot to import sys.\n",
    "\n",
    "Fortunately model training has succeeded and the model has been stored in COS. Phew.\n",
    "\n",
    "<small>\n",
    "\n",
    "```\n",
    "...\n",
    "Batch  1611\n",
    "Batch  1612\n",
    "After batch  1612 0.002384878075476655\n",
    "[1] Training loss: 0.002384878075476655 \t Validation loss: 0.0014487287297119242 \n",
    "Training complete...\n",
    "Model saved in file: /mnt/results/githubanalyzer-donotdelete-pr-b9xa3kxotzh5in/training-h9VOfZVMR/model\n",
    "['_submitted_code', 'learner-1', 'model', 'training-log.txt']\n",
    "/mnt/results/githubanalyzer-donotdelete-pr-b9xa3kxotzh5in/training-h9VOfZVMR\n",
    "Traceback (most recent call last):\n",
    "  File \"wml_telemanom.py\", line 61, in <module>\n",
    "    sys.stdout.flush()\n",
    "NameError: name 'sys' is not defined\n",
    "```\n",
    "    \n",
    "</small>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training parameters\n",
    "\n",
    "```\n",
    "loss_metric: 'mse'    # minimize mean square error\n",
    "optimizer: 'adam'     # sort of adaptive stochastic gradient descent\n",
    "validation_split: 0.2 # 20% of the data is used for validating (val_loss)\n",
    "dropout: 0.3          # ditch 30% of the LSTMs results when minimizing the loss function to avoid overfitting\n",
    "lstm_batch_size: 64   # number of training data batches to evaluate per optimizer run to update the model’s parameters\n",
    "\n",
    "patience: 10          # try at least 10 times to decrease val_loss smaller by ...\n",
    "min_delta: 0.0003     # ... at least min_delta, else stop, so we get at least 'patience' epochs\n",
    "epochs: 35            # no more than 35 passes through the entier training dataset.\n",
    "\n",
    "l_s: 250              # lookback: num previous timesteps provided to model to predict future values\n",
    "n_predictions: 10     # number of steps ahead to predict\n",
    "```\n",
    "\n",
    "This is defined in `telemanom/config.yaml`\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
