{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/.local/lib/python3.7/site-packages/iotfunctions/bif.py:1899: UserWarning: IoTCalcSettings is deprecated. Use entity type constants instead of a metadata provider to set entity type properties\n",
      "  warnings.warn(('IoTCalcSettings is deprecated. Use entity type constants'\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import threading\n",
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as seabornInstance\n",
    "from sqlalchemy import Column, Integer, String, Float, DateTime, Boolean, func\n",
    "from iotfunctions import base\n",
    "from iotfunctions import bif\n",
    "from iotfunctions import entity\n",
    "from iotfunctions import metadata\n",
    "from iotfunctions.metadata import EntityType\n",
    "from iotfunctions.db import Database\n",
    "from iotfunctions.enginelog import EngineLogging\n",
    "from iotfunctions import estimator\n",
    "from iotfunctions.ui import (UISingle, UIMultiItem, UIFunctionOutSingle,\n",
    "                 UISingleItem, UIFunctionOutMulti, UIMulti, UIExpression,\n",
    "                 UIText, UIStatusFlag, UIParameters)\n",
    "from iotfunctions.enginelog import EngineLogging\n",
    "from iotfunctions import pipeline as pp\n",
    "from iotfunctions.pipeline import Db2DataWriter, JobController, DataWriterFile, DataAggregator\n",
    "\n",
    "import datetime as dt\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-27T14:01:02.760 DEBUG iotfunctions.enginelog.configure_console_logging Console logging has been configured. Level = 10\n"
     ]
    }
   ],
   "source": [
    "credentials = {\n",
    "  \"tenantId\": \"AnalyticsServiceDev\",\n",
    "  \"as_api_host\": \"https://api-dev.connectedproducts.internetofthings.ibmcloud.com\",\n",
    "  \"as_api_key\": \"a-69xgm4-8bdgtvnsv4\",\n",
    "  \"as_api_token\": \"9X_tMKdupOiJ!mzaPV\",\n",
    "  \"config\" : {\n",
    "      \"objectStorageEndpoint\" : \"https://s3-api.us-geo.objectstorage.softlayer.net\",\n",
    "      \"bos_runtime_bucket\" : \"analytics-runtime-analyticsservicedev-799d2008b460\",\n",
    "      \"bos_logs_bucket\" : \"analytics-logs-analyticsservicedev-32703c52ec8b\"\n",
    "  },\n",
    "  \"objectStorage\": {\n",
    "      \"username\" : \"58ddd86b5de8468b819d385046f17033\",\n",
    "      \"password\" : \"ee0d6c5521ce9ff100f91b0e37d4eb8cc1a038b5a6d05b38\",\n",
    "      \"region\" : \"us\",\n",
    "      \"endpoint\" : \"https://s3-api.us-geo.objectstorage.softlayer.net\"\n",
    "  },\n",
    "  \"db2\": {\n",
    "    \"username\": \"bluadmin\",\n",
    "    \"password\": \"ZmM5MmE5NmZkZGZl\",\n",
    "    \"databaseName\": \"BLUDB\",\n",
    "    \"port\": 50000,\n",
    "    \"httpsUrl\": \"https://dashdb-enterprise-yp-dal13-74.services.dal.bluemix.net:50000\",\n",
    "    \"host\": \"dashdb-enterprise-yp-dal13-74.services.dal.bluemix.net\"\n",
    "  },\n",
    "  \"postgres\": None\n",
    "}\n",
    "EngineLogging.configure_console_logging(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-27T14:01:04.044 DEBUG iotfunctions.db.__init__ Unable to locate message_hub credentials. Database object created, but it will not be able interact with message hub.\n",
      "2019-09-27T14:01:04.045 DEBUG iotfunctions.db.__init__ created a CosClient object\n",
      "2019-09-27T14:01:06.623 DEBUG iotfunctions.db.__init__ Db connection established\n",
      "2019-09-27T14:01:06.625 DEBUG urllib3.connectionpool._new_conn Starting new HTTPS connection (1): api-dev.connectedproducts.internetofthings.ibmcloud.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/.local/lib/python3.7/site-packages/urllib3/connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-27T14:01:10.955 DEBUG urllib3.connectionpool._make_request https://api-dev.connectedproducts.internetofthings.ibmcloud.com:443 \"GET /api/meta/v1/AnalyticsServiceDev/entityType HTTP/1.1\" 200 None\n",
      "2019-09-27T14:01:11.510 DEBUG iotfunctions.db.http_request http request successful. status 200\n",
      "<iotfunctions.db.Database object at 0x7f34287397f0>\n"
     ]
    }
   ],
   "source": [
    "db_schema = None\n",
    "db = Database(credentials=credentials)\n",
    "print (db)\n",
    "# extend iotfunctions/db.py to support postgres connection string in credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/.local/lib/python3.7/site-packages/sqlalchemy/engine/reflection.py:913: SAWarning: index key 'sqlnotapplicable' was not located in columns for table 'MARKUS_TEST_ROBOT1'\n",
      "  \"columns for table '%s'\" % (flavor, c, table_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARKUS_TEST_ROBOT1\n"
     ]
    }
   ],
   "source": [
    "table = db.get_table(\"MARKUS_TEST_ROBOT1\")\n",
    "start_ts = dt.datetime.utcnow() - dt.timedelta(days=1)\n",
    "end_ts = dt.datetime.utcnow()\n",
    "df = db.read_table(table, None, None, None, \"evt_timestamp\", start_ts, end_ts)\n",
    "print (table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [deviceid, evt_timestamp, devicetype, logicalinterface_id, eventtype, format, updated_utc, plant_code, torque, tool_type, load, speed, travel_time, acc]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print (df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-26T18:52:59.912 DEBUG iotfunctions.enginelog.configure_console_logging Console logging has been configured. Level = 10\n",
      "2019-09-26T18:52:59.914 DEBUG iotfunctions.metadata.__init__ Initializing new entity type using iotfunctions 2.0.3\n",
      "2019-09-26T18:52:59.917 DEBUG iotfunctions.pipeline.__init__ Starting trace\n",
      "2019-09-26T18:52:59.919 DEBUG iotfunctions.pipeline.__init__ Trace name: auto_trace_markus_testdata_20190926165259\n",
      "2019-09-26T18:52:59.920 DEBUG iotfunctions.pipeline.__init__ auto_save None\n",
      "2019-09-26T18:52:59.923 WARNING iotfunctions.metadata.__init__ No _db_schema specified in **kwargs. Usingdefault database schema.\n",
      "2019-09-26T18:52:59.924 DEBUG iotfunctions.util.categorize_args categorizing arguments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/.local/lib/python3.7/site-packages/sqlalchemy/engine/reflection.py:913: SAWarning: index key 'sqlnotapplicable' was not located in columns for table 'markus_testdata'\n",
      "  \"columns for table '%s'\" % (flavor, c, table_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-26T18:53:02.692 DEBUG iotfunctions.metadata.__init__ Initialized entity type \n",
      "EntityType:markus_testdata\n",
      "Functions:\n",
      "EntityDataGenerator at granularity None required inputs not evaluated yet outputs produced not evaluated yet on schedule None\n",
      "Granularities:\n",
      "No schedules metadata\n",
      "2019-09-26T18:53:02.694 DEBUG iotfunctions.metadata.generate_data Generating data for markus_testdata with metrics ['TestData'] and dimensions ['eventtype'] and dates []\n",
      "2019-09-26T18:53:02.715 DEBUG iotfunctions.automation.get_data Generated 6 rows of time series data from 2019-09-26 16:48:02.695417 to 2019-09-26 16:53:02.695417\n",
      "2019-09-26T18:53:03.819 INFO iotfunctions.db.write_frame Wrote data to table markus_testdata \n"
     ]
    }
   ],
   "source": [
    "# Generate 5 mins of data in table 'testdata' with a single additional column of TestData\n",
    "EngineLogging.configure_console_logging(logging.DEBUG)\n",
    "jobsettings = {}\n",
    "#jobsettings = {'_timestamp' : 'TIMESTAMP'}\n",
    "et = metadata.EntityType('markus_testdata', db, \n",
    "                         bif.EntityDataGenerator(output_item='my_test_gen'),\n",
    "                         \n",
    "                         Column('TestData',Float()),\n",
    "                         **jobsettings)\n",
    "\n",
    "#start_date = dt.datetime.utcnow() - dt.timedelta(days=1)\n",
    "#et.exec_local_pipeline(start_ts = start_date)\n",
    "df = et.generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-27T12:07:03.800 DEBUG iotfunctions.metadata.__init__ Initializing new entity type using iotfunctions 2.0.3\n",
      "2019-09-27T12:07:03.801 DEBUG iotfunctions.pipeline.__init__ Starting trace\n",
      "2019-09-27T12:07:03.801 DEBUG iotfunctions.pipeline.__init__ Trace name: auto_trace_markus_testdata_20190927100703\n",
      "2019-09-27T12:07:03.802 DEBUG iotfunctions.pipeline.__init__ auto_save None\n",
      "2019-09-27T12:07:03.803 WARNING iotfunctions.metadata.__init__ No _db_schema specified in **kwargs. Usingdefault database schema.\n",
      "2019-09-27T12:07:03.803 DEBUG iotfunctions.util.categorize_args categorizing arguments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/.local/lib/python3.7/site-packages/sqlalchemy/engine/reflection.py:913: SAWarning: index key 'sqlnotapplicable' was not located in columns for table 'markus_testdata'\n",
      "  \"columns for table '%s'\" % (flavor, c, table_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-27T12:07:07.025 DEBUG iotfunctions.metadata.__init__ Initialized entity type \n",
      "EntityType:markus_testdata\n",
      "Functions:\n",
      "Granularities:\n",
      "No schedules metadata\n",
      "2019-09-27T12:07:07.521 DEBUG iotfunctions.metadata.index_df Indexed dataframe on id, evt_timestamp\n",
      "\n",
      "EntityType:markus_testdata\n",
      "Functions:\n",
      "Granularities:\n",
      "No schedules metadata\n"
     ]
    }
   ],
   "source": [
    "#jobsettings = {'_timestamp' : 'TIMESTAMP'}\n",
    "jobsettings = {}\n",
    "et2 = metadata.EntityType('markus_testdata', db, \n",
    "                          Column('TestData',Float()),\n",
    "                          **jobsettings)\n",
    "et2.get_data()\n",
    "print (et2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iotfunctions import pipeline as pp\n",
    "#job = pp.JobController(et)\n",
    "#job.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1b4ab341a265>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "markus_testdata\n",
      "(12, 8)\n"
     ]
    }
   ],
   "source": [
    "# read it back\n",
    "table = db.get_table(\"markus_testdata\")\n",
    "start_ts = dt.datetime.utcnow() - dt.timedelta(days=1)\n",
    "end_ts = dt.datetime.utcnow()\n",
    "df_in = db.read_table(table, None, None, None, \"evt_timestamp\", start_ts, end_ts)\n",
    "print (table)\n",
    "print (df_in.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   deviceid              evt_timestamp       devicetype logicalinterface_id  \\\n",
      "7     73004 2019-09-26 16:49:02.695417  markus_testdata                       \n",
      "8     73004 2019-09-26 16:50:02.695417  markus_testdata                       \n",
      "9     73002 2019-09-26 16:51:02.695417  markus_testdata                       \n",
      "10    73001 2019-09-26 16:52:02.695417  markus_testdata                       \n",
      "11    73000 2019-09-26 16:53:02.695417  markus_testdata                       \n",
      "\n",
      "   eventtype format updated_utc  TestData  \n",
      "7         te               None  0.372196  \n",
      "8         ne               None -0.391822  \n",
      "9         te               None -1.279792  \n",
      "10        np               None  1.143055  \n",
      "11        et               None -0.361705  \n"
     ]
    }
   ],
   "source": [
    "print (df_in.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-692bc3a810cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# check whether the data for the last 5 minutes is the same - must return True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TestData'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TestData'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# check whether the data for the last 5 minutes is the same - must return True\n",
    "print (np.array_equal(df['TestData'].tail(5), df_in['TestData'].tail(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'et' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-b25dabf634ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0met\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#del (et2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'et' is not defined"
     ]
    }
   ],
   "source": [
    "print (et._functions)\n",
    "#del (et2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-27T14:01:16.541 DEBUG iotfunctions.metadata.__init__ Initializing new entity type using iotfunctions 2.0.3\n",
      "2019-09-27T14:01:16.542 DEBUG iotfunctions.pipeline.__init__ Starting trace\n",
      "2019-09-27T14:01:16.543 DEBUG iotfunctions.pipeline.__init__ Trace name: auto_trace_markus_testdata_20190927120116\n",
      "2019-09-27T14:01:16.543 DEBUG iotfunctions.pipeline.__init__ auto_save None\n",
      "2019-09-27T14:01:16.544 WARNING iotfunctions.metadata.__init__ No _db_schema specified in **kwargs. Usingdefault database schema.\n",
      "2019-09-27T14:01:16.545 DEBUG iotfunctions.util.categorize_args categorizing arguments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/.local/lib/python3.7/site-packages/sqlalchemy/engine/reflection.py:913: SAWarning: index key 'sqlnotapplicable' was not located in columns for table 'markus_testdata'\n",
      "  \"columns for table '%s'\" % (flavor, c, table_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-27T14:01:19.783 DEBUG iotfunctions.metadata.__init__ Initialized entity type \n",
      "EntityType:markus_testdata\n",
      "Functions:\n",
      "Granularities:\n",
      "No schedules metadata\n",
      "2019-09-27T14:01:19.784 DEBUG iotfunctions.base.parse_expression expression (5*df[\"TestData\"])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "jobsettings = {}\n",
    "et2 = metadata.EntityType('markus_testdata', db, \n",
    "                          Column('TestData',Float()),\n",
    "                          **jobsettings)\n",
    "et2._functions = [bif.PythonExpression('5*df[\"TestData\"]','TestOut')]\n",
    "\n",
    "# make sure the results of the python expression is saved to the derived metrics table\n",
    "et2._data_items.append({'columnName': 'TestOut', 'columnType': 'NUMBER', 'kpiFunctionId': 22856, \n",
    "                         'kpiFunctionDto': {'output': {'name': 'TestOut'}},\n",
    "                        'name': 'TestOut', 'parentDataItemName': None, 'sourceTableName': 'dm_markus_testdata',\n",
    "                        'transient': False,'type': 'DERIVED_METRIC'})\n",
    "# map device id to entity id for the derived metrics table\n",
    "et2._data_items.append({'columnName': 'deviceid', 'columnType': 'LITERAL', 'kpiFunctionId': None,\n",
    "                         'kpiFunctionDto': {},\n",
    "                         'name': 'ENTITY_ID', 'parentDataItemName': None,'sourceTableName': 'dm_markus_testdata',\n",
    "                         'transient': False,'type': 'METRIC'})\n",
    "\n",
    "# make sure the results of the python expression is saved to the derived metrics daily table\n",
    "et2._data_items.append({'columnName': 'TestData_max', 'columnType': 'NUMBER', 'kpiFunctionId': 22856, \n",
    "                         'kpiFunctionDto': {'output': {'name': 'TestData_max'}},\n",
    "                        'name': 'TestData_max', 'parentDataItemName': None, 'sourceTableName': 'dm_markus_testdata_daily',\n",
    "                        'transient': False,'type': 'DERIVED_METRIC'})\n",
    "# map device id to entity id for the derived metrics daily table\n",
    "et2._data_items.append({'columnName': 'deviceid', 'columnType': 'LITERAL', 'kpiFunctionId': None,\n",
    "                         'kpiFunctionDto': {},\n",
    "                         'name': 'ENTITY_ID', 'parentDataItemName': None,'sourceTableName': 'dm_markus_testdata_daily',\n",
    "                         'transient': False,'type': 'METRIC'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-27T14:04:42.995 DEBUG iotfunctions.pipeline.set_payload_params Setting param writer_name on payload to <class 'iotfunctions.pipeline.Db2DataWriter'>\n",
      "2019-09-27T14:04:42.996 DEBUG iotfunctions.pipeline.set_payload_params Setting param db on payload to <iotfunctions.db.Database object at 0x7f34287397f0>\n",
      "2019-09-27T14:04:42.997 DEBUG iotfunctions.pipeline.set_payload_params Setting param _db_schema on payload to BLUADMIN\n",
      "2019-09-27T14:04:42.998 DEBUG iotfunctions.pipeline.set_payload_params Setting param save_trace_to_file on payload to True\n",
      "2019-09-27T14:04:42.999 DEBUG iotfunctions.pipeline.set_payload_params Setting param tenant_id on payload to AnalyticsServiceDev\n",
      "2019-09-27T14:04:44.291 DEBUG iotfunctions.pipeline.get_output_list The payload has candidate data items ['deviceid', 'evt_timestamp', 'TestData', 'deviceid', 'deviceid']. The DataReader has no projection list\n",
      "2019-09-27T14:04:44.292 DEBUG iotfunctions.metadata.classify_stages Input set was preset for function PythonExpression\n",
      "2019-09-27T14:04:44.293 DEBUG iotfunctions.metadata.classify_stages Output list set was preset for function PythonExpression\n",
      "2019-09-27T14:04:44.294 INFO iotfunctions.pipeline.__init__ Initialized job.\n",
      "\n",
      "2019-09-27T14:04:44.295 INFO iotfunctions.pipeline.__init__ \n",
      "Default schedule 5min \n",
      "    Schedule 5min start None:None backtracks: None \n",
      "Stages of type: get_data at grain None: \n",
      "   System generated DataReader stage: read_entity_data. Reads data from objects: markus_testdata\n",
      "Stages of type: transform at grain None: \n",
      "   PythonExpression at granularity None requires inputs {'TestData'} produces outputs ['TestOut'] on schedule 5min\n",
      "\n",
      "\n",
      "2019-09-27T14:04:44.297 INFO iotfunctions.enginelog.start_run_log Started logging into file run.log. Object Store path will be AnalyticsServiceDev/markus_testdata/20190927/120444_run.gz\n",
      "2019-09-27T14:04:44.299 DEBUG iotfunctions.pipeline.execute Starting execution number: 0 with execution date: 2019-09-27 12:04:44.297393\n",
      "2019-09-27T14:04:44.917 DEBUG iotfunctions.pipeline.get_next_execution_date Last execution of schedule 5min was 2019-09-27 12:01:28.667056. Next execution due 2019-09-27 12:06:28.667056.\n",
      "2019-09-27T14:04:44.918 DEBUG iotfunctions.pipeline.evaluate_schedules Schedule 5min will execute\n",
      "2019-09-27T14:04:44.918 DEBUG iotfunctions.pipeline.reset Started a new trace auto_trace_markus_testdata_20190927120444 \n",
      "2019-09-27T14:04:44.919 DEBUG iotfunctions.pipeline.reset Initiating auto save for trace\n",
      "2019-09-27T14:04:46.693 DEBUG iotfunctions.pipeline.insert Created job log entry (markus_testdata,5min): 2019-09-27 12:04:44.297393\n",
      "2019-09-27T14:04:46.694 DEBUG iotfunctions.pipeline.build_job_spec Building a job spec for schedule 5min with subsumbed schedules ['5min']\n",
      "2019-09-27T14:04:46.694 DEBUG iotfunctions.pipeline.build_stages_of_type Gathered stages of type get_data. Iteration 0: ['System generated DataReader stage: read_entity_data. Reads data from objects: markus_testdata']\n",
      "2019-09-27T14:04:46.695 DEBUG iotfunctions.pipeline.build_stages_of_type Built stages of type get_data\n",
      "2019-09-27T14:04:46.696 DEBUG iotfunctions.pipeline.build_stages_of_type Available columns: {'evt_timestamp', 'deviceid', 'TestData'}\n",
      "2019-09-27T14:04:46.697 DEBUG iotfunctions.pipeline.build_stages_of_type Gathered stages of type transform. Iteration 0: [\"PythonExpression at granularity None requires inputs {'TestData'} produces outputs ['TestOut'] on schedule 5min\"]\n",
      "2019-09-27T14:04:46.699 DEBUG iotfunctions.pipeline.build_stages_of_type Built stages of type transform\n",
      "2019-09-27T14:04:46.701 DEBUG iotfunctions.pipeline.build_stages_of_type Available columns: {'TestOut', 'evt_timestamp', 'deviceid', 'TestData'}\n",
      "2019-09-27T14:04:46.703 DEBUG iotfunctions.pipeline.build_job_spec Evaluating data source read_entity_data. Data items required from this source for this execution are {'deviceid', 'TestData', 'evt_timestamp'}\n",
      "2019-09-27T14:04:46.704 DEBUG iotfunctions.pipeline.build_job_spec Build of job spec is complete.\n",
      "2019-09-27T14:04:46.705 DEBUG iotfunctions.pipeline.build_job_spec -------------------------------\n",
      "2019-09-27T14:04:46.706 DEBUG iotfunctions.pipeline.build_job_spec skipped_stages: [None]\n",
      "2019-09-27T14:04:46.706 DEBUG iotfunctions.pipeline.build_job_spec input_level:\n",
      "2019-09-27T14:04:46.707 INFO iotfunctions.pipeline.build_job_spec   System generated DataReader stage: read_entity_data. Reads data from objects: markus_testdata\n",
      "2019-09-27T14:04:46.709 INFO iotfunctions.pipeline.build_job_spec   System generated DropNull stage\n",
      "2019-09-27T14:04:46.710 INFO iotfunctions.pipeline.build_job_spec   PythonExpression at granularity None requires inputs {'TestData'} produces outputs ['TestOut'] on schedule 5min\n",
      "2019-09-27T14:04:46.712 INFO iotfunctions.pipeline.build_job_spec   System generated Db2DataWriter stage: markus_testdata_input_level\n",
      "2019-09-27T14:04:46.713 DEBUG iotfunctions.pipeline.build_job_spec -------------------------------\n",
      "TBD ***** - Add stages for usage stats and write to MessageHub\n",
      "OrderedDict([('skipped_stages', set()), ('input_level', [<iotfunctions.pipeline.DataReader object at 0x7f33f79ed278>, <iotfunctions.pipeline.DropNull object at 0x7f33f79edef0>, <iotfunctions.bif.PythonExpression object at 0x7f33f7ae0198>, <iotfunctions.pipeline.Db2DataWriter object at 0x7f33f79ed630>])])\n",
      "2019-09-27T14:04:46.714 WARNING iotfunctions.pipeline.write Execution aborted\n",
      "2019-09-27T14:04:46.717 DEBUG urllib3.connectionpool._new_conn Starting new HTTPS connection (1): s3-api.us-geo.objectstorage.softlayer.net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/.local/lib/python3.7/site-packages/urllib3/connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-27T14:04:47.691 DEBUG urllib3.connectionpool._make_request https://s3-api.us-geo.objectstorage.softlayer.net:443 \"PUT /analytics-runtime-analyticsservicedev-799d2008b460/AnalyticsServiceDev/markus_testdata/20190927/markus_testdata_trace_120444 HTTP/1.1\" 200 0\n",
      "2019-09-27T14:04:47.696 DEBUG iotfunctions.pipeline.save Saved trace to cos AnalyticsServiceDev/markus_testdata/20190927/markus_testdata_trace_120444\n",
      "2019-09-27T14:04:47.698 DEBUG iotfunctions.pipeline.save wrote trace to file auto_trace_markus_testdata_20190927120444.json\n",
      "2019-09-27T14:04:48.280 DEBUG iotfunctions.pipeline.update Updated job log (markus_testdata,5min): 2019-09-27 12:04:44.297393\n",
      "2019-09-27T14:04:48.283 DEBUG iotfunctions.pipeline.get_next_future_execution Next scheduled execution date is 2019-09-27 12:09:44.297393\n",
      "2019-09-27T14:04:48.284 DEBUG iotfunctions.pipeline.execute Ending job normally as there are no scheduled executions  due before execution end time\n",
      "2019-09-27T14:04:48.380 DEBUG iotfunctions.pipeline.run_auto_save auto_trace_markus_testdata_20190927120444 autosave thread has stopped\n",
      "2019-09-27T14:04:48.382 DEBUG iotfunctions.pipeline.stop Stopping autosave on trace auto_trace_markus_testdata_20190927120444\n"
     ]
    }
   ],
   "source": [
    "# dm_markus_testdate MUST exist, so run the following sql statment in DBeaver\n",
    "#CREATE TABLE BLUADMIN.DM_MARKUS_TESTDATA (\n",
    "#  entity_id varchar(255),\n",
    "#  key varchar(255),\n",
    "#  value_n double,\n",
    "#  value_b boolean,\n",
    "#  value_s varchar(255),\n",
    "#  value_t timestamp,\n",
    "#  timestamp timestamp,\n",
    "#  last_update timestamp\n",
    "# );\n",
    "\n",
    "\n",
    "# The commented out version just dumps the job spec\n",
    "#jobsettings = {'writer_name' : Db2DataWriter, 'db': db, '_db_schema': 'BLUADMIN', 'save_trace_to_file' : True, '_get_job_spec': True}\n",
    "jobsettings = {'writer_name' : Db2DataWriter, 'db': db, '_db_schema': 'BLUADMIN', 'save_trace_to_file' : True}\n",
    "job = pp.JobController(et2, **jobsettings)\n",
    "job.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'deviceid', 'type': 'METRIC', 'parentDataItem': None, 'kpiFunctionDto': None, 'columnName': 'deviceid', 'columnType': 'LITERAL', 'sourceTableName': 'markus_testdata', 'tags': [], 'transient': False}, {'name': 'evt_timestamp', 'type': 'METRIC', 'parentDataItem': None, 'kpiFunctionDto': None, 'columnName': 'evt_timestamp', 'columnType': 'TIMESTAMP', 'sourceTableName': 'markus_testdata', 'tags': [], 'transient': False}, {'name': 'TestData', 'type': 'METRIC', 'parentDataItem': None, 'kpiFunctionDto': None, 'columnName': 'TestData', 'columnType': 'NUMBER', 'sourceTableName': 'markus_testdata', 'tags': [], 'transient': False}, {'columnName': 'TestOut', 'columnType': 'NUMBER', 'kpiFunctionId': 22856, 'kpiFunctionDto': {'output': {'name': 'TestOut'}}, 'name': 'TestOut', 'parentDataItemName': None, 'sourceTableName': 'dm_markus_testdata', 'transient': False, 'type': 'DERIVED_METRIC'}, {'columnName': 'deviceid', 'columnType': 'LITERAL', 'kpiFunctionId': None, 'kpiFunctionDto': {}, 'name': 'ENTITY_ID', 'parentDataItemName': None, 'sourceTableName': 'dm_markus_testdata', 'transient': False, 'type': 'METRIC'}, {'columnName': 'TestData_max', 'columnType': 'NUMBER', 'kpiFunctionId': 22856, 'kpiFunctionDto': {'output': {'name': 'TestData_max'}}, 'name': 'TestData_max', 'parentDataItemName': None, 'sourceTableName': 'dm_markus_testdata_daily', 'transient': False, 'type': 'DERIVED_METRIC'}, {'columnName': 'deviceid', 'columnType': 'LITERAL', 'kpiFunctionId': None, 'kpiFunctionDto': {}, 'name': 'ENTITY_ID', 'parentDataItemName': None, 'sourceTableName': 'dm_markus_testdata_daily', 'transient': False, 'type': 'METRIC'}]\n"
     ]
    }
   ],
   "source": [
    "print (et2.get_data_items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-27T12:50:01.158 DEBUG iotfunctions.pipeline.set_payload_params Setting param writer_name on payload to <class 'iotfunctions.pipeline.Db2DataWriter'>\n",
      "2019-09-27T12:50:01.159 DEBUG iotfunctions.pipeline.set_payload_params Setting param _db_schema on payload to BLUADMIN\n",
      "2019-09-27T12:50:01.160 DEBUG iotfunctions.pipeline.set_payload_params Setting param save_trace_to_file on payload to True\n",
      "2019-09-27T12:50:01.160 DEBUG iotfunctions.pipeline.set_payload_params Setting param tenant_id on payload to AnalyticsServiceDev\n",
      "2019-09-27T12:50:02.361 DEBUG iotfunctions.pipeline.get_output_list The payload has candidate data items ['deviceid', 'evt_timestamp', 'TestData', 'deviceid', 'deviceid']. The DataReader has no projection list\n",
      "2019-09-27T12:50:02.363 DEBUG iotfunctions.metadata.classify_stages Output list set was preset for function AggregateItems\n",
      "2019-09-27T12:50:02.364 DEBUG iotfunctions.metadata.classify_stages Function AggregateItems has no _metadata_params property. This property allows the stage to add properties to the entity type. Using default of {}\n",
      "2019-09-27T12:50:02.365 INFO iotfunctions.pipeline.__init__ Initialized job.\n",
      "\n",
      "2019-09-27T12:50:02.366 INFO iotfunctions.pipeline.__init__ \n",
      "Default schedule 5min \n",
      "    Schedule 5min start None:None backtracks: None \n",
      "Stages of type: get_data at grain None: \n",
      "   System generated DataReader stage: read_entity_data. Reads data from objects: markus_testdata\n",
      "Stages of type: simple_aggregate at grain daily: \n",
      "   <iotfunctions.pipeline.AggregateItems object at 0x7f412e9b0d68>\n",
      "\n",
      "\n",
      "2019-09-27T12:50:02.368 INFO iotfunctions.enginelog.start_run_log Started logging into file run.log. Object Store path will be AnalyticsServiceDev/markus_testdata/20190927/105002_run.gz\n",
      "2019-09-27T12:50:02.369 DEBUG iotfunctions.pipeline.execute Starting execution number: 0 with execution date: 2019-09-27 10:50:02.367674\n",
      "2019-09-27T12:50:03.004 DEBUG iotfunctions.pipeline.get_next_execution_date Last execution of schedule 5min was 2019-09-27 10:49:26.872512. Next execution due 2019-09-27 10:54:26.872512.\n",
      "2019-09-27T12:50:03.006 DEBUG iotfunctions.pipeline.evaluate_schedules Schedule 5min will execute\n",
      "2019-09-27T12:50:03.008 DEBUG iotfunctions.pipeline.reset Started a new trace auto_trace_markus_testdata_20190927105002 \n",
      "2019-09-27T12:50:03.009 DEBUG iotfunctions.pipeline.reset Initiating auto save for trace\n",
      "2019-09-27T12:50:04.572 DEBUG iotfunctions.pipeline.insert Created job log entry (markus_testdata,5min): 2019-09-27 10:50:02.367674\n",
      "2019-09-27T12:50:04.574 DEBUG iotfunctions.pipeline.build_job_spec Building a job spec for schedule 5min with subsumbed schedules ['5min']\n",
      "2019-09-27T12:50:04.577 DEBUG iotfunctions.pipeline.build_stages_of_type Gathered stages of type get_data. Iteration 0: ['System generated DataReader stage: read_entity_data. Reads data from objects: markus_testdata']\n",
      "2019-09-27T12:50:04.579 DEBUG iotfunctions.pipeline.build_stages_of_type Built stages of type get_data\n",
      "2019-09-27T12:50:04.581 DEBUG iotfunctions.pipeline.build_stages_of_type Available columns: {'deviceid', 'evt_timestamp', 'TestData'}\n",
      "2019-09-27T12:50:04.583 DEBUG iotfunctions.pipeline.build_stages_of_type Built stages of type transform\n",
      "2019-09-27T12:50:04.584 DEBUG iotfunctions.pipeline.build_stages_of_type Available columns: {'deviceid', 'evt_timestamp', 'TestData'}\n",
      "2019-09-27T12:50:04.585 DEBUG iotfunctions.pipeline.build_job_spec Building job spec for aggregation to grain: daily\n",
      "2019-09-27T12:50:04.585 DEBUG iotfunctions.pipeline.build_job_spec Collapsed aggregation stages ['AggregateItems'] down to a single\"\n",
      "2019-09-27T12:50:04.586 DEBUG iotfunctions.pipeline.build_job_spec OrderedDict([('TestData', ['max'])])\n",
      "2019-09-27T12:50:04.586 DEBUG iotfunctions.pipeline.build_job_spec Added aggregregator to job spec: Aggregator: auto_aggregate with granularity: daily.  Aggregates TestData using ['max'] .\n",
      "2019-09-27T12:50:04.587 DEBUG iotfunctions.pipeline.build_stages_of_type Built stages of type transform\n",
      "2019-09-27T12:50:04.588 DEBUG iotfunctions.pipeline.build_stages_of_type Available columns: {'TestData_max', 'deviceid', 'evt_timestamp', 'TestData'}\n",
      "2019-09-27T12:50:04.588 DEBUG iotfunctions.pipeline.build_job_spec Completed job spec build for grain: daily\n",
      "2019-09-27T12:50:04.589 DEBUG iotfunctions.pipeline.build_job_spec Evaluating data source read_entity_data. Data items required from this source for this execution are {'deviceid', 'evt_timestamp', 'TestData'}\n",
      "2019-09-27T12:50:04.590 DEBUG iotfunctions.pipeline.build_job_spec Build of job spec is complete.\n",
      "2019-09-27T12:50:04.590 DEBUG iotfunctions.pipeline.build_job_spec -------------------------------\n",
      "2019-09-27T12:50:04.591 DEBUG iotfunctions.pipeline.build_job_spec skipped_stages: [None]\n",
      "2019-09-27T12:50:04.592 DEBUG iotfunctions.pipeline.build_job_spec input_level:\n",
      "2019-09-27T12:50:04.593 INFO iotfunctions.pipeline.build_job_spec   System generated DataReader stage: read_entity_data. Reads data from objects: markus_testdata\n",
      "2019-09-27T12:50:04.594 INFO iotfunctions.pipeline.build_job_spec   System generated DropNull stage\n",
      "2019-09-27T12:50:04.595 INFO iotfunctions.pipeline.build_job_spec   System generated Db2DataWriter stage: markus_testdata_input_level\n",
      "2019-09-27T12:50:04.597 DEBUG iotfunctions.pipeline.build_job_spec daily:\n",
      "2019-09-27T12:50:04.597 INFO iotfunctions.pipeline.build_job_spec   Aggregator: auto_aggregate with granularity: daily.  Aggregates TestData using ['max'] .\n",
      "2019-09-27T12:50:04.598 INFO iotfunctions.pipeline.build_job_spec   System generated Db2DataWriter stage: markus_testdata_daily\n",
      "2019-09-27T12:50:04.599 DEBUG iotfunctions.pipeline.build_job_spec -------------------------------\n",
      "TBD ***** - Add stages for usage stats and write to MessageHub\n",
      "2019-09-27T12:50:04.600 DEBUG iotfunctions.pipeline.exec_payload_method Returned default output for get_early_timestamp() on payload EntityType markus_testdata.  Default value is: None\n",
      "2019-09-27T12:50:04.601 DEBUG iotfunctions.pipeline.get_chunks The payload does not have an get_early_timestamp method or the method did not retrieve an early timestamp. Data will be retrieved in a single chunk\n",
      "2019-09-27T12:50:04.602 DEBUG iotfunctions.pipeline.write Processing as a single chunk\n",
      "2019-09-27T12:50:04.604 DEBUG iotfunctions.pipeline.write Executing stage read_entity_data.\n",
      "2019-09-27T12:50:05.037 DEBUG iotfunctions.metadata.index_df Indexed dataframe on id, evt_timestamp\n",
      "2019-09-27T12:50:05.039 DEBUG iotfunctions.pipeline.exec_stage_method No method get_column_map on read_entity_data returning default None. 'DataReader' object has no attribute 'get_column_map'\n",
      "2019-09-27T12:50:05.040 DEBUG iotfunctions.pipeline.merge_dataframe Merging dataframe with columns [] and index []\n",
      "2019-09-27T12:50:05.041 DEBUG iotfunctions.metadata.index_df Found existing index on id, evt_timestamp.No need to recreate index\n",
      "2019-09-27T12:50:05.042 INFO iotfunctions.pipeline.update_last_entry Trace message:  Completed stage.\n",
      "2019-09-27T12:50:05.043 INFO iotfunctions.pipeline.update_last_entry Trace payload: {'produces_output_items': True, 'output_items': ['deviceid', 'evt_timestamp', 'TestData', 'deviceid', 'deviceid'], 'discard_prior_data': False, 'merge_result': 'existing empty df with new DataFrame', 'usage': 170, 'index': ['id', 'evt_timestamp'], 'can_proceed': True, 'updated': datetime.datetime(2019, 9, 27, 10, 50, 5, 42754), 'cumulative_usage': 170}\n",
      "2019-09-27T12:50:05.044 DEBUG iotfunctions.pipeline.write Executing stage drop_null.\n",
      "2019-09-27T12:50:05.044 DEBUG iotfunctions.pipeline.execute columns excluded when dropping null rows ['deviceid', '_timestamp', 'logicalinterface_id', 'devicetype', 'format', 'updated_utc', 'evt_timestamp']\n",
      "2019-09-27T12:50:05.045 DEBUG iotfunctions.pipeline.execute columns considered when dropping null rows ['eventtype', 'TestData']\n",
      "2019-09-27T12:50:05.046 DEBUG iotfunctions.pipeline.execute eventtype count not null: 34\n",
      "2019-09-27T12:50:05.047 DEBUG iotfunctions.pipeline.execute TestData count not null: 34\n",
      "2019-09-27T12:50:05.052 DEBUG iotfunctions.pipeline.exec_stage_method No method get_column_map on drop_null returning default None. 'DropNull' object has no attribute 'get_column_map'\n",
      "2019-09-27T12:50:05.053 INFO iotfunctions.pipeline.update_last_entry Trace message:  Completed stage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-27T12:50:05.053 INFO iotfunctions.pipeline.update_last_entry Trace payload: {'produces_output_items': False, 'output_items': None, 'discard_prior_data': False, 'new_data_items_info': 'Function is configured not to produce any new data items  during execution', 'can_proceed': True, 'updated': datetime.datetime(2019, 9, 27, 10, 50, 5, 53355), 'cumulative_usage': 170}\n",
      "2019-09-27T12:50:05.054 DEBUG iotfunctions.pipeline.write Executing stage markus_testdata_input_level.\n",
      "2019-09-27T12:50:05.055 DEBUG iotfunctions.pipeline.execute Data items will be written to database for interval (None, 2019-09-27 10:50:02.367674)\n",
      "2019-09-27T12:50:05.057 WARNING iotfunctions.pipeline._get_active_cols_properties No function definition defined for data item deviceid.\n",
      "2019-09-27T12:50:05.058 INFO iotfunctions.pipeline._get_active_cols_properties The column devicetype in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-09-27T12:50:05.059 INFO iotfunctions.pipeline._get_active_cols_properties The column logicalinterface_id in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-09-27T12:50:05.060 INFO iotfunctions.pipeline._get_active_cols_properties The column eventtype in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-09-27T12:50:05.061 INFO iotfunctions.pipeline._get_active_cols_properties The column format in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-09-27T12:50:05.062 INFO iotfunctions.pipeline._get_active_cols_properties The column updated_utc in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-09-27T12:50:05.063 WARNING iotfunctions.pipeline._get_active_cols_properties No function definition defined for data item TestData.\n",
      "2019-09-27T12:50:05.064 INFO iotfunctions.pipeline._get_active_cols_properties The column _timestamp in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-09-27T12:50:05.065 INFO iotfunctions.pipeline.execute The following data items will be written to the database: \n",
      "2019-09-27T12:50:05.066 DEBUG iotfunctions.pipeline._get_table_properties Mapping between index name and index position: id -> 0, evt_timestamp -> 1\n",
      "2019-09-27T12:50:05.066 INFO iotfunctions.pipeline.execute The data items will be written into the following tables: \n",
      "2019-09-27T12:50:05.068 WARNING iotfunctions.pipeline.execute There are no data items that have to be written to the database.\n",
      "2019-09-27T12:50:05.069 DEBUG iotfunctions.pipeline.exec_stage_method No method get_column_map on markus_testdata_input_level returning default None. 'Db2DataWriter' object has no attribute 'get_column_map'\n",
      "2019-09-27T12:50:05.070 INFO iotfunctions.pipeline.update_last_entry Trace message:  Completed stage.\n",
      "2019-09-27T12:50:05.071 INFO iotfunctions.pipeline.update_last_entry Trace payload: {'produces_output_items': False, 'output_items': None, 'discard_prior_data': False, 'new_data_items_info': 'Function is configured not to produce any new data items  during execution', 'can_proceed': True, 'updated': datetime.datetime(2019, 9, 27, 10, 50, 5, 70101), 'cumulative_usage': 170}\n",
      "2019-09-27T12:50:05.074 WARNING iotfunctions.pipeline.write Error aligning input data to granularity daily\n",
      "2019-09-27T12:50:05.074 DEBUG iotfunctions.pipeline.write Executing stage auto_aggregate.\n",
      "2019-09-27T12:50:05.085 INFO iotfunctions.pipeline.execute Completed aggregation: daily\n",
      "2019-09-27T12:50:05.086 DEBUG iotfunctions.pipeline.exec_stage_method No method get_column_map on auto_aggregate returning default None. 'DataAggregator' object has no attribute 'get_column_map'\n",
      "2019-09-27T12:50:05.087 INFO iotfunctions.pipeline.update_last_entry Trace message:  Completed stage.\n",
      "2019-09-27T12:50:05.088 INFO iotfunctions.pipeline.update_last_entry Trace payload: {'produces_output_items': True, 'output_items': ['TestData_max'], 'discard_prior_data': True, 'merge_result': 'replaced prior data', 'index': [], 'can_proceed': True, 'updated': datetime.datetime(2019, 9, 27, 10, 50, 5, 87358), 'cumulative_usage': 170}\n",
      "2019-09-27T12:50:05.088 DEBUG iotfunctions.pipeline.write Executing stage markus_testdata_daily.\n",
      "2019-09-27T12:50:05.089 DEBUG iotfunctions.pipeline.execute Data items will be written to database for interval (None, 2019-09-27 10:50:02.367674)\n",
      "2019-09-27T12:50:05.090 INFO iotfunctions.pipeline.execute The following data items will be written to the database: TestData_max (dm_markus_testdata_daily, NUMBER)\n",
      "2019-09-27T12:50:05.091 DEBUG iotfunctions.pipeline._get_table_properties Mapping between index name and index position: deviceid -> 0, evt_timestamp -> 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/.local/lib/python3.7/site-packages/ibm_db_sa/reflection.py:255: SAWarning: Did not recognize type 'BOOLEAN' of column 'VALUE_B'\n",
      "  (coltype, r[0]))\n",
      "/home/markus/.local/lib/python3.7/site-packages/sqlalchemy/engine/reflection.py:913: SAWarning: index key 'sqlnotapplicable' was not located in columns for table 'dm_markus_testdata_daily'\n",
      "  \"columns for table '%s'\" % (flavor, c, table_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-27T12:50:08.159 DEBUG iotfunctions.pipeline._get_table_properties For table dm_markus_testdata_daily: delete statement: DELETE FROM dm_markus_testdata_daily, insert statement: INSERT INTO dm_markus_testdata_daily (entity_id, \"KEY\", value_n, value_b, value_s, value_t, \"TIMESTAMP\", last_update) VALUES (:entity_id, :KEY, :value_n, :value_b, :value_s, :value_t, :TIMESTAMP, :last_update)\n",
      "2019-09-27T12:50:08.160 DEBUG iotfunctions.pipeline._get_table_properties For table dm_markus_testdata_daily: Mapping between column name and dataframe index position: ('entity_id', 0), ('TIMESTAMP', 1)\n",
      "2019-09-27T12:50:08.161 INFO iotfunctions.pipeline.execute The data items will be written into the following tables: dm_markus_testdata_daily\n",
      "2019-09-27T12:50:08.161 DEBUG iotfunctions.pipeline._delete_old_data Deleting old data items from table dm_markus_testdata_daily for time range [None, 2019-09-27 10:50:02.367674]\n",
      "2019-09-27T12:50:08.958 INFO iotfunctions.pipeline._delete_old_data 0 data items have been deleted from table dm_markus_testdata_daily\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73004', 'TIMESTAMP': Timestamp('2019-09-19 00:00:00', freq='D'), 'value_b': None, 'value_n': 1.5042421970969237, 'value_s': None, 'value_t': None})\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73004', 'TIMESTAMP': Timestamp('2019-09-25 00:00:00', freq='D'), 'value_b': None, 'value_n': 0.6323318288940809, 'value_s': None, 'value_t': None})\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73004', 'TIMESTAMP': Timestamp('2019-09-26 00:00:00', freq='D'), 'value_b': None, 'value_n': 0.37219643279116404, 'value_s': None, 'value_t': None})\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73002', 'TIMESTAMP': Timestamp('2019-09-19 00:00:00', freq='D'), 'value_b': None, 'value_n': -0.683287047402222, 'value_s': None, 'value_t': None})\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73002', 'TIMESTAMP': Timestamp('2019-09-25 00:00:00', freq='D'), 'value_b': None, 'value_n': 2.9433508116510034, 'value_s': None, 'value_t': None})\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73002', 'TIMESTAMP': Timestamp('2019-09-26 00:00:00', freq='D'), 'value_b': None, 'value_n': 2.6481058098912054, 'value_s': None, 'value_t': None})\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73001', 'TIMESTAMP': Timestamp('2019-09-19 00:00:00', freq='D'), 'value_b': None, 'value_n': 0.48644626410428893, 'value_s': None, 'value_t': None})\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73001', 'TIMESTAMP': Timestamp('2019-09-25 00:00:00', freq='D'), 'value_b': None, 'value_n': 0.2241422029496413, 'value_s': None, 'value_t': None})\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73001', 'TIMESTAMP': Timestamp('2019-09-26 00:00:00', freq='D'), 'value_b': None, 'value_n': 1.5099158782105526, 'value_s': None, 'value_t': None})\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73000', 'TIMESTAMP': Timestamp('2019-09-19 00:00:00', freq='D'), 'value_b': None, 'value_n': 1.6025817246852831, 'value_s': None, 'value_t': None})\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73000', 'TIMESTAMP': Timestamp('2019-09-25 00:00:00', freq='D'), 'value_b': None, 'value_n': 1.894009523806188, 'value_s': None, 'value_t': None})\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73000', 'TIMESTAMP': Timestamp('2019-09-26 00:00:00', freq='D'), 'value_b': None, 'value_n': -0.3617052294893387, 'value_s': None, 'value_t': None})\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73003', 'TIMESTAMP': Timestamp('2019-09-19 00:00:00', freq='D'), 'value_b': None, 'value_n': -0.5063391141402293, 'value_s': None, 'value_t': None})\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73003', 'TIMESTAMP': Timestamp('2019-09-25 00:00:00', freq='D'), 'value_b': None, 'value_n': 1.8504000182714633, 'value_s': None, 'value_t': None})\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73003', 'TIMESTAMP': Timestamp('2019-09-26 00:00:00', freq='D'), 'value_b': None, 'value_n': 0.2434480896530682, 'value_s': None, 'value_t': None})\n",
      "2019-09-27T12:50:09.793 INFO iotfunctions.pipeline._persist_data Number of data item values persisted so far: 15 (dm_markus_testdata_daily)\n",
      "2019-09-27T12:50:09.795 INFO iotfunctions.pipeline._persist_data Total number of persisted data item values: 15\n",
      "2019-09-27T12:50:09.796 DEBUG iotfunctions.pipeline.exec_stage_method No method get_column_map on markus_testdata_daily returning default None. 'Db2DataWriter' object has no attribute 'get_column_map'\n",
      "[]\n",
      "2019-09-27T12:50:09.797 INFO iotfunctions.pipeline.update_last_entry Trace message:  Completed stage.\n",
      "2019-09-27T12:50:09.798 INFO iotfunctions.pipeline.update_last_entry Trace payload: {'produces_output_items': False, 'output_items': None, 'discard_prior_data': False, 'new_data_items_info': 'Function is configured not to produce any new data items  during execution', 'can_proceed': True, 'updated': datetime.datetime(2019, 9, 27, 10, 50, 9, 797660), 'cumulative_usage': 170}\n",
      "2019-09-27T12:50:09.799 INFO iotfunctions.pipeline.write Execution complete\n",
      "2019-09-27T12:50:09.803 DEBUG urllib3.connectionpool._new_conn Starting new HTTPS connection (1): s3-api.us-geo.objectstorage.softlayer.net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/.local/lib/python3.7/site-packages/urllib3/connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-27T12:50:10.943 DEBUG urllib3.connectionpool._make_request https://s3-api.us-geo.objectstorage.softlayer.net:443 \"PUT /analytics-runtime-analyticsservicedev-799d2008b460/AnalyticsServiceDev/markus_testdata/20190927/markus_testdata_trace_105002 HTTP/1.1\" 200 0\n",
      "2019-09-27T12:50:10.946 DEBUG iotfunctions.pipeline.save Saved trace to cos AnalyticsServiceDev/markus_testdata/20190927/markus_testdata_trace_105002\n",
      "2019-09-27T12:50:10.950 DEBUG iotfunctions.pipeline.save wrote trace to file auto_trace_markus_testdata_20190927105002.json\n",
      "2019-09-27T12:50:11.518 DEBUG iotfunctions.pipeline.update Updated job log (markus_testdata,5min): 2019-09-27 10:50:02.367674\n",
      "2019-09-27T12:50:11.520 DEBUG iotfunctions.pipeline.get_next_future_execution Next scheduled execution date is 2019-09-27 10:55:02.367674\n",
      "2019-09-27T12:50:11.522 DEBUG iotfunctions.pipeline.execute Ending job normally as there are no scheduled executions  due before execution end time\n",
      "2019-09-27T12:50:11.618 DEBUG iotfunctions.pipeline.run_auto_save auto_trace_markus_testdata_20190927105002 autosave thread has stopped\n",
      "2019-09-27T12:50:11.620 DEBUG iotfunctions.pipeline.stop Stopping autosave on trace auto_trace_markus_testdata_20190927105002\n"
     ]
    }
   ],
   "source": [
    "# dm_markus_testdate MUST exist, so run the following sql statment in DBeaver\n",
    "#CREATE TABLE BLUADMIN.DM_MARKUS_TESTDATA_DAILY (\n",
    "#  entity_id varchar(255),\n",
    "#  key varchar(255),\n",
    "#  value_n double,\n",
    "#  value_b boolean,\n",
    "#  value_s varchar(255),\n",
    "#  value_t timestamp,\n",
    "#  timestamp timestamp,\n",
    "#  last_update timestamp\n",
    "# );\n",
    "\n",
    "from iotfunctions.metadata import Granularity\n",
    "from iotfunctions.pipeline import AggregateItems\n",
    "daily = Granularity(\n",
    "    name = 'daily',\n",
    "    freq = '1D',                 # pandas frequency string\n",
    "    timestamp= 'evt_timestamp',      # build time aggregations using this datetime col\n",
    "    entity_id = 'deviceid',            # aggregate by id\n",
    "    dimensions = None,\n",
    "    entity_name = None\n",
    ")\n",
    "\n",
    "#myAgg = bif.AggregateWithExpression(['TestData'],'x.max()','TestMax')\n",
    "myAgg = AggregateItems(['TestData'], 'max')\n",
    "myAgg.granularity = daily\n",
    "\n",
    "et2._functions = [myAgg]\n",
    "et2.grains = [daily]\n",
    "#et2._granularities_dict['daily'] = daily\n",
    "\n",
    "jobsettings = {'writer_name' : Db2DataWriter, '_db_schema': 'BLUADMIN', 'save_trace_to_file' : True}\n",
    "job = pp.JobController(et2, **jobsettings)\n",
    "#job.data_writer = DataWriterFile\n",
    "job.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-27T14:04:54.880 DEBUG iotfunctions.metadata.index_df Indexed dataframe on id, evt_timestamp\n",
      "                                 deviceid       devicetype  \\\n",
      "id    evt_timestamp                                          \n",
      "73004 2019-09-19 11:36:35.244129    73004  markus_testdata   \n",
      "73002 2019-09-19 11:37:35.244129    73002  markus_testdata   \n",
      "73001 2019-09-19 11:38:35.244129    73001  markus_testdata   \n",
      "73004 2019-09-19 11:39:35.244129    73004  markus_testdata   \n",
      "73000 2019-09-19 11:40:35.244129    73000  markus_testdata   \n",
      "73003 2019-09-19 11:41:35.244129    73003  markus_testdata   \n",
      "73004 2019-09-19 11:37:03.228606    73004  markus_testdata   \n",
      "      2019-09-19 11:42:03.228606    73004  markus_testdata   \n",
      "73003 2019-09-25 09:52:17.892082    73003  markus_testdata   \n",
      "      2019-09-25 09:53:17.892082    73003  markus_testdata   \n",
      "73001 2019-09-25 09:54:17.892082    73001  markus_testdata   \n",
      "73004 2019-09-25 09:55:17.892082    73004  markus_testdata   \n",
      "73002 2019-09-25 09:56:17.892082    73002  markus_testdata   \n",
      "73000 2019-09-25 09:57:17.892082    73000  markus_testdata   \n",
      "73002 2019-09-25 09:56:36.762580    73002  markus_testdata   \n",
      "73003 2019-09-25 10:01:36.762580    73003  markus_testdata   \n",
      "73000 2019-09-25 13:36:20.560426    73000  markus_testdata   \n",
      "73002 2019-09-25 13:37:20.560426    73002  markus_testdata   \n",
      "73000 2019-09-25 13:38:20.560426    73000  markus_testdata   \n",
      "73004 2019-09-25 13:39:20.560426    73004  markus_testdata   \n",
      "73001 2019-09-25 13:40:20.560426    73001  markus_testdata   \n",
      "73002 2019-09-25 13:41:20.560426    73002  markus_testdata   \n",
      "73001 2019-09-26 16:41:33.275868    73001  markus_testdata   \n",
      "73002 2019-09-26 16:42:33.275868    73002  markus_testdata   \n",
      "73001 2019-09-26 16:43:33.275868    73001  markus_testdata   \n",
      "73003 2019-09-26 16:44:33.275868    73003  markus_testdata   \n",
      "      2019-09-26 16:45:33.275868    73003  markus_testdata   \n",
      "73002 2019-09-26 16:46:33.275868    73002  markus_testdata   \n",
      "73000 2019-09-26 16:48:02.695417    73000  markus_testdata   \n",
      "73004 2019-09-26 16:49:02.695417    73004  markus_testdata   \n",
      "      2019-09-26 16:50:02.695417    73004  markus_testdata   \n",
      "73002 2019-09-26 16:51:02.695417    73002  markus_testdata   \n",
      "73001 2019-09-26 16:52:02.695417    73001  markus_testdata   \n",
      "73000 2019-09-26 16:53:02.695417    73000  markus_testdata   \n",
      "\n",
      "                                 logicalinterface_id eventtype format  \\\n",
      "id    evt_timestamp                                                     \n",
      "73004 2019-09-19 11:36:35.244129                            yn          \n",
      "73002 2019-09-19 11:37:35.244129                            tn          \n",
      "73001 2019-09-19 11:38:35.244129                            ny          \n",
      "73004 2019-09-19 11:39:35.244129                            ey          \n",
      "73000 2019-09-19 11:40:35.244129                            ve          \n",
      "73003 2019-09-19 11:41:35.244129                            ty          \n",
      "73004 2019-09-19 11:37:03.228606                            ee          \n",
      "      2019-09-19 11:42:03.228606                            en          \n",
      "73003 2019-09-25 09:52:17.892082                            en          \n",
      "      2019-09-25 09:53:17.892082                            ep          \n",
      "73001 2019-09-25 09:54:17.892082                            en          \n",
      "73004 2019-09-25 09:55:17.892082                            vn          \n",
      "73002 2019-09-25 09:56:17.892082                            ne          \n",
      "73000 2019-09-25 09:57:17.892082                            ee          \n",
      "73002 2019-09-25 09:56:36.762580                            ey          \n",
      "73003 2019-09-25 10:01:36.762580                            ep          \n",
      "73000 2019-09-25 13:36:20.560426                            en          \n",
      "73002 2019-09-25 13:37:20.560426                            te          \n",
      "73000 2019-09-25 13:38:20.560426                            et          \n",
      "73004 2019-09-25 13:39:20.560426                            ee          \n",
      "73001 2019-09-25 13:40:20.560426                            et          \n",
      "73002 2019-09-25 13:41:20.560426                            en          \n",
      "73001 2019-09-26 16:41:33.275868                            et          \n",
      "73002 2019-09-26 16:42:33.275868                            tt          \n",
      "73001 2019-09-26 16:43:33.275868                            ep          \n",
      "73003 2019-09-26 16:44:33.275868                            nt          \n",
      "      2019-09-26 16:45:33.275868                            te          \n",
      "73002 2019-09-26 16:46:33.275868                            te          \n",
      "73000 2019-09-26 16:48:02.695417                            yt          \n",
      "73004 2019-09-26 16:49:02.695417                            te          \n",
      "      2019-09-26 16:50:02.695417                            ne          \n",
      "73002 2019-09-26 16:51:02.695417                            te          \n",
      "73001 2019-09-26 16:52:02.695417                            np          \n",
      "73000 2019-09-26 16:53:02.695417                            et          \n",
      "\n",
      "                                 updated_utc  TestData  \\\n",
      "id    evt_timestamp                                      \n",
      "73004 2019-09-19 11:36:35.244129        None  0.317105   \n",
      "73002 2019-09-19 11:37:35.244129        None -0.683287   \n",
      "73001 2019-09-19 11:38:35.244129        None  0.486446   \n",
      "73004 2019-09-19 11:39:35.244129        None  0.536891   \n",
      "73000 2019-09-19 11:40:35.244129        None  1.602582   \n",
      "73003 2019-09-19 11:41:35.244129        None -0.506339   \n",
      "73004 2019-09-19 11:37:03.228606        None  0.224947   \n",
      "      2019-09-19 11:42:03.228606        None  1.504242   \n",
      "73003 2019-09-25 09:52:17.892082        None -0.259429   \n",
      "      2019-09-25 09:53:17.892082        None  0.756584   \n",
      "73001 2019-09-25 09:54:17.892082        None -0.707649   \n",
      "73004 2019-09-25 09:55:17.892082        None  0.632332   \n",
      "73002 2019-09-25 09:56:17.892082        None  0.161956   \n",
      "73000 2019-09-25 09:57:17.892082        None  1.894010   \n",
      "73002 2019-09-25 09:56:36.762580        None  2.943351   \n",
      "73003 2019-09-25 10:01:36.762580        None  1.850400   \n",
      "73000 2019-09-25 13:36:20.560426        None  0.544338   \n",
      "73002 2019-09-25 13:37:20.560426        None  1.790286   \n",
      "73000 2019-09-25 13:38:20.560426        None  0.573832   \n",
      "73004 2019-09-25 13:39:20.560426        None -0.602018   \n",
      "73001 2019-09-25 13:40:20.560426        None  0.224142   \n",
      "73002 2019-09-25 13:41:20.560426        None  0.607019   \n",
      "73001 2019-09-26 16:41:33.275868        None  1.509916   \n",
      "73002 2019-09-26 16:42:33.275868        None  0.664349   \n",
      "73001 2019-09-26 16:43:33.275868        None -1.396522   \n",
      "73003 2019-09-26 16:44:33.275868        None  0.243448   \n",
      "      2019-09-26 16:45:33.275868        None  0.022209   \n",
      "73002 2019-09-26 16:46:33.275868        None  2.648106   \n",
      "73000 2019-09-26 16:48:02.695417        None -1.606241   \n",
      "73004 2019-09-26 16:49:02.695417        None  0.372196   \n",
      "      2019-09-26 16:50:02.695417        None -0.391822   \n",
      "73002 2019-09-26 16:51:02.695417        None -1.279792   \n",
      "73001 2019-09-26 16:52:02.695417        None  1.143055   \n",
      "73000 2019-09-26 16:53:02.695417        None -0.361705   \n",
      "\n",
      "                                                 _timestamp  \n",
      "id    evt_timestamp                                          \n",
      "73004 2019-09-19 11:36:35.244129 2019-09-19 11:36:35.244129  \n",
      "73002 2019-09-19 11:37:35.244129 2019-09-19 11:37:35.244129  \n",
      "73001 2019-09-19 11:38:35.244129 2019-09-19 11:38:35.244129  \n",
      "73004 2019-09-19 11:39:35.244129 2019-09-19 11:39:35.244129  \n",
      "73000 2019-09-19 11:40:35.244129 2019-09-19 11:40:35.244129  \n",
      "73003 2019-09-19 11:41:35.244129 2019-09-19 11:41:35.244129  \n",
      "73004 2019-09-19 11:37:03.228606 2019-09-19 11:37:03.228606  \n",
      "      2019-09-19 11:42:03.228606 2019-09-19 11:42:03.228606  \n",
      "73003 2019-09-25 09:52:17.892082 2019-09-25 09:52:17.892082  \n",
      "      2019-09-25 09:53:17.892082 2019-09-25 09:53:17.892082  \n",
      "73001 2019-09-25 09:54:17.892082 2019-09-25 09:54:17.892082  \n",
      "73004 2019-09-25 09:55:17.892082 2019-09-25 09:55:17.892082  \n",
      "73002 2019-09-25 09:56:17.892082 2019-09-25 09:56:17.892082  \n",
      "73000 2019-09-25 09:57:17.892082 2019-09-25 09:57:17.892082  \n",
      "73002 2019-09-25 09:56:36.762580 2019-09-25 09:56:36.762580  \n",
      "73003 2019-09-25 10:01:36.762580 2019-09-25 10:01:36.762580  \n",
      "73000 2019-09-25 13:36:20.560426 2019-09-25 13:36:20.560426  \n",
      "73002 2019-09-25 13:37:20.560426 2019-09-25 13:37:20.560426  \n",
      "73000 2019-09-25 13:38:20.560426 2019-09-25 13:38:20.560426  \n",
      "73004 2019-09-25 13:39:20.560426 2019-09-25 13:39:20.560426  \n",
      "73001 2019-09-25 13:40:20.560426 2019-09-25 13:40:20.560426  \n",
      "73002 2019-09-25 13:41:20.560426 2019-09-25 13:41:20.560426  \n",
      "73001 2019-09-26 16:41:33.275868 2019-09-26 16:41:33.275868  \n",
      "73002 2019-09-26 16:42:33.275868 2019-09-26 16:42:33.275868  \n",
      "73001 2019-09-26 16:43:33.275868 2019-09-26 16:43:33.275868  \n",
      "73003 2019-09-26 16:44:33.275868 2019-09-26 16:44:33.275868  \n",
      "      2019-09-26 16:45:33.275868 2019-09-26 16:45:33.275868  \n",
      "73002 2019-09-26 16:46:33.275868 2019-09-26 16:46:33.275868  \n",
      "73000 2019-09-26 16:48:02.695417 2019-09-26 16:48:02.695417  \n",
      "73004 2019-09-26 16:49:02.695417 2019-09-26 16:49:02.695417  \n",
      "      2019-09-26 16:50:02.695417 2019-09-26 16:50:02.695417  \n",
      "73002 2019-09-26 16:51:02.695417 2019-09-26 16:51:02.695417  \n",
      "73001 2019-09-26 16:52:02.695417 2019-09-26 16:52:02.695417  \n",
      "73000 2019-09-26 16:53:02.695417 2019-09-26 16:53:02.695417  \n"
     ]
    }
   ],
   "source": [
    "print (et2.get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
