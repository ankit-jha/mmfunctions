{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/.local/lib/python3.7/site-packages/iotfunctions/bif.py:1946: UserWarning: IoTCalcSettings is deprecated. Use entity type constants instead of a metadata provider to set entity type properties\n",
      "  warnings.warn(('IoTCalcSettings is deprecated. Use entity type constants'\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import threading\n",
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as seabornInstance\n",
    "from sqlalchemy import Column, Integer, String, Float, DateTime, Boolean, func\n",
    "from iotfunctions import base\n",
    "from iotfunctions import bif\n",
    "from iotfunctions import entity\n",
    "from iotfunctions import metadata\n",
    "from iotfunctions.metadata import EntityType\n",
    "from iotfunctions.db import Database\n",
    "from iotfunctions.enginelog import EngineLogging\n",
    "from iotfunctions import estimator\n",
    "from iotfunctions.ui import (UISingle, UIMultiItem, UIFunctionOutSingle,\n",
    "                 UISingleItem, UIFunctionOutMulti, UIMulti, UIExpression,\n",
    "                 UIText, UIStatusFlag, UIParameters)\n",
    "from iotfunctions.enginelog import EngineLogging\n",
    "from iotfunctions import pipeline as pp\n",
    "from iotfunctions.pipeline import Db2DataWriter\n",
    "\n",
    "import datetime as dt\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-18T21:18:42.932 DEBUG iotfunctions.enginelog.configure_console_logging Console logging has been configured. Level = 10\n"
     ]
    }
   ],
   "source": [
    "credentials = {\n",
    "  \"tenantId\": \"AnalyticsServiceDev\",\n",
    "  \"as_api_host\": \"https://api-dev.connectedproducts.internetofthings.ibmcloud.com\",\n",
    "  \"as_api_key\": \"a-69xgm4-8bdgtvnsv4\",\n",
    "  \"as_api_token\": \"9X_tMKdupOiJ!mzaPV\",\n",
    "  \"config\" : {\n",
    "      \"objectStorageEndpoint\" : \"https://s3-api.us-geo.objectstorage.softlayer.net\",\n",
    "      \"bos_runtime_bucket\" : \"analytics-runtime-analyticsservicedev-799d2008b460\",\n",
    "      \"bos_logs_bucket\" : \"analytics-logs-analyticsservicedev-32703c52ec8b\"\n",
    "  },\n",
    "  \"objectStorage\": {\n",
    "      \"username\" : \"58ddd86b5de8468b819d385046f17033\",\n",
    "      \"password\" : \"ee0d6c5521ce9ff100f91b0e37d4eb8cc1a038b5a6d05b38\",\n",
    "      \"region\" : \"us\",\n",
    "      \"endpoint\" : \"https://s3-api.us-geo.objectstorage.softlayer.net\"\n",
    "  },\n",
    "  \"db2\": {\n",
    "    \"username\": \"bluadmin\",\n",
    "    \"password\": \"ZmM5MmE5NmZkZGZl\",\n",
    "    \"databaseName\": \"BLUDB\",\n",
    "    \"port\": 50000,\n",
    "    \"httpsUrl\": \"https://dashdb-enterprise-yp-dal13-74.services.dal.bluemix.net:50000\",\n",
    "    \"host\": \"dashdb-enterprise-yp-dal13-74.services.dal.bluemix.net\"\n",
    "  },\n",
    "  \"postgres\": None\n",
    "}\n",
    "EngineLogging.configure_console_logging(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-18T21:18:44.693 DEBUG iotfunctions.db.__init__ Unable to locate message_hub credentials. Database object created, but it will not be able interact with message hub.\n",
      "2019-09-18T21:18:44.695 DEBUG iotfunctions.db.__init__ created a CosClient object\n",
      "2019-09-18T21:18:47.504 DEBUG iotfunctions.db.__init__ Db connection established\n",
      "2019-09-18T21:18:47.507 DEBUG urllib3.connectionpool._new_conn Starting new HTTPS connection (1): api-dev.connectedproducts.internetofthings.ibmcloud.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/.local/lib/python3.7/site-packages/urllib3/connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-18T21:18:51.171 DEBUG urllib3.connectionpool._make_request https://api-dev.connectedproducts.internetofthings.ibmcloud.com:443 \"GET /api/meta/v1/AnalyticsServiceDev/entityType HTTP/1.1\" 200 None\n",
      "2019-09-18T21:18:51.692 DEBUG iotfunctions.db.http_request http request successful. status 200\n",
      "Database object:\n",
      "  Connection: db2+ibm_db://bluadmin:ZmM5MmE5NmZkZGZl@dashdb-enterprise-yp-dal13-74.services.dal.bluemix.net:50000/BLUDB;\n"
     ]
    }
   ],
   "source": [
    "db_schema = None\n",
    "db = Database(credentials=credentials)\n",
    "print (db)\n",
    "# extend iotfunctions/db.py to support postgres connection string in credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/.local/lib/python3.7/site-packages/sqlalchemy/engine/reflection.py:913: SAWarning: index key 'sqlnotapplicable' was not located in columns for table 'MARKUS_TEST_ROBOT1'\n",
      "  \"columns for table '%s'\" % (flavor, c, table_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARKUS_TEST_ROBOT1\n"
     ]
    }
   ],
   "source": [
    "table = db.get_table(\"MARKUS_TEST_ROBOT1\")\n",
    "start_ts = dt.datetime.utcnow() - dt.timedelta(days=30)\n",
    "end_ts = dt.datetime.utcnow()\n",
    "df = db.read_table(table, None, None, None, \"evt_timestamp\", start_ts, end_ts)\n",
    "print (table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  deviceid              evt_timestamp          devicetype logicalinterface_id  \\\n",
      "0    73002 2019-08-19 17:12:46.947824  Markus_Test_Robot1                       \n",
      "1    73001 2019-08-19 17:12:36.690739  Markus_Test_Robot1                       \n",
      "2    73001 2019-08-19 17:17:36.690739  Markus_Test_Robot1                       \n",
      "3    73001 2019-08-19 17:17:39.128473  Markus_Test_Robot1                       \n",
      "4    73003 2019-08-19 17:22:39.128473  Markus_Test_Robot1                       \n",
      "5    73000 2019-08-19 17:27:53.336565  Markus_Test_Robot1                       \n",
      "6    73004 2019-08-19 17:32:53.336565  Markus_Test_Robot1                       \n",
      "7    73002 2019-08-19 17:33:00.612067  Markus_Test_Robot1                       \n",
      "8    73002 2019-08-19 17:38:00.612067  Markus_Test_Robot1                       \n",
      "9    73002 2019-08-19 17:37:39.072851  Markus_Test_Robot1                       \n",
      "\n",
      "  eventtype format updated_utc plant_code     torque  tool_type        load  \\\n",
      "0        tn               None  Holtsburg  14.249186      909.0  375.252518   \n",
      "1        ee               None     Midway  11.523243      691.0  376.008552   \n",
      "2        yp               None     Midway  11.230746      907.0  376.774660   \n",
      "3        tp               None  Holtsburg  13.593918      909.0  374.104212   \n",
      "4        nn               None  Holtsburg  10.881530      909.0  373.464856   \n",
      "5        ep               None     Midway  10.824251      907.0  375.249786   \n",
      "6        ve               None     Midway  11.547486      909.0  373.398024   \n",
      "7        en               None  Holtsburg  11.704153      909.0  376.223759   \n",
      "8        tv               None     Midway  11.731020      909.0  376.105556   \n",
      "9        tt               None  Holtsburg  13.872623      909.0  374.888830   \n",
      "\n",
      "      speed  travel_time       acc  \n",
      "0  4.847994     0.802011  0.837789  \n",
      "1  3.068409     0.818241  0.395829  \n",
      "2  2.475041     0.141185  0.090898  \n",
      "3  1.658000     1.990635  1.561130  \n",
      "4  4.096755     0.547130  1.643005  \n",
      "5  2.335772     2.732748 -0.239118  \n",
      "6  2.668284     1.742484 -0.161613  \n",
      "7  3.413799     0.301977  1.904432  \n",
      "8  2.114063    -0.104269  0.628490  \n",
      "9  2.597359    -0.133905  1.264078  \n"
     ]
    }
   ],
   "source": [
    "print (df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-18T21:18:58.232 DEBUG iotfunctions.enginelog.configure_console_logging Console logging has been configured. Level = 10\n",
      "2019-09-18T21:18:58.235 DEBUG iotfunctions.metadata.__init__ Initializing new entity type using iotfunctions 2.0.3\n",
      "2019-09-18T21:18:58.237 DEBUG iotfunctions.pipeline.__init__ Starting trace\n",
      "2019-09-18T21:18:58.238 DEBUG iotfunctions.pipeline.__init__ Trace name: auto_trace_markus_testdata_20190918191858\n",
      "2019-09-18T21:18:58.240 DEBUG iotfunctions.pipeline.__init__ auto_save None\n",
      "2019-09-18T21:18:58.241 WARNING iotfunctions.metadata.__init__ No _db_schema specified in **kwargs. Usingdefault database schema.\n",
      "2019-09-18T21:18:58.243 DEBUG iotfunctions.util.categorize_args categorizing arguments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/.local/lib/python3.7/site-packages/sqlalchemy/engine/reflection.py:913: SAWarning: index key 'sqlnotapplicable' was not located in columns for table 'markus_testdata'\n",
      "  \"columns for table '%s'\" % (flavor, c, table_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-18T21:19:02.208 DEBUG iotfunctions.metadata.__init__ Initialized entity type \n",
      "EntityType:markus_testdata\n",
      "Functions:\n",
      "EntityDataGenerator at granularity None required inputs not evaluated yet outputs produced not evaluated yet on schedule None\n",
      "Granularities:\n",
      "No schedules metadata\n",
      "2019-09-18T21:19:02.210 DEBUG iotfunctions.metadata.generate_data Generating data for markus_testdata with metrics ['TestData'] and dimensions ['eventtype'] and dates []\n",
      "2019-09-18T21:19:02.236 DEBUG iotfunctions.automation.get_data Generated 6 rows of time series data from 2019-09-18 19:14:02.212462 to 2019-09-18 19:19:02.212462\n",
      "2019-09-18T21:19:03.694 INFO iotfunctions.db.write_frame Wrote data to table markus_testdata \n"
     ]
    }
   ],
   "source": [
    "# Generate 5 mins of data in table 'testdata' with a single additional column of TestData\n",
    "EngineLogging.configure_console_logging(logging.DEBUG)\n",
    "jobsettings = {'_timestamp' : 'TIMESTAMP'}\n",
    "et = metadata.EntityType('markus_testdata', db, \n",
    "                         bif.EntityDataGenerator(output_item='my_test_gen'),\n",
    "                         Column('TestData',Float()),\n",
    "                         **jobsettings)\n",
    "\n",
    "#start_date = dt.datetime.utcnow() - dt.timedelta(days=1)\n",
    "#et.exec_local_pipeline(start_ts = start_date)\n",
    "df = et.generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TBD ***** - Add stages for usage stats and write to MessageHub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/.local/lib/python3.7/site-packages/urllib3/connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "from iotfunctions import pipeline as pp\n",
    "job = pp.JobController(et)\n",
    "job.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TestData deviceid                  timestamp entity_id eventtype  \\\n",
      "0 -0.442028    73004 2019-09-18 17:08:07.081637        tt        te   \n",
      "1  1.459966    73002 2019-09-18 17:09:07.081637        ny        ee   \n",
      "2  1.521797    73001 2019-09-18 17:10:07.081637        yn        en   \n",
      "3  0.432316    73001 2019-09-18 17:11:07.081637        ty        ee   \n",
      "4  2.985946    73004 2019-09-18 17:12:07.081637        ei        ve   \n",
      "5  1.247824    73002 2019-09-18 17:13:07.081637        td        vp   \n",
      "\n",
      "  logicalinterface_id       devicetype format updated_utc  \n",
      "0                      markus_testdata               None  \n",
      "1                      markus_testdata               None  \n",
      "2                      markus_testdata               None  \n",
      "3                      markus_testdata               None  \n",
      "4                      markus_testdata               None  \n",
      "5                      markus_testdata               None  \n"
     ]
    }
   ],
   "source": [
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testdata\n",
      "(281, 8)\n"
     ]
    }
   ],
   "source": [
    "# read it back\n",
    "table = db.get_table(\"testdata\")\n",
    "start_ts = dt.datetime.utcnow() - dt.timedelta(days=1)\n",
    "end_ts = dt.datetime.utcnow()\n",
    "df_in = db.read_table(table, None, None, None, \"evt_timestamp\", start_ts, end_ts)\n",
    "print (table)\n",
    "print (df_in.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    deviceid              evt_timestamp devicetype logicalinterface_id  \\\n",
      "276    73000 2019-09-18 14:15:46.843620   testdata                       \n",
      "277    73001 2019-09-18 14:16:46.843620   testdata                       \n",
      "278    73003 2019-09-18 14:17:46.843620   testdata                       \n",
      "279    73002 2019-09-18 14:18:46.843620   testdata                       \n",
      "280    73002 2019-09-18 14:19:46.843620   testdata                       \n",
      "\n",
      "    eventtype format updated_utc  TestData  \n",
      "276        tn               None  0.790283  \n",
      "277        ne               None -0.032254  \n",
      "278        nn               None  0.501532  \n",
      "279        ee               None  0.781938  \n",
      "280        pn               None  1.688854  \n"
     ]
    }
   ],
   "source": [
    "print (df_in.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# check whether the data for the last 5 minutes is the same - must return True\n",
    "print (np.array_equal(df['TestData'].tail(5), df_in['TestData'].tail(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<iotfunctions.bif.EntityDataGenerator object at 0x7f38ddff5080>]\n"
     ]
    }
   ],
   "source": [
    "print (et._functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-18T21:19:51.184 DEBUG iotfunctions.enginelog.configure_console_logging Console logging has been configured. Level = 10\n",
      "2019-09-18T21:19:51.186 DEBUG iotfunctions.base.parse_expression expression (5*df[\"TestData\"])\n",
      "<ibm_db.IBM_DBConnection object at 0x7fcdfd284120>\n",
      "\n",
      "('73000', datetime.datetime(2019, 9, 18, 17, 49, 33, 728536), 'markus_testdata', '', 'ny', '', None, 0.0014821722656370995)\n",
      "2019-09-18T21:19:52.292 DEBUG iotfunctions.pipeline.set_payload_params Setting param writer_name on payload to <class 'iotfunctions.pipeline.Db2DataWriter'>\n",
      "2019-09-18T21:19:52.293 DEBUG iotfunctions.pipeline.set_payload_params Setting param db_connection on payload to <ibm_db.IBM_DBConnection object at 0x7fcdfd284120>\n",
      "2019-09-18T21:19:52.295 DEBUG iotfunctions.pipeline.set_payload_params Setting param _db_schema on payload to BLUADMIN\n",
      "2019-09-18T21:19:52.296 DEBUG iotfunctions.pipeline.set_payload_params Setting param save_trace_to_file on payload to True\n",
      "2019-09-18T21:19:52.298 DEBUG iotfunctions.pipeline.set_payload_params Setting param tenant_id on payload to AnalyticsServiceDev\n",
      "2019-09-18T21:19:53.098 DEBUG iotfunctions.pipeline.get_output_list The payload has candidate data items ['deviceid', 'TIMESTAMP', 'TestData']. The DataReader has no projection list\n",
      "2019-09-18T21:19:53.100 DEBUG iotfunctions.metadata.build_arg_metadata Function PythonExpression has explicit required input items  delivered by the get_input_items() method: {'TestData'}\n",
      "2019-09-18T21:19:53.101 DEBUG iotfunctions.metadata.build_arg_metadata Using output items TestOut for output_name\n",
      "2019-09-18T21:19:53.103 INFO iotfunctions.pipeline.__init__ Initialized job.\n",
      "\n",
      "2019-09-18T21:19:53.105 INFO iotfunctions.pipeline.__init__ \n",
      "Default schedule 5min \n",
      "    Schedule 5min start None:None backtracks: None \n",
      "Stages of type: get_data at grain None: \n",
      "   System generated DataReader stage: read_entity_data. Reads data from objects: markus_testdata\n",
      "Stages of type: transform at grain None: \n",
      "   PythonExpression at granularity None requires inputs {'TestData'} produces outputs ['TestOut'] on schedule None\n",
      "\n",
      "\n",
      "2019-09-18T21:19:53.108 INFO iotfunctions.enginelog.start_run_log Started logging into file run.log. Object Store path will be AnalyticsServiceDev/markus_testdata/20190918/191953_run.gz\n",
      "2019-09-18T21:19:53.109 DEBUG iotfunctions.pipeline.execute Starting execution number: 0 with execution date: 2019-09-18 19:19:53.107124\n",
      "2019-09-18T21:19:53.725 DEBUG iotfunctions.pipeline.get_next_execution_date Last execution of schedule 5min was 2019-09-18 18:06:39.382880. Next execution due 2019-09-18 18:11:39.382880.\n",
      "2019-09-18T21:19:53.726 DEBUG iotfunctions.pipeline.evaluate_schedules Schedule 5min will execute\n",
      "2019-09-18T21:19:53.728 DEBUG iotfunctions.pipeline.reset Started a new trace auto_trace_markus_testdata_20190918191953 \n",
      "2019-09-18T21:19:53.731 DEBUG iotfunctions.pipeline.reset Initiating auto save for trace\n",
      "2019-09-18T21:19:55.396 DEBUG iotfunctions.pipeline.insert Created job log entry (markus_testdata,5min): 2019-09-18 19:19:53.107124\n",
      "2019-09-18T21:19:55.398 DEBUG iotfunctions.pipeline.build_job_spec Building a job spec for schedule 5min with subsumbed schedules ['5min']\n",
      "2019-09-18T21:19:55.399 DEBUG iotfunctions.pipeline.build_stages_of_type Gathered stages of type get_data. Iteration 0: ['System generated DataReader stage: read_entity_data. Reads data from objects: markus_testdata']\n",
      "2019-09-18T21:19:55.401 DEBUG iotfunctions.pipeline.build_stages_of_type Built stages of type get_data\n",
      "2019-09-18T21:19:55.403 DEBUG iotfunctions.pipeline.build_stages_of_type Available columns: {'TIMESTAMP', 'TestData', 'deviceid'}\n",
      "2019-09-18T21:19:55.405 DEBUG iotfunctions.pipeline.build_stages_of_type Gathered stages of type transform. Iteration 0: [\"PythonExpression at granularity None requires inputs {'TestData'} produces outputs ['TestOut'] on schedule 5min\"]\n",
      "2019-09-18T21:19:55.406 DEBUG iotfunctions.pipeline.build_stages_of_type Built stages of type transform\n",
      "2019-09-18T21:19:55.408 DEBUG iotfunctions.pipeline.build_stages_of_type Available columns: {'TestOut', 'TIMESTAMP', 'TestData', 'deviceid'}\n",
      "2019-09-18T21:19:55.409 DEBUG iotfunctions.pipeline.build_job_spec Evaluating data source read_entity_data. Data items required from this source for this execution are {'TIMESTAMP', 'TestData', 'deviceid'}\n",
      "2019-09-18T21:19:55.410 DEBUG iotfunctions.pipeline.build_job_spec Build of job spec is complete.\n",
      "2019-09-18T21:19:55.411 DEBUG iotfunctions.pipeline.build_job_spec -------------------------------\n",
      "2019-09-18T21:19:55.413 DEBUG iotfunctions.pipeline.build_job_spec skipped_stages: [None]\n",
      "2019-09-18T21:19:55.414 DEBUG iotfunctions.pipeline.build_job_spec input_level:\n",
      "2019-09-18T21:19:55.415 INFO iotfunctions.pipeline.build_job_spec   System generated DataReader stage: read_entity_data. Reads data from objects: markus_testdata\n",
      "2019-09-18T21:19:55.417 INFO iotfunctions.pipeline.build_job_spec   System generated DropNull stage\n",
      "2019-09-18T21:19:55.419 INFO iotfunctions.pipeline.build_job_spec   PythonExpression at granularity None requires inputs {'TestData'} produces outputs ['TestOut'] on schedule 5min\n",
      "2019-09-18T21:19:55.421 INFO iotfunctions.pipeline.build_job_spec   System generated Db2DataWriter stage: markus_testdata_input_level\n",
      "2019-09-18T21:19:55.423 DEBUG iotfunctions.pipeline.build_job_spec -------------------------------\n",
      "TBD ***** - Add stages for usage stats and write to MessageHub\n",
      "2019-09-18T21:19:55.426 DEBUG iotfunctions.pipeline.exec_payload_method Returned default output for get_early_timestamp() on payload EntityType markus_testdata.  Default value is: None\n",
      "2019-09-18T21:19:55.428 DEBUG iotfunctions.pipeline.get_chunks The payload does not have an get_early_timestamp method or the method did not retrieve an early timestamp. Data will be retrieved in a single chunk\n",
      "2019-09-18T21:19:55.430 DEBUG iotfunctions.pipeline.write Processing as a single chunk\n",
      "2019-09-18T21:19:55.434 DEBUG iotfunctions.pipeline.write Executing stage read_entity_data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/.local/lib/python3.7/site-packages/sqlalchemy/engine/reflection.py:913: SAWarning: index key 'sqlnotapplicable' was not located in columns for table 'markus_testdata'\n",
      "  \"columns for table '%s'\" % (flavor, c, table_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-18T21:19:58.574 DEBUG iotfunctions.metadata.index_df Indexed dataframe on id, TIMESTAMP\n",
      "2019-09-18T21:19:58.577 DEBUG iotfunctions.pipeline.exec_stage_method No method get_column_map on read_entity_data returning default None. 'DataReader' object has no attribute 'get_column_map'\n",
      "2019-09-18T21:19:58.579 DEBUG iotfunctions.pipeline.merge_dataframe Merging dataframe with columns [] and index []\n",
      "2019-09-18T21:19:58.582 DEBUG iotfunctions.metadata.index_df Found existing index on id, TIMESTAMP.No need to recreate index\n",
      "2019-09-18T21:19:58.587 INFO iotfunctions.pipeline.update_last_entry Trace message:  Completed stage.\n",
      "2019-09-18T21:19:58.588 INFO iotfunctions.pipeline.update_last_entry Trace payload: {'produces_output_items': True, 'output_items': ['deviceid', 'TIMESTAMP', 'TestData'], 'discard_prior_data': False, 'merge_result': 'existing empty df with new DataFrame', 'usage': 72, 'index': ['id', 'TIMESTAMP'], 'can_proceed': True, 'updated': datetime.datetime(2019, 9, 18, 19, 19, 58, 586864), 'cumulative_usage': 72}\n",
      "2019-09-18T21:19:58.590 DEBUG iotfunctions.pipeline.write Executing stage drop_null.\n",
      "2019-09-18T21:19:58.592 DEBUG iotfunctions.pipeline.execute columns excluded when dropping null rows ['deviceid', '_timestamp', 'logicalinterface_id', 'devicetype', 'format', 'updated_utc', 'evt_timestamp']\n",
      "2019-09-18T21:19:58.593 DEBUG iotfunctions.pipeline.execute columns considered when dropping null rows ['eventtype', 'TestData']\n",
      "2019-09-18T21:19:58.595 DEBUG iotfunctions.pipeline.execute eventtype count not null: 24\n",
      "2019-09-18T21:19:58.598 DEBUG iotfunctions.pipeline.execute TestData count not null: 24\n",
      "2019-09-18T21:19:58.610 DEBUG iotfunctions.pipeline.exec_stage_method No method get_column_map on drop_null returning default None. 'DropNull' object has no attribute 'get_column_map'\n",
      "2019-09-18T21:19:58.611 INFO iotfunctions.pipeline.update_last_entry Trace message:  Completed stage.\n",
      "2019-09-18T21:19:58.613 INFO iotfunctions.pipeline.update_last_entry Trace payload: {'produces_output_items': False, 'output_items': None, 'discard_prior_data': False, 'new_data_items_info': 'Function is configured not to produce any new data items  during execution', 'can_proceed': True, 'updated': datetime.datetime(2019, 9, 18, 19, 19, 58, 611827), 'cumulative_usage': 72}\n",
      "2019-09-18T21:19:58.616 DEBUG iotfunctions.pipeline.write Executing stage PythonExpression.\n",
      "2019-09-18T21:19:58.621 DEBUG iotfunctions.pipeline.execute Input dataframe has columns ['deviceid', 'devicetype', 'logicalinterface_id', 'eventtype', 'format', 'updated_utc', 'TestData', '_timestamp'] and index ['id', 'TIMESTAMP']\n",
      "2019-09-18T21:19:58.623 DEBUG iotfunctions.pipeline.merge_dataframe Merging dataframe with columns ['deviceid', 'devicetype', 'logicalinterface_id', 'eventtype', 'format', 'updated_utc', 'TestData', '_timestamp'] and index ['id', 'TIMESTAMP']\n",
      "2019-09-18T21:19:58.625 DEBUG iotfunctions.pipeline.merge_dataframe Merging dataframe with the same index\n",
      "2019-09-18T21:19:58.636 INFO iotfunctions.pipeline.update_last_entry Trace message:  Completed stage.\n",
      "2019-09-18T21:19:58.638 INFO iotfunctions.pipeline.update_last_entry Trace payload: {'produces_output_items': True, 'output_items': ['TestOut'], 'discard_prior_data': False, 'merge_result': 'existing df with new DataFrame', 'usage': 24, 'can_proceed': True, 'updated': datetime.datetime(2019, 9, 18, 19, 19, 58, 636307), 'cumulative_usage': 96}\n",
      "2019-09-18T21:19:58.640 DEBUG iotfunctions.pipeline.write Executing stage markus_testdata_input_level.\n",
      "2019-09-18T21:19:58.642 DEBUG iotfunctions.pipeline.execute Data items will be written to database for interval (None, 2019-09-18 19:19:53.107124)\n",
      "2019-09-18T21:19:58.645 INFO iotfunctions.pipeline._get_active_cols_properties The column devicetype in data frame does not correspond to a data item.\n",
      "2019-09-18T21:19:58.646 INFO iotfunctions.pipeline._get_active_cols_properties The column logicalinterface_id in data frame does not correspond to a data item.\n",
      "2019-09-18T21:19:58.648 INFO iotfunctions.pipeline._get_active_cols_properties The column eventtype in data frame does not correspond to a data item.\n",
      "2019-09-18T21:19:58.650 INFO iotfunctions.pipeline._get_active_cols_properties The column format in data frame does not correspond to a data item.\n",
      "2019-09-18T21:19:58.653 INFO iotfunctions.pipeline._get_active_cols_properties The column updated_utc in data frame does not correspond to a data item.\n",
      "2019-09-18T21:19:58.655 INFO iotfunctions.pipeline._get_active_cols_properties The column _timestamp in data frame does not correspond to a data item.\n",
      "2019-09-18T21:19:58.657 INFO iotfunctions.pipeline._get_active_cols_properties The column TestOut in data frame does not correspond to a data item.\n",
      "2019-09-18T21:19:58.659 INFO iotfunctions.pipeline._delete_and_prepare The following data items will be written to the database: deviceid (markus_testdata, LITERAL), TestData (markus_testdata, NUMBER)\n",
      "2019-09-18T21:19:58.661 DEBUG iotfunctions.pipeline._get_table_properties Mapping between index name and index position: id -> 0, TIMESTAMP -> 1\n",
      "2019-09-18T21:19:58.663 DEBUG iotfunctions.pipeline._get_table_properties For table markus_testdata: delete statement: DELETE FROM \"BLUADMIN\".\"MARKUS_TESTDATA\" WHERE TIMESTAMP >=  ?  AND TIMESTAMP <  ?  insert statement: INSERT INTO \"BLUADMIN\".\"MARKUS_TESTDATA\" (KEY, \"ENTITY_ID\", \"TIMESTAMP\", VALUE_B, VALUE_N, VALUE_S, VALUE_T, LAST_UPDATE) VALUES (?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)\n",
      "2019-09-18T21:19:58.666 DEBUG iotfunctions.pipeline._get_table_properties For table markus_testdata: Index elements are at positions: 0, 1\n",
      "2019-09-18T21:19:58.667 INFO iotfunctions.pipeline._delete_and_prepare The data items will be written into the following tables: markus_testdata\n",
      "2019-09-18T21:19:58.669 INFO iotfunctions.pipeline._delete_and_prepare Engine(db2+ibm_db://bluadmin:***@dashdb-enterprise-yp-dal13-74.services.dal.bluemix.net:50000/BLUDB;)\n",
      "2019-09-18T21:19:58.676 WARNING iotfunctions.pipeline.write Execution of stage markus_testdata_input_level failed. \n",
      "2019-09-18T21:19:58.678 WARNING iotfunctions.pipeline.write Exception\n",
      "2019-09-18T21:19:58.679 WARNING iotfunctions.pipeline.write Supplied connection object Parameter is invalid\n",
      "2019-09-18T21:19:58.681 WARNING iotfunctions.pipeline.write Traceback (most recent call last):\n",
      "  File \"/home/markus/.local/lib/python3.7/site-packages/iotfunctions/pipeline.py\", line 2230, in execute_stages\n",
      "    end_ts=end_ts)\n",
      "  File \"/home/markus/.local/lib/python3.7/site-packages/iotfunctions/pipeline.py\", line 2386, in execute_stage\n",
      "    entities = entities)\n",
      "  File \"/home/markus/.local/lib/python3.7/site-packages/iotfunctions/pipeline.py\", line 803, in execute\n",
      "    self._delete_and_prepare(df, start_ts, end_ts)\n",
      "  File \"/home/markus/.local/lib/python3.7/site-packages/iotfunctions/pipeline.py\", line 834, in _delete_and_prepare\n",
      "    stmt_delete = ibm_db.prepare(self.db_connection, sql_delete)\n",
      "Exception: Supplied connection object Parameter is invalid\n",
      "\n",
      "2019-09-18T21:19:58.682 DEBUG iotfunctions.pipeline.exec_stage_method No method get_column_map on markus_testdata_input_level returning default None. 'Db2DataWriter' object has no attribute 'get_column_map'\n",
      "2019-09-18T21:19:58.684 INFO iotfunctions.pipeline.update_last_entry Trace message:  Completed stage.\n",
      "2019-09-18T21:19:58.686 INFO iotfunctions.pipeline.update_last_entry Trace payload: {'produces_output_items': False, 'output_items': None, 'discard_prior_data': False, 'new_data_items_info': 'Function is configured not to produce any new data items  during execution', 'can_proceed': True, 'updated': datetime.datetime(2019, 9, 18, 19, 19, 58, 684478), 'cumulative_usage': 96}\n",
      "2019-09-18T21:19:58.693 INFO iotfunctions.pipeline.write Execution complete\n",
      "2019-09-18T21:19:58.704 DEBUG urllib3.connectionpool._new_conn Starting new HTTPS connection (1): s3-api.us-geo.objectstorage.softlayer.net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/.local/lib/python3.7/site-packages/urllib3/connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-18T21:19:59.889 DEBUG urllib3.connectionpool._make_request https://s3-api.us-geo.objectstorage.softlayer.net:443 \"PUT /analytics-runtime-analyticsservicedev-799d2008b460/AnalyticsServiceDev/markus_testdata/20190918/markus_testdata_trace_191953 HTTP/1.1\" 200 0\n",
      "2019-09-18T21:19:59.892 DEBUG iotfunctions.pipeline.save Saved trace to cos AnalyticsServiceDev/markus_testdata/20190918/markus_testdata_trace_191953\n",
      "2019-09-18T21:19:59.894 DEBUG iotfunctions.pipeline.save wrote trace to file auto_trace_markus_testdata_20190918191953.json\n",
      "2019-09-18T21:20:00.544 DEBUG iotfunctions.pipeline.update Updated job log (markus_testdata,5min): 2019-09-18 19:19:53.107124\n",
      "2019-09-18T21:20:00.546 DEBUG iotfunctions.pipeline.get_next_future_execution Next scheduled execution date is 2019-09-18 19:24:53.107124\n",
      "2019-09-18T21:20:00.549 DEBUG iotfunctions.pipeline.execute Ending job normally as there are no scheduled executions  due before execution end time\n",
      "2019-09-18T21:20:00.644 DEBUG iotfunctions.pipeline.run_auto_save auto_trace_markus_testdata_20190918191953 autosave thread has stopped\n",
      "2019-09-18T21:20:00.646 DEBUG iotfunctions.pipeline.stop Stopping autosave on trace auto_trace_markus_testdata_20190918191953\n"
     ]
    }
   ],
   "source": [
    "import ibm_db\n",
    "EngineLogging.configure_console_logging(logging.DEBUG)\n",
    "et._functions = [bif.PythonExpression('5*df[\"TestData\"]','TestOut')]\n",
    "#db3 = getattr(et, 'db')\n",
    "#print (db3)\n",
    "\n",
    "db_conn = ibm_db.connect('DATABASE=BLUDB;HOSTNAME=dashdb-enterprise-yp-dal13-74.services.dal.bluemix.net;PORT=50000;PROTOCOL=TCPIP;UID=bluadmin;PWD=ZmM5MmE5NmZkZGZl;','','')\n",
    "print (db_conn)\n",
    "stmt = ibm_db.prepare(db_conn, 'SELECT * FROM \"BLUADMIN\".\"MARKUS_TESTDATA\" WHERE TIMESTAMP >=  ?  AND TIMESTAMP <  ? ')\n",
    "stmt_tuple = (dt.datetime.utcnow() - dt.timedelta(days=1), dt.datetime.utcnow())\n",
    "ibm_db.execute(stmt, stmt_tuple)\n",
    "print (ibm_db.stmt_errormsg(stmt))\n",
    "row=ibm_db.fetch_tuple(stmt)\n",
    "print (row)\n",
    "#ibm_db.close(db_conn)\n",
    "jobsettings = {'writer_name' : Db2DataWriter, 'db_connection' : db_conn, '_db_schema': 'BLUADMIN', 'save_trace_to_file' : True}\n",
    "job = pp.JobController(et, **jobsettings)\n",
    "job.execute()\n",
    "#start_date = dt.datetime.utcnow() - dt.timedelta(days=1)\n",
    "#jobsettings = {'data_writer': Db2DataWriter, 'db_connection' : db}\n",
    "#df2 = et.exec_local_pipeline(start_ts = start_date, **job_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Default schedule 5min \n",
      "    Schedule 5min start None:None backtracks: None \n",
      "Stages of type: get_data at grain None: \n",
      "   System generated DataReader stage: read_entity_data. Reads data from objects: markus_testdata\n",
      "Stages of type: transform at grain None: \n",
      "   PythonExpression at granularity None requires inputs {'TestData'} produces outputs ['TestOut'] on schedule 5min\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (job)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-18T22:08:02.261 DEBUG iotfunctions.metadata.index_df Indexed dataframe on id, TIMESTAMP\n",
      "                                 deviceid       devicetype  \\\n",
      "id    TIMESTAMP                                              \n",
      "73000 2019-09-18 17:49:33.728536    73000  markus_testdata   \n",
      "73003 2019-09-18 17:50:33.728536    73003  markus_testdata   \n",
      "      2019-09-18 17:51:33.728536    73003  markus_testdata   \n",
      "73002 2019-09-18 17:52:33.728536    73002  markus_testdata   \n",
      "73004 2019-09-18 17:53:33.728536    73004  markus_testdata   \n",
      "73003 2019-09-18 17:54:33.728536    73003  markus_testdata   \n",
      "73002 2019-09-18 17:54:31.762757    73002  markus_testdata   \n",
      "73000 2019-09-18 17:55:31.762757    73000  markus_testdata   \n",
      "73001 2019-09-18 17:56:31.762757    73001  markus_testdata   \n",
      "      2019-09-18 17:57:31.762757    73001  markus_testdata   \n",
      "73003 2019-09-18 17:58:31.762757    73003  markus_testdata   \n",
      "73004 2019-09-18 17:59:31.762757    73004  markus_testdata   \n",
      "73001 2019-09-18 18:01:27.988642    73001  markus_testdata   \n",
      "      2019-09-18 18:02:27.988642    73001  markus_testdata   \n",
      "      2019-09-18 18:03:27.988642    73001  markus_testdata   \n",
      "73000 2019-09-18 18:04:27.988642    73000  markus_testdata   \n",
      "73001 2019-09-18 18:05:27.988642    73001  markus_testdata   \n",
      "73004 2019-09-18 18:06:27.988642    73004  markus_testdata   \n",
      "73000 2019-09-18 19:14:02.212462    73000  markus_testdata   \n",
      "73004 2019-09-18 19:15:02.212462    73004  markus_testdata   \n",
      "      2019-09-18 19:16:02.212462    73004  markus_testdata   \n",
      "73001 2019-09-18 19:17:02.212462    73001  markus_testdata   \n",
      "73000 2019-09-18 19:18:02.212462    73000  markus_testdata   \n",
      "      2019-09-18 19:19:02.212462    73000  markus_testdata   \n",
      "\n",
      "                                 logicalinterface_id eventtype format  \\\n",
      "id    TIMESTAMP                                                         \n",
      "73000 2019-09-18 17:49:33.728536                            ny          \n",
      "73003 2019-09-18 17:50:33.728536                            tp          \n",
      "      2019-09-18 17:51:33.728536                            en          \n",
      "73002 2019-09-18 17:52:33.728536                            ee          \n",
      "73004 2019-09-18 17:53:33.728536                            et          \n",
      "73003 2019-09-18 17:54:33.728536                            tt          \n",
      "73002 2019-09-18 17:54:31.762757                            ev          \n",
      "73000 2019-09-18 17:55:31.762757                            et          \n",
      "73001 2019-09-18 17:56:31.762757                            tn          \n",
      "      2019-09-18 17:57:31.762757                            pv          \n",
      "73003 2019-09-18 17:58:31.762757                            ne          \n",
      "73004 2019-09-18 17:59:31.762757                            vv          \n",
      "73001 2019-09-18 18:01:27.988642                            ve          \n",
      "      2019-09-18 18:02:27.988642                            pe          \n",
      "      2019-09-18 18:03:27.988642                            py          \n",
      "73000 2019-09-18 18:04:27.988642                            ey          \n",
      "73001 2019-09-18 18:05:27.988642                            en          \n",
      "73004 2019-09-18 18:06:27.988642                            ee          \n",
      "73000 2019-09-18 19:14:02.212462                            ee          \n",
      "73004 2019-09-18 19:15:02.212462                            et          \n",
      "      2019-09-18 19:16:02.212462                            py          \n",
      "73001 2019-09-18 19:17:02.212462                            tt          \n",
      "73000 2019-09-18 19:18:02.212462                            ee          \n",
      "      2019-09-18 19:19:02.212462                            ne          \n",
      "\n",
      "                                 updated_utc  TestData  \\\n",
      "id    TIMESTAMP                                          \n",
      "73000 2019-09-18 17:49:33.728536        None  0.001482   \n",
      "73003 2019-09-18 17:50:33.728536        None  0.248096   \n",
      "      2019-09-18 17:51:33.728536        None -1.035211   \n",
      "73002 2019-09-18 17:52:33.728536        None -1.470777   \n",
      "73004 2019-09-18 17:53:33.728536        None  0.311552   \n",
      "73003 2019-09-18 17:54:33.728536        None  2.599199   \n",
      "73002 2019-09-18 17:54:31.762757        None  0.203307   \n",
      "73000 2019-09-18 17:55:31.762757        None -0.282189   \n",
      "73001 2019-09-18 17:56:31.762757        None -1.109501   \n",
      "      2019-09-18 17:57:31.762757        None  1.239154   \n",
      "73003 2019-09-18 17:58:31.762757        None  1.441583   \n",
      "73004 2019-09-18 17:59:31.762757        None -0.215852   \n",
      "73001 2019-09-18 18:01:27.988642        None  1.395841   \n",
      "      2019-09-18 18:02:27.988642        None  1.139948   \n",
      "      2019-09-18 18:03:27.988642        None -0.614075   \n",
      "73000 2019-09-18 18:04:27.988642        None  1.036912   \n",
      "73001 2019-09-18 18:05:27.988642        None -0.606308   \n",
      "73004 2019-09-18 18:06:27.988642        None -0.382685   \n",
      "73000 2019-09-18 19:14:02.212462        None  1.793904   \n",
      "73004 2019-09-18 19:15:02.212462        None -0.932474   \n",
      "      2019-09-18 19:16:02.212462        None  0.866803   \n",
      "73001 2019-09-18 19:17:02.212462        None -0.589740   \n",
      "73000 2019-09-18 19:18:02.212462        None  0.598203   \n",
      "      2019-09-18 19:19:02.212462        None -1.881895   \n",
      "\n",
      "                                                 _timestamp  \n",
      "id    TIMESTAMP                                              \n",
      "73000 2019-09-18 17:49:33.728536 2019-09-18 17:49:33.728536  \n",
      "73003 2019-09-18 17:50:33.728536 2019-09-18 17:50:33.728536  \n",
      "      2019-09-18 17:51:33.728536 2019-09-18 17:51:33.728536  \n",
      "73002 2019-09-18 17:52:33.728536 2019-09-18 17:52:33.728536  \n",
      "73004 2019-09-18 17:53:33.728536 2019-09-18 17:53:33.728536  \n",
      "73003 2019-09-18 17:54:33.728536 2019-09-18 17:54:33.728536  \n",
      "73002 2019-09-18 17:54:31.762757 2019-09-18 17:54:31.762757  \n",
      "73000 2019-09-18 17:55:31.762757 2019-09-18 17:55:31.762757  \n",
      "73001 2019-09-18 17:56:31.762757 2019-09-18 17:56:31.762757  \n",
      "      2019-09-18 17:57:31.762757 2019-09-18 17:57:31.762757  \n",
      "73003 2019-09-18 17:58:31.762757 2019-09-18 17:58:31.762757  \n",
      "73004 2019-09-18 17:59:31.762757 2019-09-18 17:59:31.762757  \n",
      "73001 2019-09-18 18:01:27.988642 2019-09-18 18:01:27.988642  \n",
      "      2019-09-18 18:02:27.988642 2019-09-18 18:02:27.988642  \n",
      "      2019-09-18 18:03:27.988642 2019-09-18 18:03:27.988642  \n",
      "73000 2019-09-18 18:04:27.988642 2019-09-18 18:04:27.988642  \n",
      "73001 2019-09-18 18:05:27.988642 2019-09-18 18:05:27.988642  \n",
      "73004 2019-09-18 18:06:27.988642 2019-09-18 18:06:27.988642  \n",
      "73000 2019-09-18 19:14:02.212462 2019-09-18 19:14:02.212462  \n",
      "73004 2019-09-18 19:15:02.212462 2019-09-18 19:15:02.212462  \n",
      "      2019-09-18 19:16:02.212462 2019-09-18 19:16:02.212462  \n",
      "73001 2019-09-18 19:17:02.212462 2019-09-18 19:17:02.212462  \n",
      "73000 2019-09-18 19:18:02.212462 2019-09-18 19:18:02.212462  \n",
      "      2019-09-18 19:19:02.212462 2019-09-18 19:19:02.212462  \n"
     ]
    }
   ],
   "source": [
    "print (et.get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
