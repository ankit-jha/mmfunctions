{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/.local/lib/python3.7/site-packages/iotfunctions/bif.py:1899: UserWarning: IoTCalcSettings is deprecated. Use entity type constants instead of a metadata provider to set entity type properties\n",
      "  warnings.warn(('IoTCalcSettings is deprecated. Use entity type constants'\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import threading\n",
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as seabornInstance\n",
    "from sqlalchemy import Column, Integer, String, Float, DateTime, Boolean, func\n",
    "from iotfunctions import base\n",
    "from iotfunctions import bif\n",
    "from iotfunctions import entity\n",
    "from iotfunctions import metadata\n",
    "from iotfunctions.metadata import EntityType\n",
    "from iotfunctions.db import Database\n",
    "from iotfunctions.enginelog import EngineLogging\n",
    "from iotfunctions import estimator, ui, base, bif\n",
    "from iotfunctions.base import BaseTransformer\n",
    "from iotfunctions.ui import (UISingle, UIMultiItem, UIFunctionOutSingle,\n",
    "                 UISingleItem, UIFunctionOutMulti, UIMulti, UIExpression,\n",
    "                 UIText, UIStatusFlag, UIParameters)\n",
    "from iotfunctions.enginelog import EngineLogging\n",
    "from iotfunctions import pipeline as pp\n",
    "from iotfunctions.pipeline import SqlAlchemyDataWriter, JobController, DataWriterFile, DataAggregator\n",
    "\n",
    "import datetime as dt\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-14T18:24:43.922 DEBUG iotfunctions.enginelog.configure_console_logging Console logging has been configured. Level = 10\n"
     ]
    }
   ],
   "source": [
    "credentials = {\n",
    "  \"tenantId\": \"AnalyticsServiceDev\",\n",
    "  \"as_api_host\": \"https://api-dev.connectedproducts.internetofthings.ibmcloud.com\",\n",
    "  \"as_api_key\": \"a-69xgm4-8bdgtvnsv4\",\n",
    "  \"as_api_token\": \"9X_tMKdupOiJ!mzaPV\",\n",
    "  \"config\" : {\n",
    "      \"objectStorageEndpoint\" : \"https://s3-api.us-geo.objectstorage.softlayer.net\",\n",
    "      \"bos_runtime_bucket\" : \"analytics-runtime-analyticsservicedev-799d2008b460\",\n",
    "      \"bos_logs_bucket\" : \"analytics-logs-analyticsservicedev-32703c52ec8b\"\n",
    "  },\n",
    "  \"objectStorage\": {\n",
    "      \"username\" : \"58ddd86b5de8468b819d385046f17033\",\n",
    "      \"password\" : \"ee0d6c5521ce9ff100f91b0e37d4eb8cc1a038b5a6d05b38\",\n",
    "      \"region\" : \"us\",\n",
    "      \"endpoint\" : \"https://s3-api.us-geo.objectstorage.softlayer.net\"\n",
    "  },\n",
    "  \"db2-nada\": {\n",
    "    \"username\": \"bluadmin\",\n",
    "    \"password\": \"ZmM5MmE5NmZkZGZl\",\n",
    "    \"databaseName\": \"BLUDB\",\n",
    "    \"port\": 50000,\n",
    "    \"httpsUrl\": \"https://dashdb-enterprise-yp-dal13-74.services.dal.bluemix.net:50000\",\n",
    "    \"host\": \"dashdb-enterprise-yp-dal13-74.services.dal.bluemix.net\"\n",
    "  },\n",
    "  \"postgresql\": {\n",
    "      \"username\": \"ibm_cloud_7d201f19_ffd0_475b_b058_26a76cec9905\",\n",
    "      \"password\": \"04cdf453585baa96c19b5e7f65c7e2762288c3c2a6043ac059283fe38a3761f1\",\n",
    "      \"region\": \"us\",\n",
    "      \"host\": \"0e899846-39a1-4b58-9b60-67cb5a0aada4.bkvfvtld0lmh0umkfi70.databases.appdomain.cloud\",\n",
    "      \"port\": 32698,\n",
    "      \"databaseName\": \"ibmclouddb\"\n",
    "  }\n",
    "}\n",
    "EngineLogging.configure_console_logging(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-14T18:24:43.929 DEBUG iotfunctions.db.__init__ Unable to locate message_hub credentials. Database object created, but it will not be able interact with message hub.\n",
      "2019-10-14T18:24:43.930 INFO iotfunctions.db.__init__ Connection string for SqlAlchemy => postgresql): postgresql://ibm_cloud_7d201f19_ffd0_475b_b058_26a76cec9905:04cdf453585baa96c19b5e7f65c7e2762288c3c2a6043ac059283fe38a3761f1@0e899846-39a1-4b58-9b60-67cb5a0aada4.bkvfvtld0lmh0umkfi70.databases.appdomain.cloud:32698/ibmclouddb\n",
      "2019-10-14T18:24:43.930 DEBUG iotfunctions.db.__init__ created a CosClient object\n",
      "2019-10-14T18:24:43.963 DEBUG iotfunctions.db.__init__ Db connection established\n",
      "2019-10-14T18:24:43.964 DEBUG urllib3.connectionpool._new_conn Starting new HTTPS connection (1): api-dev.connectedproducts.internetofthings.ibmcloud.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/.local/lib/python3.7/site-packages/urllib3/connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-14T18:24:48.624 DEBUG urllib3.connectionpool._make_request https://api-dev.connectedproducts.internetofthings.ibmcloud.com:443 \"GET /api/meta/v1/AnalyticsServiceDev/entityType HTTP/1.1\" 200 None\n",
      "2019-10-14T18:24:49.374 DEBUG iotfunctions.db.http_request http request successful. status 200\n",
      "<iotfunctions.db.Database object at 0x7f223c656eb8>\n"
     ]
    }
   ],
   "source": [
    "db_schema = None\n",
    "db = Database(credentials=credentials)\n",
    "print (db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetaData(bind=Engine(postgresql://ibm_cloud_7d201f19_ffd0_475b_b058_26a76cec9905:***@0e899846-39a1-4b58-9b60-67cb5a0aada4.bkvfvtld0lmh0umkfi70.databases.appdomain.cloud:32698/ibmclouddb))\n"
     ]
    }
   ],
   "source": [
    "print (db.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOT_TYPE_7EQAJ\n"
     ]
    }
   ],
   "source": [
    "table = db.get_table(\"IOT_TYPE_7EQAJ\")\n",
    "start_ts = dt.datetime.utcnow() - dt.timedelta(days=40)\n",
    "end_ts = dt.datetime.utcnow()\n",
    "df = db.read_table(table, None, None, None, \"rcv_timestamp_utc\", start_ts, end_ts)\n",
    "print (table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field1</th>\n",
       "      <th>field3</th>\n",
       "      <th>field2</th>\n",
       "      <th>devicetype</th>\n",
       "      <th>deviceid</th>\n",
       "      <th>logicalinterface_id</th>\n",
       "      <th>eventtype</th>\n",
       "      <th>format</th>\n",
       "      <th>rcv_timestamp_utc</th>\n",
       "      <th>updated_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>True</td>\n",
       "      <td>field2 description</td>\n",
       "      <td>TYPE_7EQAJ</td>\n",
       "      <td>DEVICE_7EQAJ</td>\n",
       "      <td>5d82799ce24d7e0022912865</td>\n",
       "      <td>tree</td>\n",
       "      <td>json</td>\n",
       "      <td>2019-09-18 18:38:36.875</td>\n",
       "      <td>2019-09-18 18:38:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>172.0</td>\n",
       "      <td>False</td>\n",
       "      <td>field2 description</td>\n",
       "      <td>TYPE_7EQAJ</td>\n",
       "      <td>DEVICE_7EQAJ</td>\n",
       "      <td>5d82799ce24d7e0022912865</td>\n",
       "      <td>tree</td>\n",
       "      <td>json</td>\n",
       "      <td>2019-09-18 18:38:37.614</td>\n",
       "      <td>2019-09-18 18:38:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>771.0</td>\n",
       "      <td>False</td>\n",
       "      <td>field2 description</td>\n",
       "      <td>TYPE_7EQAJ</td>\n",
       "      <td>DEVICE_7EQAJ</td>\n",
       "      <td>5d82799ce24d7e0022912865</td>\n",
       "      <td>tree</td>\n",
       "      <td>json</td>\n",
       "      <td>2019-09-18 18:38:39.745</td>\n",
       "      <td>2019-09-18 18:38:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>940.0</td>\n",
       "      <td>True</td>\n",
       "      <td>field2 description</td>\n",
       "      <td>TYPE_7EQAJ</td>\n",
       "      <td>DEVICE_7EQAJ</td>\n",
       "      <td>5d82799ce24d7e0022912865</td>\n",
       "      <td>tree</td>\n",
       "      <td>json</td>\n",
       "      <td>2019-09-18 18:38:38.355</td>\n",
       "      <td>2019-09-18 18:38:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>910.0</td>\n",
       "      <td>True</td>\n",
       "      <td>field2 description</td>\n",
       "      <td>TYPE_7EQAJ</td>\n",
       "      <td>DEVICE_7EQAJ</td>\n",
       "      <td>5d82799ce24d7e0022912865</td>\n",
       "      <td>tree</td>\n",
       "      <td>json</td>\n",
       "      <td>2019-09-18 18:38:38.989</td>\n",
       "      <td>2019-09-18 18:38:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>580.0</td>\n",
       "      <td>True</td>\n",
       "      <td>field2 description</td>\n",
       "      <td>TYPE_7EQAJ</td>\n",
       "      <td>DEVICE_7EQAJ</td>\n",
       "      <td>5d82799ce24d7e0022912865</td>\n",
       "      <td>tree</td>\n",
       "      <td>json</td>\n",
       "      <td>2019-09-18 18:38:40.695</td>\n",
       "      <td>2019-09-18 18:38:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>710.0</td>\n",
       "      <td>False</td>\n",
       "      <td>field2 description</td>\n",
       "      <td>TYPE_7EQAJ</td>\n",
       "      <td>DEVICE_7EQAJ</td>\n",
       "      <td>5d82799ce24d7e0022912865</td>\n",
       "      <td>tree</td>\n",
       "      <td>json</td>\n",
       "      <td>2019-09-18 18:38:41.355</td>\n",
       "      <td>2019-09-18 18:38:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>240.0</td>\n",
       "      <td>False</td>\n",
       "      <td>field2 description</td>\n",
       "      <td>TYPE_7EQAJ</td>\n",
       "      <td>DEVICE_7EQAJ</td>\n",
       "      <td>5d82799ce24d7e0022912865</td>\n",
       "      <td>tree</td>\n",
       "      <td>json</td>\n",
       "      <td>2019-09-18 18:38:42.001</td>\n",
       "      <td>2019-09-18 18:38:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>921.0</td>\n",
       "      <td>False</td>\n",
       "      <td>field2 description</td>\n",
       "      <td>TYPE_7EQAJ</td>\n",
       "      <td>DEVICE_7EQAJ</td>\n",
       "      <td>5d82799ce24d7e0022912865</td>\n",
       "      <td>tree</td>\n",
       "      <td>json</td>\n",
       "      <td>2019-09-18 18:38:42.706</td>\n",
       "      <td>2019-09-18 18:38:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>319.0</td>\n",
       "      <td>False</td>\n",
       "      <td>field2 description</td>\n",
       "      <td>TYPE_7EQAJ</td>\n",
       "      <td>DEVICE_7EQAJ</td>\n",
       "      <td>5d82799ce24d7e0022912865</td>\n",
       "      <td>tree</td>\n",
       "      <td>json</td>\n",
       "      <td>2019-09-18 18:38:43.468</td>\n",
       "      <td>2019-09-18 18:38:43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   field1  field3              field2  devicetype      deviceid  \\\n",
       "0   540.0    True  field2 description  TYPE_7EQAJ  DEVICE_7EQAJ   \n",
       "1   172.0   False  field2 description  TYPE_7EQAJ  DEVICE_7EQAJ   \n",
       "2   771.0   False  field2 description  TYPE_7EQAJ  DEVICE_7EQAJ   \n",
       "3   940.0    True  field2 description  TYPE_7EQAJ  DEVICE_7EQAJ   \n",
       "4   910.0    True  field2 description  TYPE_7EQAJ  DEVICE_7EQAJ   \n",
       "5   580.0    True  field2 description  TYPE_7EQAJ  DEVICE_7EQAJ   \n",
       "6   710.0   False  field2 description  TYPE_7EQAJ  DEVICE_7EQAJ   \n",
       "7   240.0   False  field2 description  TYPE_7EQAJ  DEVICE_7EQAJ   \n",
       "8   921.0   False  field2 description  TYPE_7EQAJ  DEVICE_7EQAJ   \n",
       "9   319.0   False  field2 description  TYPE_7EQAJ  DEVICE_7EQAJ   \n",
       "\n",
       "        logicalinterface_id eventtype format       rcv_timestamp_utc  \\\n",
       "0  5d82799ce24d7e0022912865      tree   json 2019-09-18 18:38:36.875   \n",
       "1  5d82799ce24d7e0022912865      tree   json 2019-09-18 18:38:37.614   \n",
       "2  5d82799ce24d7e0022912865      tree   json 2019-09-18 18:38:39.745   \n",
       "3  5d82799ce24d7e0022912865      tree   json 2019-09-18 18:38:38.355   \n",
       "4  5d82799ce24d7e0022912865      tree   json 2019-09-18 18:38:38.989   \n",
       "5  5d82799ce24d7e0022912865      tree   json 2019-09-18 18:38:40.695   \n",
       "6  5d82799ce24d7e0022912865      tree   json 2019-09-18 18:38:41.355   \n",
       "7  5d82799ce24d7e0022912865      tree   json 2019-09-18 18:38:42.001   \n",
       "8  5d82799ce24d7e0022912865      tree   json 2019-09-18 18:38:42.706   \n",
       "9  5d82799ce24d7e0022912865      tree   json 2019-09-18 18:38:43.468   \n",
       "\n",
       "          updated_utc  \n",
       "0 2019-09-18 18:38:36  \n",
       "1 2019-09-18 18:38:37  \n",
       "2 2019-09-18 18:38:39  \n",
       "3 2019-09-18 18:38:38  \n",
       "4 2019-09-18 18:38:38  \n",
       "5 2019-09-18 18:38:40  \n",
       "6 2019-09-18 18:38:41  \n",
       "7 2019-09-18 18:38:41  \n",
       "8 2019-09-18 18:38:42  \n",
       "9 2019-09-18 18:38:43  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-14T18:25:02.565 DEBUG iotfunctions.enginelog.configure_console_logging Console logging has been configured. Level = 10\n",
      "2019-10-14T18:25:02.566 DEBUG iotfunctions.metadata.__init__ Initializing new entity type using iotfunctions 2.0.3\n",
      "2019-10-14T18:25:02.568 DEBUG iotfunctions.pipeline.__init__ Starting trace\n",
      "2019-10-14T18:25:02.568 DEBUG iotfunctions.pipeline.__init__ Trace name: auto_trace_markus_testdata_20191014162502\n",
      "2019-10-14T18:25:02.569 DEBUG iotfunctions.pipeline.__init__ auto_save None\n",
      "2019-10-14T18:25:02.570 WARNING iotfunctions.metadata.__init__ No _db_schema specified in **kwargs. Usingdefault database schema.\n",
      "2019-10-14T18:25:02.571 DEBUG iotfunctions.util.categorize_args categorizing arguments\n",
      "2019-10-14T18:25:04.509 DEBUG iotfunctions.metadata.__init__ Initialized entity type \n",
      "EntityType:markus_testdata\n",
      "Functions:\n",
      "EntityDataGenerator at granularity None required inputs not evaluated yet outputs produced not evaluated yet on schedule None\n",
      "Granularities:\n",
      "No schedules metadata\n",
      "2019-10-14T18:25:04.510 DEBUG iotfunctions.metadata.generate_data Generating data for markus_testdata with metrics ['TestData'] and dimensions ['eventtype'] and dates []\n",
      "2019-10-14T18:25:04.524 DEBUG iotfunctions.automation.get_data Generated 6 rows of time series data from 2019-10-14 16:20:04.510816 to 2019-10-14 16:25:04.510816\n",
      "2019-10-14T18:25:06.134 INFO iotfunctions.db.write_frame Wrote data to table markus_testdata \n"
     ]
    }
   ],
   "source": [
    "# Generate 5 mins of data in table 'testdata' with a single additional column of TestData\n",
    "EngineLogging.configure_console_logging(logging.DEBUG)\n",
    "jobsettings = {}\n",
    "#jobsettings = {'_timestamp' : 'TIMESTAMP'}\n",
    "et = metadata.EntityType('markus_testdata', db, \n",
    "                         bif.EntityDataGenerator(output_item='my_test_gen'),\n",
    "                         \n",
    "                         Column('TestData',Float()),\n",
    "                         **jobsettings)\n",
    "\n",
    "#start_date = dt.datetime.utcnow() - dt.timedelta(days=1)\n",
    "#et.exec_local_pipeline(start_ts = start_date)\n",
    "df = et.generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-14T18:25:08.300 DEBUG iotfunctions.metadata.__init__ Initializing new entity type using iotfunctions 2.0.3\n",
      "2019-10-14T18:25:08.301 DEBUG iotfunctions.pipeline.__init__ Starting trace\n",
      "2019-10-14T18:25:08.301 DEBUG iotfunctions.pipeline.__init__ Trace name: auto_trace_markus_testdata_20191014162508\n",
      "2019-10-14T18:25:08.301 DEBUG iotfunctions.pipeline.__init__ auto_save None\n",
      "2019-10-14T18:25:08.302 WARNING iotfunctions.metadata.__init__ No _db_schema specified in **kwargs. Usingdefault database schema.\n",
      "2019-10-14T18:25:08.302 DEBUG iotfunctions.util.categorize_args categorizing arguments\n",
      "2019-10-14T18:25:08.303 DEBUG iotfunctions.metadata.__init__ Initialized entity type \n",
      "EntityType:markus_testdata\n",
      "Functions:\n",
      "Granularities:\n",
      "No schedules metadata\n",
      "2019-10-14T18:25:08.751 DEBUG iotfunctions.metadata.index_df Indexed dataframe on id, evt_timestamp\n",
      "\n",
      "EntityType:markus_testdata\n",
      "Functions:\n",
      "Granularities:\n",
      "No schedules metadata\n"
     ]
    }
   ],
   "source": [
    "#jobsettings = {'_timestamp' : 'TIMESTAMP'}\n",
    "jobsettings = {}\n",
    "et2 = metadata.EntityType('markus_testdata', db, \n",
    "                          Column('TestData',Float()),\n",
    "                          **jobsettings)\n",
    "et2.get_data()\n",
    "print (et2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iotfunctions import pipeline as pp\n",
    "#job = pp.JobController(et)\n",
    "#job.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TestData</th>\n",
       "      <th>deviceid</th>\n",
       "      <th>evt_timestamp</th>\n",
       "      <th>eventtype</th>\n",
       "      <th>logicalinterface_id</th>\n",
       "      <th>devicetype</th>\n",
       "      <th>format</th>\n",
       "      <th>updated_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.587941</td>\n",
       "      <td>73002</td>\n",
       "      <td>2019-10-14 16:20:04.510816</td>\n",
       "      <td>te</td>\n",
       "      <td></td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.706391</td>\n",
       "      <td>73001</td>\n",
       "      <td>2019-10-14 16:21:04.510816</td>\n",
       "      <td>te</td>\n",
       "      <td></td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.141919</td>\n",
       "      <td>73000</td>\n",
       "      <td>2019-10-14 16:22:04.510816</td>\n",
       "      <td>vv</td>\n",
       "      <td></td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.565156</td>\n",
       "      <td>73000</td>\n",
       "      <td>2019-10-14 16:23:04.510816</td>\n",
       "      <td>te</td>\n",
       "      <td></td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.197177</td>\n",
       "      <td>73002</td>\n",
       "      <td>2019-10-14 16:24:04.510816</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.202053</td>\n",
       "      <td>73002</td>\n",
       "      <td>2019-10-14 16:25:04.510816</td>\n",
       "      <td>pe</td>\n",
       "      <td></td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TestData deviceid              evt_timestamp eventtype logicalinterface_id  \\\n",
       "0  0.587941    73002 2019-10-14 16:20:04.510816        te                       \n",
       "1  1.706391    73001 2019-10-14 16:21:04.510816        te                       \n",
       "2 -0.141919    73000 2019-10-14 16:22:04.510816        vv                       \n",
       "3  1.565156    73000 2019-10-14 16:23:04.510816        te                       \n",
       "4 -1.197177    73002 2019-10-14 16:24:04.510816        en                       \n",
       "5 -0.202053    73002 2019-10-14 16:25:04.510816        pe                       \n",
       "\n",
       "        devicetype format updated_utc  \n",
       "0  markus_testdata               None  \n",
       "1  markus_testdata               None  \n",
       "2  markus_testdata               None  \n",
       "3  markus_testdata               None  \n",
       "4  markus_testdata               None  \n",
       "5  markus_testdata               None  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "markus_testdata\n",
      "(12, 8)\n"
     ]
    }
   ],
   "source": [
    "# read it back\n",
    "table = db.get_table(\"markus_testdata\")\n",
    "start_ts = dt.datetime.utcnow() - dt.timedelta(days=1)\n",
    "end_ts = dt.datetime.utcnow()\n",
    "df_in = db.read_table(table, None, None, None, \"evt_timestamp\", start_ts, end_ts)\n",
    "print (table)\n",
    "print (df_in.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deviceid</th>\n",
       "      <th>evt_timestamp</th>\n",
       "      <th>devicetype</th>\n",
       "      <th>logicalinterface_id</th>\n",
       "      <th>eventtype</th>\n",
       "      <th>format</th>\n",
       "      <th>updated_utc</th>\n",
       "      <th>TestData</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73002</td>\n",
       "      <td>2019-10-14 11:08:40.150040</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>vt</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>0.444660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73000</td>\n",
       "      <td>2019-10-14 11:09:40.150040</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>et</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>0.079940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73000</td>\n",
       "      <td>2019-10-14 11:10:40.150040</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>ye</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>-0.835057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73000</td>\n",
       "      <td>2019-10-14 11:11:40.150040</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>nn</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>0.772725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73000</td>\n",
       "      <td>2019-10-14 11:12:40.150040</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>0.900834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>73003</td>\n",
       "      <td>2019-10-14 11:13:40.150040</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>vt</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>0.225089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>73002</td>\n",
       "      <td>2019-10-14 16:20:04.510816</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>te</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>0.587941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>73001</td>\n",
       "      <td>2019-10-14 16:21:04.510816</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>te</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>1.706391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>73000</td>\n",
       "      <td>2019-10-14 16:22:04.510816</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>vv</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>-0.141919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>73000</td>\n",
       "      <td>2019-10-14 16:23:04.510816</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>te</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>1.565156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>73002</td>\n",
       "      <td>2019-10-14 16:24:04.510816</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>-1.197177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>73002</td>\n",
       "      <td>2019-10-14 16:25:04.510816</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>pe</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>-0.202053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   deviceid              evt_timestamp       devicetype logicalinterface_id  \\\n",
       "0     73002 2019-10-14 11:08:40.150040  markus_testdata                       \n",
       "1     73000 2019-10-14 11:09:40.150040  markus_testdata                       \n",
       "2     73000 2019-10-14 11:10:40.150040  markus_testdata                       \n",
       "3     73000 2019-10-14 11:11:40.150040  markus_testdata                       \n",
       "4     73000 2019-10-14 11:12:40.150040  markus_testdata                       \n",
       "5     73003 2019-10-14 11:13:40.150040  markus_testdata                       \n",
       "6     73002 2019-10-14 16:20:04.510816  markus_testdata                       \n",
       "7     73001 2019-10-14 16:21:04.510816  markus_testdata                       \n",
       "8     73000 2019-10-14 16:22:04.510816  markus_testdata                       \n",
       "9     73000 2019-10-14 16:23:04.510816  markus_testdata                       \n",
       "10    73002 2019-10-14 16:24:04.510816  markus_testdata                       \n",
       "11    73002 2019-10-14 16:25:04.510816  markus_testdata                       \n",
       "\n",
       "   eventtype format updated_utc  TestData  \n",
       "0         vt               None  0.444660  \n",
       "1         et               None  0.079940  \n",
       "2         ye               None -0.835057  \n",
       "3         nn               None  0.772725  \n",
       "4         en               None  0.900834  \n",
       "5         vt               None  0.225089  \n",
       "6         te               None  0.587941  \n",
       "7         te               None  1.706391  \n",
       "8         vv               None -0.141919  \n",
       "9         te               None  1.565156  \n",
       "10        en               None -1.197177  \n",
       "11        pe               None -0.202053  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# check whether the data for the last 5 minutes is the same - must return True\n",
    "print (np.array_equal(df['TestData'].tail(5), df_in['TestData'].tail(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<iotfunctions.bif.EntityDataGenerator object at 0x7f21f0b1a2e8>]\n"
     ]
    }
   ],
   "source": [
    "print (et._functions)\n",
    "#del (et2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#define SpectralFeatureExtract\n",
    "class SpectralFeatureExtract(BaseTransformer):\n",
    "    '''\n",
    "    Employs spectral analysis to extract features from the time series data\n",
    "    '''\n",
    "    def __init__(self, input_item, windowsize, zscore, output_item):\n",
    "        super().__init__()\n",
    "        print (input_item)\n",
    "        self.input_item = input_item\n",
    "\n",
    "        # zscore - 3 deviation above mean\n",
    "        self.zscore = zscore\n",
    "\n",
    "        # use 24 by default - must be larger than 12\n",
    "        self.windowsize = windowsize\n",
    "\n",
    "        # overlap \n",
    "        self.windowoverlap = self.windowsize - self.windowsize // 12\n",
    "\n",
    "        # assume 1 per sec for now\n",
    "        self.frame_rate = 1\n",
    "\n",
    "        self.output_item = output_item\n",
    "        \n",
    "        \n",
    "    def execute(self, df):\n",
    "\n",
    "        print (df.index.levels[0])\n",
    "        entities = np.unique(df.index.levels[0])\n",
    "        logger.info (entities)\n",
    "        \n",
    "        df[self.output_item] = 0\n",
    "        \n",
    "        for entity in entities: \n",
    "            # per entity\n",
    "            dfe = df.loc[[entity]].dropna(how='all')\n",
    "            \n",
    "            # interpolate gaps - data imputation\n",
    "            #dfe.set_index('timestamp')\n",
    "            dfe = dfe.reset_index(level=[0])\n",
    "            Size = dfe[[self.input_item]].fillna(0).to_numpy().size\n",
    "            dfe = dfe.interpolate(method='time')\n",
    "            \n",
    "            # one dimensional time series - named temperature for catchyness\n",
    "            temperature = dfe[[self.input_item]].fillna(0).to_numpy().reshape(-1,)\n",
    "            \n",
    "            print (entity, self.input_item, self.windowsize, self.zscore, self.output_item, self.windowoverlap, temperature.size)\n",
    "            \n",
    "            if temperature.size > self.windowsize:\n",
    "                print (temperature.size, self.windowsize)\n",
    "                # Fourier transform:\n",
    "                #   frequency, time, spectral density\n",
    "                freqsTS, timesTS, SxTS = signal.spectrogram(temperature, fs = self.frame_rate, window = 'hanning',\n",
    "                                                        nperseg = self.windowsize, noverlap = self.windowoverlap,\n",
    "                                                        detrend = False, scaling='spectrum')\n",
    "\n",
    "                # cut off freqencies too low to fit into the window\n",
    "                freqsTSb = (freqsTS > 2/self.windowsize).astype(int)\n",
    "                freqsTS = freqsTS * freqsTSb\n",
    "                freqsTS[freqsTS == 0] = 1 / self.windowsize\n",
    "\n",
    "                # Compute energy = frequency * spectral density over time in decibel\n",
    "                ETS = np.log10(np.dot(SxTS.T, freqsTS))\n",
    "                print (entity, ETS)\n",
    "\n",
    "                # compute zscore over the energy\n",
    "                ets_zscore = (ETS - ETS.mean())/ETS.std(ddof=0)\n",
    "                print (entity, ets_zscore)\n",
    "\n",
    "                # length of timesTS, ETS and ets_zscore is smaller than half the original\n",
    "                #   extend it to cover the full original length \n",
    "                #timesI = np.linspace(0, temperature.size-1, temperature.size)\n",
    "                timesI = np.linspace(0, Size - 1, Size)\n",
    "                zscoreI = np.interp(timesI, timesTS, ets_zscore)\n",
    "\n",
    "                # absolute zscore > 3 ---> anomaly\n",
    "                ets_zscoreb = (abs(zscoreI) > self.zscore).astype(float)\n",
    "                df.loc[[entity]][self.output_item] = zscoreI #ets_zscoreb\n",
    "\n",
    "        msg = 'SpectralAnalysisFeatureExtract'\n",
    "        self.trace_append(msg)\n",
    "        return (df)\n",
    "\n",
    "    @classmethod\n",
    "    def build_ui(cls):\n",
    "        #define arguments that behave as function inputs\n",
    "        inputs = []\n",
    "        inputs.append(ui.UISingleItem(\n",
    "                name = 'input_item',\n",
    "                datatype=float,\n",
    "                description = 'Column for feature extraction'\n",
    "                                              ))\n",
    "        inputs.append(ui.UISingle(\n",
    "                name = 'windowsize',\n",
    "                datatype=int,\n",
    "                description = 'Window size for spectral analysis - default 24'\n",
    "                                              ))\n",
    "        inputs.append(ui.UISingle(\n",
    "                name = 'zscore',\n",
    "                datatype=float,\n",
    "                description = 'Zscore to be interpreted as anomaly'\n",
    "                                              ))\n",
    "        #define arguments that behave as function outputs\n",
    "        outputs = []\n",
    "        outputs.append(ui.UIFunctionOutSingle(\n",
    "                name = 'output_item',\n",
    "                datatype=float,\n",
    "                description='zscore'\n",
    "                ))\n",
    "        return (inputs,outputs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-14T19:27:23.120 DEBUG iotfunctions.metadata.__init__ Initializing new entity type using iotfunctions 2.0.3\n",
      "2019-10-14T19:27:23.126 DEBUG iotfunctions.pipeline.__init__ Starting trace\n",
      "2019-10-14T19:27:23.127 DEBUG iotfunctions.pipeline.__init__ Trace name: auto_trace_markus_testdata_20191014172723\n",
      "2019-10-14T19:27:23.128 DEBUG iotfunctions.pipeline.__init__ auto_save None\n",
      "2019-10-14T19:27:23.128 WARNING iotfunctions.metadata.__init__ No _db_schema specified in **kwargs. Usingdefault database schema.\n",
      "2019-10-14T19:27:23.129 DEBUG iotfunctions.util.categorize_args categorizing arguments\n",
      "2019-10-14T19:27:23.130 DEBUG iotfunctions.metadata.__init__ Initialized entity type \n",
      "EntityType:markus_testdata\n",
      "Functions:\n",
      "Granularities:\n",
      "No schedules metadata\n",
      "TestData\n"
     ]
    }
   ],
   "source": [
    "\n",
    "jobsettings = {}\n",
    "et2 = metadata.EntityType('markus_testdata', db, \n",
    "                          Column('TestData',Float()),\n",
    "                          **jobsettings)\n",
    "#et2._functions = [bif.PythonExpression('5*df[\"TestData\"]','TestOut')]\n",
    "et2._functions = [SpectralFeatureExtract('TestData',12, 2.4, 'TestOut')]\n",
    "\n",
    "\n",
    "# make sure the results of the python expression is saved to the derived metrics table\n",
    "et2._data_items.append({'columnName': 'TestOut', 'columnType': 'NUMBER', 'kpiFunctionId': 22856, \n",
    "                         'kpiFunctionDto': {'output': {'name': 'TestOut'}},\n",
    "                        'name': 'TestOut', 'parentDataItemName': None, 'sourceTableName': 'dm_markus_testdata',\n",
    "                        'transient': False,'type': 'DERIVED_METRIC'})\n",
    "# map device id to entity id for the derived metrics table\n",
    "et2._data_items.append({'columnName': 'deviceid', 'columnType': 'LITERAL', 'kpiFunctionId': None,\n",
    "                         'kpiFunctionDto': {},\n",
    "                         'name': 'ENTITY_ID', 'parentDataItemName': None,'sourceTableName': 'dm_markus_testdata',\n",
    "                         'transient': False,'type': 'METRIC'})\n",
    "\n",
    "# make sure the results of the python expression is saved to the derived metrics daily table\n",
    "et2._data_items.append({'columnName': 'TestData_max', 'columnType': 'NUMBER', 'kpiFunctionId': 22856, \n",
    "                         'kpiFunctionDto': {'output': {'name': 'TestData_max'}},\n",
    "                        'name': 'TestData_max', 'parentDataItemName': None, 'sourceTableName': 'dm_markus_testdata_daily',\n",
    "                        'transient': False,'type': 'DERIVED_METRIC'})\n",
    "# map device id to entity id for the derived metrics daily table\n",
    "et2._data_items.append({'columnName': 'deviceid', 'columnType': 'LITERAL', 'kpiFunctionId': None,\n",
    "                         'kpiFunctionDto': {},\n",
    "                         'name': 'ENTITY_ID', 'parentDataItemName': None,'sourceTableName': 'dm_markus_testdata_daily',\n",
    "                         'transient': False,'type': 'METRIC'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-01 13:06:25.341693\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "dt = datetime.datetime.strptime('2019-10-01 13:06:25.341693','%Y-%m-%d %H:%M:%S.%f')\n",
    "print (dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-14T19:27:29.932 DEBUG iotfunctions.pipeline.set_payload_params Setting param writer_name on payload to <class 'iotfunctions.pipeline.SqlAlchemyDataWriter'>\n",
      "2019-10-14T19:27:29.933 DEBUG iotfunctions.pipeline.set_payload_params Setting param db on payload to <iotfunctions.db.Database object at 0x7f223c656eb8>\n",
      "2019-10-14T19:27:29.933 DEBUG iotfunctions.pipeline.set_payload_params Setting param _db_schema on payload to public\n",
      "2019-10-14T19:27:29.934 DEBUG iotfunctions.pipeline.set_payload_params Setting param save_trace_to_file on payload to True\n",
      "2019-10-14T19:27:29.934 DEBUG iotfunctions.pipeline.set_payload_params Setting param tenant_id on payload to AnalyticsServiceDev\n",
      "2019-10-14T19:27:30.804 DEBUG iotfunctions.pipeline.get_output_list The payload has candidate data items ['deviceid', 'evt_timestamp', 'TestData', 'deviceid', 'deviceid']. The DataReader has no projection list\n",
      "2019-10-14T19:27:30.806 DEBUG iotfunctions.metadata.build_arg_metadata Using input items TestData for input_item\n",
      "2019-10-14T19:27:30.807 DEBUG iotfunctions.metadata.build_arg_metadata Using output items TestOut for output_item\n",
      "2019-10-14T19:27:30.809 INFO iotfunctions.pipeline.__init__ Initialized job.\n",
      "\n",
      "2019-10-14T19:27:30.810 INFO iotfunctions.pipeline.__init__ \n",
      "Default schedule 5min \n",
      "    Schedule 5min start None:None backtracks: None \n",
      "Stages of type: get_data at grain None: \n",
      "   System generated DataReader stage: read_entity_data. Reads data from objects: markus_testdata\n",
      "Stages of type: transform at grain None: \n",
      "   SpectralFeatureExtract at granularity None requires inputs {'TestData'} produces outputs ['TestOut'] on schedule None\n",
      "\n",
      "\n",
      "2019-10-14T19:27:30.813 INFO iotfunctions.enginelog.start_run_log Started logging into file run.log. Object Store path will be AnalyticsServiceDev/markus_testdata/20191014/172730_run.gz\n",
      "2019-10-14T19:27:30.815 DEBUG iotfunctions.pipeline.execute Starting execution number: 0 with execution date: 2019-10-14 17:27:30.812478\n",
      "2019-10-14T19:27:31.257 DEBUG iotfunctions.pipeline.get_next_execution_date Last execution of schedule 5min was 2019-10-14 16:35:18.164698. Next execution due 2019-10-14 16:40:18.164698.\n",
      "2019-10-14T19:27:31.258 DEBUG iotfunctions.pipeline.evaluate_schedules Schedule 5min will execute\n",
      "2019-10-14T19:27:31.259 DEBUG iotfunctions.pipeline.reset Started a new trace auto_trace_markus_testdata_20191014172730 \n",
      "2019-10-14T19:27:31.259 DEBUG iotfunctions.pipeline.reset Initiating auto save for trace\n",
      "2019-10-14T19:27:32.125 DEBUG iotfunctions.pipeline.insert Created job log entry (markus_testdata,5min): 2019-10-14 17:27:30.812478\n",
      "2019-10-14T19:27:32.126 DEBUG iotfunctions.pipeline.build_job_spec Building a job spec for schedule 5min with subsumbed schedules ['5min']\n",
      "2019-10-14T19:27:32.127 DEBUG iotfunctions.pipeline.build_stages_of_type Gathered stages of type get_data. Iteration 0: ['System generated DataReader stage: read_entity_data. Reads data from objects: markus_testdata']\n",
      "2019-10-14T19:27:32.128 DEBUG iotfunctions.pipeline.build_stages_of_type Built stages of type get_data\n",
      "2019-10-14T19:27:32.130 DEBUG iotfunctions.pipeline.build_stages_of_type Available columns: {'deviceid', 'TestData', 'evt_timestamp'}\n",
      "2019-10-14T19:27:32.132 DEBUG iotfunctions.pipeline.build_stages_of_type Gathered stages of type transform. Iteration 0: [\"SpectralFeatureExtract at granularity None requires inputs {'TestData'} produces outputs ['TestOut'] on schedule 5min\"]\n",
      "2019-10-14T19:27:32.134 DEBUG iotfunctions.pipeline.build_stages_of_type Built stages of type transform\n",
      "2019-10-14T19:27:32.136 DEBUG iotfunctions.pipeline.build_stages_of_type Available columns: {'deviceid', 'TestOut', 'TestData', 'evt_timestamp'}\n",
      "2019-10-14T19:27:32.137 DEBUG iotfunctions.pipeline.build_job_spec Evaluating data source read_entity_data. Data items required from this source for this execution are {'deviceid', 'TestData', 'evt_timestamp'}\n",
      "2019-10-14T19:27:32.139 DEBUG iotfunctions.pipeline.build_job_spec Build of job spec is complete.\n",
      "2019-10-14T19:27:32.141 DEBUG iotfunctions.pipeline.build_job_spec -------------------------------\n",
      "2019-10-14T19:27:32.142 DEBUG iotfunctions.pipeline.build_job_spec skipped_stages: [None]\n",
      "2019-10-14T19:27:32.143 DEBUG iotfunctions.pipeline.build_job_spec input_level:\n",
      "2019-10-14T19:27:32.144 INFO iotfunctions.pipeline.build_job_spec   System generated DataReader stage: read_entity_data. Reads data from objects: markus_testdata\n",
      "2019-10-14T19:27:32.145 INFO iotfunctions.pipeline.build_job_spec   System generated DropNull stage\n",
      "2019-10-14T19:27:32.145 INFO iotfunctions.pipeline.build_job_spec   SpectralFeatureExtract at granularity None requires inputs {'TestData'} produces outputs ['TestOut'] on schedule 5min\n",
      "2019-10-14T19:27:32.146 INFO iotfunctions.pipeline.build_job_spec   System generated SqlAlchemyDataWriter stage: markus_testdata_input_level\n",
      "2019-10-14T19:27:32.147 DEBUG iotfunctions.pipeline.build_job_spec -------------------------------\n",
      "TBD ***** - Add stages for usage stats and write to MessageHub\n",
      "2019-10-14T19:27:32.148 DEBUG iotfunctions.pipeline.exec_payload_method Returned default output for get_early_timestamp() on payload EntityType markus_testdata.  Default value is: None\n",
      "2019-10-14T19:27:32.148 DEBUG iotfunctions.pipeline.get_chunks The payload does not have an get_early_timestamp method or the method did not retrieve an early timestamp. Data will be retrieved in a single chunk\n",
      "2019-10-14T19:27:32.149 DEBUG iotfunctions.pipeline.write Processing as a single chunk\n",
      "2019-10-14T19:27:32.150 DEBUG iotfunctions.pipeline.write Executing stage read_entity_data.\n",
      "2019-10-14T19:27:32.600 DEBUG iotfunctions.metadata.index_df Indexed dataframe on id, evt_timestamp\n",
      "2019-10-14T19:27:32.602 DEBUG iotfunctions.pipeline.exec_stage_method No method get_column_map on read_entity_data returning default None. 'DataReader' object has no attribute 'get_column_map'\n",
      "2019-10-14T19:27:32.603 DEBUG iotfunctions.pipeline.merge_dataframe Merging dataframe with columns [] and index []\n",
      "2019-10-14T19:27:32.605 DEBUG iotfunctions.metadata.index_df Found existing index on id, evt_timestamp.No need to recreate index\n",
      "2019-10-14T19:27:32.608 INFO iotfunctions.pipeline.update_last_entry Trace message:  Completed stage.\n",
      "2019-10-14T19:27:32.608 INFO iotfunctions.pipeline.update_last_entry Trace payload: {'produces_output_items': True, 'output_items': ['deviceid', 'evt_timestamp', 'TestData', 'deviceid', 'deviceid'], 'discard_prior_data': False, 'merge_result': 'existing empty df with new DataFrame', 'usage': 330, 'index': ['id', 'evt_timestamp'], 'can_proceed': True, 'updated': datetime.datetime(2019, 10, 14, 17, 27, 32, 607930), 'cumulative_usage': 330}\n",
      "2019-10-14T19:27:32.609 DEBUG iotfunctions.pipeline.write Executing stage drop_null.\n",
      "2019-10-14T19:27:32.610 DEBUG iotfunctions.pipeline.execute columns excluded when dropping null rows ['deviceid', '_timestamp', 'logicalinterface_id', 'devicetype', 'format', 'updated_utc', 'evt_timestamp']\n",
      "2019-10-14T19:27:32.611 DEBUG iotfunctions.pipeline.execute columns considered when dropping null rows ['eventtype', 'TestData']\n",
      "2019-10-14T19:27:32.612 DEBUG iotfunctions.pipeline.execute eventtype count not null: 66\n",
      "2019-10-14T19:27:32.613 DEBUG iotfunctions.pipeline.execute TestData count not null: 66\n",
      "2019-10-14T19:27:32.616 DEBUG iotfunctions.pipeline.exec_stage_method No method get_column_map on drop_null returning default None. 'DropNull' object has no attribute 'get_column_map'\n",
      "2019-10-14T19:27:32.617 INFO iotfunctions.pipeline.update_last_entry Trace message:  Completed stage.\n",
      "2019-10-14T19:27:32.617 INFO iotfunctions.pipeline.update_last_entry Trace payload: {'produces_output_items': False, 'output_items': None, 'discard_prior_data': False, 'new_data_items_info': 'Function is configured not to produce any new data items  during execution', 'can_proceed': True, 'updated': datetime.datetime(2019, 10, 14, 17, 27, 32, 617061), 'cumulative_usage': 330}\n",
      "2019-10-14T19:27:32.618 DEBUG iotfunctions.pipeline.write Executing stage SpectralFeatureExtract.\n",
      "Index(['73000', '73001', '73002', '73003', '73004'], dtype='object', name='id')\n",
      "2019-10-14T19:27:32.619 INFO __main__.execute ['73000' '73001' '73002' '73003' '73004']\n",
      "73000 TestData 12 2.4 TestOut 11 17\n",
      "17 12\n",
      "73000 [-1.07801579 -1.16029384 -1.08490687 -0.94111771 -0.81687811 -0.73341822]\n",
      "73000 [-0.70962671 -1.24572373 -0.75452673  0.18235668  0.99186151  1.53565899]\n",
      "73001 TestData 12 2.4 TestOut 11 10\n",
      "73002 TestData 12 2.4 TestOut 11 16\n",
      "16 12\n",
      "73002 [-0.39138485 -0.45833706 -0.51234837 -0.57254538 -0.66038294]\n",
      "73002 [ 1.37825399  0.65516312  0.07183532 -0.57829879 -1.52695364]\n",
      "73003 TestData 12 2.4 TestOut 11 11\n",
      "73004 TestData 12 2.4 TestOut 11 12\n",
      "2019-10-14T19:27:32.672 DEBUG iotfunctions.pipeline.execute Input dataframe has columns ['deviceid', 'devicetype', 'logicalinterface_id', 'eventtype', 'format', 'updated_utc', 'TestData', '_timestamp', 'TestOut'] and index ['id', 'evt_timestamp']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-14T19:27:32.672 DEBUG iotfunctions.pipeline.merge_dataframe Merging dataframe with columns ['deviceid', 'devicetype', 'logicalinterface_id', 'eventtype', 'format', 'updated_utc', 'TestData', '_timestamp', 'TestOut'] and index ['id', 'evt_timestamp']\n",
      "2019-10-14T19:27:32.673 DEBUG iotfunctions.pipeline.merge_dataframe Skipping df merge as it looks like the merge has already taken place. To bypass this check and merge set force_overwrite = True\n",
      "2019-10-14T19:27:32.674 DEBUG iotfunctions.pipeline.merge_dataframe Merging dataframe with the same index\n",
      "2019-10-14T19:27:32.675 INFO iotfunctions.pipeline.update_last_entry Trace message:  Completed stage.\n",
      "2019-10-14T19:27:32.676 INFO iotfunctions.pipeline.update_last_entry Trace payload: {'produces_output_items': True, 'output_items': ['TestOut'], 'discard_prior_data': False, 'merge_result': 'existing df with new DataFrame', 'usage': 66, 'can_proceed': True, 'updated': datetime.datetime(2019, 10, 14, 17, 27, 32, 675527), 'cumulative_usage': 396}\n",
      "2019-10-14T19:27:32.676 DEBUG iotfunctions.pipeline.write Executing stage markus_testdata_input_level.\n",
      "2019-10-14T19:27:32.677 DEBUG iotfunctions.pipeline.execute Data items will be written to database for interval (None, 2019-10-14 17:27:30.812478)\n",
      "2019-10-14T19:27:32.678 INFO iotfunctions.pipeline._get_active_cols_properties The column deviceid in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-10-14T19:27:32.679 INFO iotfunctions.pipeline._get_active_cols_properties The column devicetype in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-10-14T19:27:32.679 INFO iotfunctions.pipeline._get_active_cols_properties The column logicalinterface_id in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-10-14T19:27:32.680 INFO iotfunctions.pipeline._get_active_cols_properties The column eventtype in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-10-14T19:27:32.681 INFO iotfunctions.pipeline._get_active_cols_properties The column format in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-10-14T19:27:32.681 INFO iotfunctions.pipeline._get_active_cols_properties The column updated_utc in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-10-14T19:27:32.682 INFO iotfunctions.pipeline._get_active_cols_properties The column TestData in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-10-14T19:27:32.683 INFO iotfunctions.pipeline._get_active_cols_properties The column _timestamp in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-10-14T19:27:32.684 INFO iotfunctions.pipeline.execute The following data items will be written to the database: TestOut (dm_markus_testdata, NUMBER)\n",
      "2019-10-14T19:27:32.685 DEBUG iotfunctions.pipeline._get_table_properties Mapping between index name and index position: id -> 0, evt_timestamp -> 1\n",
      "2019-10-14T19:27:34.588 DEBUG iotfunctions.pipeline._get_table_properties For table dm_markus_testdata: delete statement: DELETE FROM dm_markus_testdata, insert statement: INSERT INTO dm_markus_testdata (entity_id, key, value_n, value_b, value_s, value_t, timestamp, last_update) VALUES (:entity_id, :key, :value_n, :value_b, :value_s, :value_t, :timestamp, :last_update)\n",
      "2019-10-14T19:27:34.589 DEBUG iotfunctions.pipeline._get_table_properties For table dm_markus_testdata: Mapping between column name and dataframe index position: ('entity_id', 0), ('timestamp', 1)\n",
      "2019-10-14T19:27:34.589 INFO iotfunctions.pipeline.execute The data items will be written into the following tables: dm_markus_testdata\n",
      "2019-10-14T19:27:34.590 DEBUG iotfunctions.pipeline._delete_old_data Deleting old data items from table dm_markus_testdata for time range [None, 2019-10-14 17:27:30.812478]\n",
      "2019-10-14T19:27:35.017 INFO iotfunctions.pipeline._delete_old_data 66 data items have been deleted from table dm_markus_testdata. Elapsed time in sec: 0.426\n",
      "entity_id 0 73004\n",
      "timestamp 1 2019-10-01 13:06:25.341693\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73004', 'timestamp': Timestamp('2019-10-01 13:06:25.341693'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73000\n",
      "timestamp 1 2019-10-01 13:07:25.341693\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73000', 'timestamp': Timestamp('2019-10-01 13:07:25.341693'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73000\n",
      "timestamp 1 2019-10-01 13:08:25.341693\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73000', 'timestamp': Timestamp('2019-10-01 13:08:25.341693'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73002\n",
      "timestamp 1 2019-10-01 13:09:25.341693\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73002', 'timestamp': Timestamp('2019-10-01 13:09:25.341693'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73004\n",
      "timestamp 1 2019-10-01 13:10:25.341693\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73004', 'timestamp': Timestamp('2019-10-01 13:10:25.341693'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73000\n",
      "timestamp 1 2019-10-01 13:11:25.341693\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73000', 'timestamp': Timestamp('2019-10-01 13:11:25.341693'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73003\n",
      "timestamp 1 2019-10-01 13:12:27.737110\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73003', 'timestamp': Timestamp('2019-10-01 13:12:27.737110'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73004\n",
      "timestamp 1 2019-10-01 13:13:27.737110\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73004', 'timestamp': Timestamp('2019-10-01 13:13:27.737110'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73003\n",
      "timestamp 1 2019-10-01 13:14:27.737110\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73003', 'timestamp': Timestamp('2019-10-01 13:14:27.737110'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73003\n",
      "timestamp 1 2019-10-01 13:15:27.737110\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73003', 'timestamp': Timestamp('2019-10-01 13:15:27.737110'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73000\n",
      "timestamp 1 2019-10-01 13:16:27.737110\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73000', 'timestamp': Timestamp('2019-10-01 13:16:27.737110'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73000\n",
      "timestamp 1 2019-10-01 13:17:27.737110\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73000', 'timestamp': Timestamp('2019-10-01 13:17:27.737110'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73000\n",
      "timestamp 1 2019-10-01 14:03:59.994715\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73000', 'timestamp': Timestamp('2019-10-01 14:03:59.994715'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73001\n",
      "timestamp 1 2019-10-01 14:04:59.994715\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73001', 'timestamp': Timestamp('2019-10-01 14:04:59.994715'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73001\n",
      "timestamp 1 2019-10-01 14:05:59.994715\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73001', 'timestamp': Timestamp('2019-10-01 14:05:59.994715'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73001\n",
      "timestamp 1 2019-10-01 14:06:59.994715\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73001', 'timestamp': Timestamp('2019-10-01 14:06:59.994715'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73002\n",
      "timestamp 1 2019-10-01 14:07:59.994715\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73002', 'timestamp': Timestamp('2019-10-01 14:07:59.994715'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73001\n",
      "timestamp 1 2019-10-01 14:08:59.994715\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73001', 'timestamp': Timestamp('2019-10-01 14:08:59.994715'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73001\n",
      "timestamp 1 2019-10-01 14:07:29.501986\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73001', 'timestamp': Timestamp('2019-10-01 14:07:29.501986'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73001\n",
      "timestamp 1 2019-10-01 14:08:29.501986\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73001', 'timestamp': Timestamp('2019-10-01 14:08:29.501986'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73002\n",
      "timestamp 1 2019-10-01 14:09:29.501986\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73002', 'timestamp': Timestamp('2019-10-01 14:09:29.501986'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73003\n",
      "timestamp 1 2019-10-01 14:10:29.501986\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73003', 'timestamp': Timestamp('2019-10-01 14:10:29.501986'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73000\n",
      "timestamp 1 2019-10-01 14:11:29.501986\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73000', 'timestamp': Timestamp('2019-10-01 14:11:29.501986'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73003\n",
      "timestamp 1 2019-10-01 14:12:29.501986\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73003', 'timestamp': Timestamp('2019-10-01 14:12:29.501986'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73002\n",
      "timestamp 1 2019-10-01 14:11:56.729650\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73002', 'timestamp': Timestamp('2019-10-01 14:11:56.729650'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73001\n",
      "timestamp 1 2019-10-01 14:12:56.729650\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73001', 'timestamp': Timestamp('2019-10-01 14:12:56.729650'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73004\n",
      "timestamp 1 2019-10-01 14:13:56.729650\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73004', 'timestamp': Timestamp('2019-10-01 14:13:56.729650'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73002\n",
      "timestamp 1 2019-10-01 14:14:56.729650\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73002', 'timestamp': Timestamp('2019-10-01 14:14:56.729650'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73004\n",
      "timestamp 1 2019-10-01 14:15:56.729650\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73004', 'timestamp': Timestamp('2019-10-01 14:15:56.729650'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73004\n",
      "timestamp 1 2019-10-01 14:16:56.729650\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73004', 'timestamp': Timestamp('2019-10-01 14:16:56.729650'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73003\n",
      "timestamp 1 2019-10-01 14:13:53.168118\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73003', 'timestamp': Timestamp('2019-10-01 14:13:53.168118'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73004\n",
      "timestamp 1 2019-10-01 14:14:53.168118\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73004', 'timestamp': Timestamp('2019-10-01 14:14:53.168118'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73003\n",
      "timestamp 1 2019-10-01 14:15:53.168118\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73003', 'timestamp': Timestamp('2019-10-01 14:15:53.168118'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73004\n",
      "timestamp 1 2019-10-01 14:16:53.168118\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73004', 'timestamp': Timestamp('2019-10-01 14:16:53.168118'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73004\n",
      "timestamp 1 2019-10-01 14:17:53.168118\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73004', 'timestamp': Timestamp('2019-10-01 14:17:53.168118'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73002\n",
      "timestamp 1 2019-10-01 14:18:53.168118\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73002', 'timestamp': Timestamp('2019-10-01 14:18:53.168118'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73002\n",
      "timestamp 1 2019-10-01 17:34:41.293838\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73002', 'timestamp': Timestamp('2019-10-01 17:34:41.293838'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73001\n",
      "timestamp 1 2019-10-01 17:35:41.293838\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73001', 'timestamp': Timestamp('2019-10-01 17:35:41.293838'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73002\n",
      "timestamp 1 2019-10-01 17:36:41.293838\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73002', 'timestamp': Timestamp('2019-10-01 17:36:41.293838'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73004\n",
      "timestamp 1 2019-10-01 17:37:41.293838\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73004', 'timestamp': Timestamp('2019-10-01 17:37:41.293838'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73000\n",
      "timestamp 1 2019-10-01 17:38:41.293838\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73000', 'timestamp': Timestamp('2019-10-01 17:38:41.293838'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73000\n",
      "timestamp 1 2019-10-01 17:39:41.293838\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73000', 'timestamp': Timestamp('2019-10-01 17:39:41.293838'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73003\n",
      "timestamp 1 2019-10-08 14:11:49.678492\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73003', 'timestamp': Timestamp('2019-10-08 14:11:49.678492'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73004\n",
      "timestamp 1 2019-10-08 14:12:49.678492\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73004', 'timestamp': Timestamp('2019-10-08 14:12:49.678492'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73002\n",
      "timestamp 1 2019-10-08 14:13:49.678492\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73002', 'timestamp': Timestamp('2019-10-08 14:13:49.678492'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73002\n",
      "timestamp 1 2019-10-08 14:14:49.678492\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73002', 'timestamp': Timestamp('2019-10-08 14:14:49.678492'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73003\n",
      "timestamp 1 2019-10-08 14:15:49.678492\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73003', 'timestamp': Timestamp('2019-10-08 14:15:49.678492'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73001\n",
      "timestamp 1 2019-10-08 14:16:49.678492\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73001', 'timestamp': Timestamp('2019-10-08 14:16:49.678492'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73000\n",
      "timestamp 1 2019-10-09 11:26:27.861150\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73000', 'timestamp': Timestamp('2019-10-09 11:26:27.861150'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73002\n",
      "timestamp 1 2019-10-09 11:27:27.861150\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73002', 'timestamp': Timestamp('2019-10-09 11:27:27.861150'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73003\n",
      "timestamp 1 2019-10-09 11:28:27.861150\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73003', 'timestamp': Timestamp('2019-10-09 11:28:27.861150'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73004\n",
      "timestamp 1 2019-10-09 11:29:27.861150\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73004', 'timestamp': Timestamp('2019-10-09 11:29:27.861150'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73002\n",
      "timestamp 1 2019-10-09 11:30:27.861150\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73002', 'timestamp': Timestamp('2019-10-09 11:30:27.861150'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73000\n",
      "timestamp 1 2019-10-09 11:31:27.861150\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73000', 'timestamp': Timestamp('2019-10-09 11:31:27.861150'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73002\n",
      "timestamp 1 2019-10-14 11:08:40.150040\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73002', 'timestamp': Timestamp('2019-10-14 11:08:40.150040'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73000\n",
      "timestamp 1 2019-10-14 11:09:40.150040\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73000', 'timestamp': Timestamp('2019-10-14 11:09:40.150040'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73000\n",
      "timestamp 1 2019-10-14 11:10:40.150040\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73000', 'timestamp': Timestamp('2019-10-14 11:10:40.150040'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73000\n",
      "timestamp 1 2019-10-14 11:11:40.150040\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73000', 'timestamp': Timestamp('2019-10-14 11:11:40.150040'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73000\n",
      "timestamp 1 2019-10-14 11:12:40.150040\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73000', 'timestamp': Timestamp('2019-10-14 11:12:40.150040'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73003\n",
      "timestamp 1 2019-10-14 11:13:40.150040\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73003', 'timestamp': Timestamp('2019-10-14 11:13:40.150040'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73002\n",
      "timestamp 1 2019-10-14 16:20:04.510816\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73002', 'timestamp': Timestamp('2019-10-14 16:20:04.510816'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73001\n",
      "timestamp 1 2019-10-14 16:21:04.510816\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73001', 'timestamp': Timestamp('2019-10-14 16:21:04.510816'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73000\n",
      "timestamp 1 2019-10-14 16:22:04.510816\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73000', 'timestamp': Timestamp('2019-10-14 16:22:04.510816'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73000\n",
      "timestamp 1 2019-10-14 16:23:04.510816\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73000', 'timestamp': Timestamp('2019-10-14 16:23:04.510816'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73002\n",
      "timestamp 1 2019-10-14 16:24:04.510816\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73002', 'timestamp': Timestamp('2019-10-14 16:24:04.510816'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73002\n",
      "timestamp 1 2019-10-14 16:25:04.510816\n",
      "Persist row: ({'KEY': 'TestOut', 'entity_id': '73002', 'timestamp': Timestamp('2019-10-14 16:25:04.510816'), 'value_b': None, 'value_n': 0.0, 'value_s': None, 'value_t': None})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-14T19:27:44.869 INFO iotfunctions.pipeline._persist_data Number of data item values persisted so far: 66 (dm_markus_testdata)\n",
      "2019-10-14T19:27:44.871 INFO iotfunctions.pipeline._persist_data Total number of persisted data item values: 66, Elapsed time in sec: 9.853, SqlAlchemy time in sec: 9.839\n",
      "2019-10-14T19:27:44.872 DEBUG iotfunctions.pipeline.exec_stage_method No method get_column_map on markus_testdata_input_level returning default None. 'SqlAlchemyDataWriter' object has no attribute 'get_column_map'\n",
      "2019-10-14T19:27:44.875 INFO iotfunctions.pipeline.update_last_entry Trace message:  Completed stage.\n",
      "2019-10-14T19:27:44.876 INFO iotfunctions.pipeline.update_last_entry Trace payload: {'produces_output_items': False, 'output_items': None, 'discard_prior_data': False, 'new_data_items_info': 'Function is configured not to produce any new data items  during execution', 'can_proceed': True, 'updated': datetime.datetime(2019, 10, 14, 17, 27, 44, 874935), 'cumulative_usage': 396}\n",
      "2019-10-14T19:27:44.879 INFO iotfunctions.pipeline.write Execution complete\n",
      "2019-10-14T19:27:44.881 DEBUG urllib3.connectionpool._new_conn Starting new HTTPS connection (1): s3-api.us-geo.objectstorage.softlayer.net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/.local/lib/python3.7/site-packages/urllib3/connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-14T19:27:46.020 DEBUG urllib3.connectionpool._make_request https://s3-api.us-geo.objectstorage.softlayer.net:443 \"PUT /analytics-runtime-analyticsservicedev-799d2008b460/AnalyticsServiceDev/markus_testdata/20191014/markus_testdata_trace_172730 HTTP/1.1\" 200 0\n",
      "2019-10-14T19:27:46.022 DEBUG iotfunctions.pipeline.save Saved trace to cos AnalyticsServiceDev/markus_testdata/20191014/markus_testdata_trace_172730\n",
      "2019-10-14T19:27:46.024 DEBUG iotfunctions.pipeline.save wrote trace to file auto_trace_markus_testdata_20191014172730.json\n",
      "2019-10-14T19:27:46.467 DEBUG iotfunctions.pipeline.update Updated job log (markus_testdata,5min): 2019-10-14 17:27:30.812478\n",
      "2019-10-14T19:27:46.469 DEBUG iotfunctions.pipeline.get_next_future_execution Next scheduled execution date is 2019-10-14 17:32:30.812478\n",
      "2019-10-14T19:27:46.469 DEBUG iotfunctions.pipeline.execute Ending job normally as there are no scheduled executions  due before execution end time\n",
      "2019-10-14T19:27:46.484 DEBUG iotfunctions.pipeline.run_auto_save auto_trace_markus_testdata_20191014172730 autosave thread has stopped\n",
      "2019-10-14T19:27:46.486 DEBUG iotfunctions.pipeline.stop Stopping autosave on trace auto_trace_markus_testdata_20191014172730\n"
     ]
    }
   ],
   "source": [
    "# dm_markus_testdate MUST exist, so run the following sql statment in DBeaver\n",
    "#     - Db2 ----\n",
    "#CREATE TABLE BLUADMIN.DM_MARKUS_TESTDATA (\n",
    "#  entity_id varchar(255),\n",
    "#  key varchar(255),\n",
    "#  value_n double,\n",
    "#  value_b boolean,\n",
    "#  value_s varchar(255),\n",
    "#  value_t timestamp,\n",
    "#  timestamp timestamp,\n",
    "#  last_update timestamp\n",
    "# );\n",
    "\n",
    "#    - Postgres ---\n",
    "#CREATE TABLE public.dm_markus_testdata (\n",
    "#  entity_id varchar(255),\n",
    "#  key varchar(255),\n",
    "#  value_n double precision,\n",
    "#  value_b boolean,\n",
    "#  value_s varchar(255),\n",
    "#  value_t timestamp,\n",
    "#  timestamp timestamp,\n",
    "#  last_update timestamp\n",
    "# );\n",
    "\n",
    "\n",
    "# The commented out version just dumps the job spec\n",
    "#jobsettings = {'writer_name' : SqlAlchemyDataWriter, 'db': db, '_db_schema': 'BLUADMIN', 'save_trace_to_file' : True}\n",
    "jobsettings = {'writer_name' : SqlAlchemyDataWriter, 'db': db, '_db_schema': 'public', 'save_trace_to_file' : True}\n",
    "job = pp.JobController(et2, **jobsettings)\n",
    "job.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (et2.get_data_items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-09T18:09:44.444 DEBUG iotfunctions.pipeline.set_payload_params Setting param writer_name on payload to <class 'iotfunctions.pipeline.SqlAlchemyDataWriter'>\n",
      "2019-10-09T18:09:44.446 DEBUG iotfunctions.pipeline.set_payload_params Setting param _db_schema on payload to public\n",
      "2019-10-09T18:09:44.447 DEBUG iotfunctions.pipeline.set_payload_params Setting param save_trace_to_file on payload to True\n",
      "2019-10-09T18:09:44.449 DEBUG iotfunctions.pipeline.set_payload_params Setting param tenant_id on payload to AnalyticsServiceDev\n",
      "2019-10-09T18:09:45.292 DEBUG iotfunctions.pipeline.get_output_list The payload has candidate data items ['deviceid', 'evt_timestamp', 'TestData', 'deviceid', 'deviceid']. The DataReader has no projection list\n",
      "2019-10-09T18:09:45.293 DEBUG iotfunctions.metadata.classify_stages Output list set was preset for function AggregateItems\n",
      "2019-10-09T18:09:45.293 DEBUG iotfunctions.metadata.classify_stages Function AggregateItems has no _metadata_params property. This property allows the stage to add properties to the entity type. Using default of {}\n",
      "2019-10-09T18:09:45.294 INFO iotfunctions.pipeline.__init__ Initialized job.\n",
      "\n",
      "2019-10-09T18:09:45.294 INFO iotfunctions.pipeline.__init__ \n",
      "Default schedule 5min \n",
      "    Schedule 5min start None:None backtracks: None \n",
      "Stages of type: get_data at grain None: \n",
      "   System generated DataReader stage: read_entity_data. Reads data from objects: markus_testdata\n",
      "Stages of type: simple_aggregate at grain daily: \n",
      "   <iotfunctions.pipeline.AggregateItems object at 0x7f0f9d396668>\n",
      "\n",
      "\n",
      "2019-10-09T18:09:45.295 INFO iotfunctions.enginelog.start_run_log Started logging into file run.log. Object Store path will be AnalyticsServiceDev/markus_testdata/20191009/160945_run.gz\n",
      "2019-10-09T18:09:45.296 DEBUG iotfunctions.pipeline.execute Starting execution number: 0 with execution date: 2019-10-09 16:09:45.295410\n",
      "2019-10-09T18:09:45.719 DEBUG iotfunctions.pipeline.get_next_execution_date Last execution of schedule 5min was 2019-10-09 15:41:12.476106. Next execution due 2019-10-09 15:46:12.476106.\n",
      "2019-10-09T18:09:45.720 DEBUG iotfunctions.pipeline.evaluate_schedules Schedule 5min will execute\n",
      "2019-10-09T18:09:45.721 DEBUG iotfunctions.pipeline.reset Started a new trace auto_trace_markus_testdata_20191009160945 \n",
      "2019-10-09T18:09:45.723 DEBUG iotfunctions.pipeline.reset Initiating auto save for trace\n",
      "2019-10-09T18:09:46.570 DEBUG iotfunctions.pipeline.insert Created job log entry (markus_testdata,5min): 2019-10-09 16:09:45.295410\n",
      "2019-10-09T18:09:46.571 DEBUG iotfunctions.pipeline.build_job_spec Building a job spec for schedule 5min with subsumbed schedules ['5min']\n",
      "2019-10-09T18:09:46.572 DEBUG iotfunctions.pipeline.build_stages_of_type Gathered stages of type get_data. Iteration 0: ['System generated DataReader stage: read_entity_data. Reads data from objects: markus_testdata']\n",
      "2019-10-09T18:09:46.573 DEBUG iotfunctions.pipeline.build_stages_of_type Built stages of type get_data\n",
      "2019-10-09T18:09:46.574 DEBUG iotfunctions.pipeline.build_stages_of_type Available columns: {'deviceid', 'evt_timestamp', 'TestData'}\n",
      "2019-10-09T18:09:46.575 DEBUG iotfunctions.pipeline.build_stages_of_type Built stages of type transform\n",
      "2019-10-09T18:09:46.576 DEBUG iotfunctions.pipeline.build_stages_of_type Available columns: {'deviceid', 'evt_timestamp', 'TestData'}\n",
      "2019-10-09T18:09:46.577 DEBUG iotfunctions.pipeline.build_job_spec Building job spec for aggregation to grain: daily\n",
      "2019-10-09T18:09:46.578 DEBUG iotfunctions.pipeline.build_job_spec Collapsed aggregation stages ['AggregateItems'] down to a single\"\n",
      "2019-10-09T18:09:46.579 DEBUG iotfunctions.pipeline.build_job_spec OrderedDict([('TestData', ['max'])])\n",
      "2019-10-09T18:09:46.581 DEBUG iotfunctions.pipeline.build_job_spec Added aggregregator to job spec: Aggregator: auto_aggregate with granularity: daily.  Aggregates TestData using ['max'] .\n",
      "2019-10-09T18:09:46.582 DEBUG iotfunctions.pipeline.build_stages_of_type Built stages of type transform\n",
      "2019-10-09T18:09:46.583 DEBUG iotfunctions.pipeline.build_stages_of_type Available columns: {'TestData_max', 'deviceid', 'TestData', 'evt_timestamp'}\n",
      "2019-10-09T18:09:46.585 DEBUG iotfunctions.pipeline.build_job_spec Completed job spec build for grain: daily\n",
      "2019-10-09T18:09:46.585 DEBUG iotfunctions.pipeline.build_job_spec Evaluating data source read_entity_data. Data items required from this source for this execution are {'deviceid', 'evt_timestamp', 'TestData'}\n",
      "2019-10-09T18:09:46.586 DEBUG iotfunctions.pipeline.build_job_spec Build of job spec is complete.\n",
      "2019-10-09T18:09:46.587 DEBUG iotfunctions.pipeline.build_job_spec -------------------------------\n",
      "2019-10-09T18:09:46.588 DEBUG iotfunctions.pipeline.build_job_spec skipped_stages: [None]\n",
      "2019-10-09T18:09:46.589 DEBUG iotfunctions.pipeline.build_job_spec input_level:\n",
      "2019-10-09T18:09:46.589 INFO iotfunctions.pipeline.build_job_spec   System generated DataReader stage: read_entity_data. Reads data from objects: markus_testdata\n",
      "2019-10-09T18:09:46.590 INFO iotfunctions.pipeline.build_job_spec   System generated DropNull stage\n",
      "2019-10-09T18:09:46.591 INFO iotfunctions.pipeline.build_job_spec   System generated SqlAlchemyDataWriter stage: markus_testdata_input_level\n",
      "2019-10-09T18:09:46.592 DEBUG iotfunctions.pipeline.build_job_spec daily:\n",
      "2019-10-09T18:09:46.594 INFO iotfunctions.pipeline.build_job_spec   Aggregator: auto_aggregate with granularity: daily.  Aggregates TestData using ['max'] .\n",
      "2019-10-09T18:09:46.596 INFO iotfunctions.pipeline.build_job_spec   System generated SqlAlchemyDataWriter stage: markus_testdata_daily\n",
      "2019-10-09T18:09:46.598 DEBUG iotfunctions.pipeline.build_job_spec -------------------------------\n",
      "TBD ***** - Add stages for usage stats and write to MessageHub\n",
      "2019-10-09T18:09:46.600 DEBUG iotfunctions.pipeline.exec_payload_method Returned default output for get_early_timestamp() on payload EntityType markus_testdata.  Default value is: None\n",
      "2019-10-09T18:09:46.602 DEBUG iotfunctions.pipeline.get_chunks The payload does not have an get_early_timestamp method or the method did not retrieve an early timestamp. Data will be retrieved in a single chunk\n",
      "2019-10-09T18:09:46.604 DEBUG iotfunctions.pipeline.write Processing as a single chunk\n",
      "2019-10-09T18:09:46.607 DEBUG iotfunctions.pipeline.write Executing stage read_entity_data.\n",
      "2019-10-09T18:09:47.034 DEBUG iotfunctions.metadata.index_df Indexed dataframe on id, evt_timestamp\n",
      "2019-10-09T18:09:47.036 DEBUG iotfunctions.pipeline.exec_stage_method No method get_column_map on read_entity_data returning default None. 'DataReader' object has no attribute 'get_column_map'\n",
      "2019-10-09T18:09:47.037 DEBUG iotfunctions.pipeline.merge_dataframe Merging dataframe with columns [] and index []\n",
      "2019-10-09T18:09:47.038 DEBUG iotfunctions.metadata.index_df Found existing index on id, evt_timestamp.No need to recreate index\n",
      "2019-10-09T18:09:47.040 INFO iotfunctions.pipeline.update_last_entry Trace message:  Completed stage.\n",
      "2019-10-09T18:09:47.041 INFO iotfunctions.pipeline.update_last_entry Trace payload: {'produces_output_items': True, 'output_items': ['deviceid', 'evt_timestamp', 'TestData', 'deviceid', 'deviceid'], 'discard_prior_data': False, 'merge_result': 'existing empty df with new DataFrame', 'usage': 270, 'index': ['id', 'evt_timestamp'], 'can_proceed': True, 'updated': datetime.datetime(2019, 10, 9, 16, 9, 47, 40200), 'cumulative_usage': 270}\n",
      "2019-10-09T18:09:47.042 DEBUG iotfunctions.pipeline.write Executing stage drop_null.\n",
      "2019-10-09T18:09:47.042 DEBUG iotfunctions.pipeline.execute columns excluded when dropping null rows ['deviceid', '_timestamp', 'logicalinterface_id', 'devicetype', 'format', 'updated_utc', 'evt_timestamp']\n",
      "2019-10-09T18:09:47.043 DEBUG iotfunctions.pipeline.execute columns considered when dropping null rows ['eventtype', 'TestData']\n",
      "2019-10-09T18:09:47.044 DEBUG iotfunctions.pipeline.execute eventtype count not null: 54\n",
      "2019-10-09T18:09:47.045 DEBUG iotfunctions.pipeline.execute TestData count not null: 54\n",
      "2019-10-09T18:09:47.053 DEBUG iotfunctions.pipeline.exec_stage_method No method get_column_map on drop_null returning default None. 'DropNull' object has no attribute 'get_column_map'\n",
      "2019-10-09T18:09:47.054 INFO iotfunctions.pipeline.update_last_entry Trace message:  Completed stage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-09T18:09:47.055 INFO iotfunctions.pipeline.update_last_entry Trace payload: {'produces_output_items': False, 'output_items': None, 'discard_prior_data': False, 'new_data_items_info': 'Function is configured not to produce any new data items  during execution', 'can_proceed': True, 'updated': datetime.datetime(2019, 10, 9, 16, 9, 47, 54561), 'cumulative_usage': 270}\n",
      "2019-10-09T18:09:47.056 DEBUG iotfunctions.pipeline.write Executing stage markus_testdata_input_level.\n",
      "2019-10-09T18:09:47.057 DEBUG iotfunctions.pipeline.execute Data items will be written to database for interval (None, 2019-10-09 16:09:45.295410)\n",
      "2019-10-09T18:09:47.058 INFO iotfunctions.pipeline._get_active_cols_properties The column deviceid in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-10-09T18:09:47.059 INFO iotfunctions.pipeline._get_active_cols_properties The column devicetype in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-10-09T18:09:47.060 INFO iotfunctions.pipeline._get_active_cols_properties The column logicalinterface_id in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-10-09T18:09:47.062 INFO iotfunctions.pipeline._get_active_cols_properties The column eventtype in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-10-09T18:09:47.067 INFO iotfunctions.pipeline._get_active_cols_properties The column format in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-10-09T18:09:47.068 INFO iotfunctions.pipeline._get_active_cols_properties The column updated_utc in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-10-09T18:09:47.074 INFO iotfunctions.pipeline._get_active_cols_properties The column TestData in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-10-09T18:09:47.076 INFO iotfunctions.pipeline._get_active_cols_properties The column _timestamp in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-10-09T18:09:47.078 INFO iotfunctions.pipeline.execute The following data items will be written to the database: \n",
      "2019-10-09T18:09:47.081 DEBUG iotfunctions.pipeline._get_table_properties Mapping between index name and index position: id -> 0, evt_timestamp -> 1\n",
      "2019-10-09T18:09:47.084 INFO iotfunctions.pipeline.execute The data items will be written into the following tables: \n",
      "2019-10-09T18:09:47.086 WARNING iotfunctions.pipeline.execute There are no data items that have to be written to the database.\n",
      "2019-10-09T18:09:47.087 DEBUG iotfunctions.pipeline.exec_stage_method No method get_column_map on markus_testdata_input_level returning default None. 'SqlAlchemyDataWriter' object has no attribute 'get_column_map'\n",
      "2019-10-09T18:09:47.088 INFO iotfunctions.pipeline.update_last_entry Trace message:  Completed stage.\n",
      "2019-10-09T18:09:47.090 INFO iotfunctions.pipeline.update_last_entry Trace payload: {'produces_output_items': False, 'output_items': None, 'discard_prior_data': False, 'new_data_items_info': 'Function is configured not to produce any new data items  during execution', 'can_proceed': True, 'updated': datetime.datetime(2019, 10, 9, 16, 9, 47, 87988), 'cumulative_usage': 270}\n",
      "2019-10-09T18:09:47.094 WARNING iotfunctions.pipeline.write Error aligning input data to granularity daily\n",
      "2019-10-09T18:09:47.096 DEBUG iotfunctions.pipeline.write Executing stage auto_aggregate.\n",
      "2019-10-09T18:09:47.130 INFO iotfunctions.pipeline.execute Completed aggregation: daily\n",
      "2019-10-09T18:09:47.131 DEBUG iotfunctions.pipeline.exec_stage_method No method get_column_map on auto_aggregate returning default None. 'DataAggregator' object has no attribute 'get_column_map'\n",
      "2019-10-09T18:09:47.132 INFO iotfunctions.pipeline.update_last_entry Trace message:  Completed stage.\n",
      "2019-10-09T18:09:47.133 INFO iotfunctions.pipeline.update_last_entry Trace payload: {'produces_output_items': True, 'output_items': ['TestData_max'], 'discard_prior_data': True, 'merge_result': 'replaced prior data', 'index': [], 'can_proceed': True, 'updated': datetime.datetime(2019, 10, 9, 16, 9, 47, 132509), 'cumulative_usage': 270}\n",
      "2019-10-09T18:09:47.143 DEBUG iotfunctions.pipeline.write Executing stage markus_testdata_daily.\n",
      "2019-10-09T18:09:47.144 DEBUG iotfunctions.pipeline.execute Data items will be written to database for interval (None, 2019-10-09 16:09:45.295410)\n",
      "2019-10-09T18:09:47.145 INFO iotfunctions.pipeline.execute The following data items will be written to the database: TestData_max (dm_markus_testdata_daily, NUMBER)\n",
      "2019-10-09T18:09:47.146 DEBUG iotfunctions.pipeline._get_table_properties Mapping between index name and index position: deviceid -> 0, evt_timestamp -> 1\n",
      "2019-10-09T18:09:48.998 DEBUG iotfunctions.pipeline._get_table_properties For table dm_markus_testdata_daily: delete statement: DELETE FROM dm_markus_testdata_daily, insert statement: INSERT INTO dm_markus_testdata_daily (entity_id, key, value_n, value_b, value_s, value_t, timestamp, last_update) VALUES (:entity_id, :key, :value_n, :value_b, :value_s, :value_t, :timestamp, :last_update)\n",
      "2019-10-09T18:09:48.999 DEBUG iotfunctions.pipeline._get_table_properties For table dm_markus_testdata_daily: Mapping between column name and dataframe index position: ('entity_id', 0), ('timestamp', 1)\n",
      "2019-10-09T18:09:49.000 INFO iotfunctions.pipeline.execute The data items will be written into the following tables: dm_markus_testdata_daily\n",
      "2019-10-09T18:09:49.001 DEBUG iotfunctions.pipeline._delete_old_data Deleting old data items from table dm_markus_testdata_daily for time range [None, 2019-10-09 16:09:45.295410]\n",
      "2019-10-09T18:09:49.428 INFO iotfunctions.pipeline._delete_old_data 9 data items have been deleted from table dm_markus_testdata_daily. Elapsed time in sec: 0.426\n",
      "entity_id 0 73004\n",
      "timestamp 1 2019-10-01 00:00:00\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73004', 'timestamp': Timestamp('2019-10-01 00:00:00', freq='D'), 'value_b': None, 'value_n': 0.953396136664403, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73004\n",
      "timestamp 1 2019-10-08 00:00:00\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73004', 'timestamp': Timestamp('2019-10-08 00:00:00', freq='D'), 'value_b': None, 'value_n': 0.927310831853966, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73004\n",
      "timestamp 1 2019-10-09 00:00:00\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73004', 'timestamp': Timestamp('2019-10-09 00:00:00', freq='D'), 'value_b': None, 'value_n': 0.01851680376576, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73000\n",
      "timestamp 1 2019-10-01 00:00:00\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73000', 'timestamp': Timestamp('2019-10-01 00:00:00', freq='D'), 'value_b': None, 'value_n': 1.52094274992196, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73000\n",
      "timestamp 1 2019-10-09 00:00:00\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73000', 'timestamp': Timestamp('2019-10-09 00:00:00', freq='D'), 'value_b': None, 'value_n': 1.13576418326954, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73002\n",
      "timestamp 1 2019-10-01 00:00:00\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73002', 'timestamp': Timestamp('2019-10-01 00:00:00', freq='D'), 'value_b': None, 'value_n': 2.12616774515587, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73002\n",
      "timestamp 1 2019-10-08 00:00:00\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73002', 'timestamp': Timestamp('2019-10-08 00:00:00', freq='D'), 'value_b': None, 'value_n': 1.19209188983732, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73002\n",
      "timestamp 1 2019-10-09 00:00:00\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73002', 'timestamp': Timestamp('2019-10-09 00:00:00', freq='D'), 'value_b': None, 'value_n': -0.163200762338566, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73003\n",
      "timestamp 1 2019-10-01 00:00:00\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73003', 'timestamp': Timestamp('2019-10-01 00:00:00', freq='D'), 'value_b': None, 'value_n': 1.39164198020907, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73003\n",
      "timestamp 1 2019-10-08 00:00:00\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73003', 'timestamp': Timestamp('2019-10-08 00:00:00', freq='D'), 'value_b': None, 'value_n': 0.0336939599395834, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73003\n",
      "timestamp 1 2019-10-09 00:00:00\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73003', 'timestamp': Timestamp('2019-10-09 00:00:00', freq='D'), 'value_b': None, 'value_n': 1.55894318586124, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73001\n",
      "timestamp 1 2019-10-01 00:00:00\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73001', 'timestamp': Timestamp('2019-10-01 00:00:00', freq='D'), 'value_b': None, 'value_n': 2.1039020307689, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73001\n",
      "timestamp 1 2019-10-08 00:00:00\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73001', 'timestamp': Timestamp('2019-10-08 00:00:00', freq='D'), 'value_b': None, 'value_n': 1.14430471557052, 'value_s': None, 'value_t': None})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-09T18:09:51.558 INFO iotfunctions.pipeline._persist_data Number of data item values persisted so far: 13 (dm_markus_testdata_daily)\n",
      "2019-10-09T18:09:51.559 INFO iotfunctions.pipeline._persist_data Total number of persisted data item values: 13, Elapsed time in sec: 2.130, SqlAlchemy time in sec: 2.123\n",
      "2019-10-09T18:09:51.561 DEBUG iotfunctions.pipeline.exec_stage_method No method get_column_map on markus_testdata_daily returning default None. 'SqlAlchemyDataWriter' object has no attribute 'get_column_map'\n",
      "[]\n",
      "2019-10-09T18:09:51.563 INFO iotfunctions.pipeline.update_last_entry Trace message:  Completed stage.\n",
      "2019-10-09T18:09:51.564 INFO iotfunctions.pipeline.update_last_entry Trace payload: {'produces_output_items': False, 'output_items': None, 'discard_prior_data': False, 'new_data_items_info': 'Function is configured not to produce any new data items  during execution', 'can_proceed': True, 'updated': datetime.datetime(2019, 10, 9, 16, 9, 51, 563062), 'cumulative_usage': 270}\n",
      "2019-10-09T18:09:51.565 INFO iotfunctions.pipeline.write Execution complete\n",
      "2019-10-09T18:09:51.570 DEBUG urllib3.connectionpool._new_conn Starting new HTTPS connection (1): s3-api.us-geo.objectstorage.softlayer.net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/.local/lib/python3.7/site-packages/urllib3/connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-09T18:09:52.683 DEBUG urllib3.connectionpool._make_request https://s3-api.us-geo.objectstorage.softlayer.net:443 \"PUT /analytics-runtime-analyticsservicedev-799d2008b460/AnalyticsServiceDev/markus_testdata/20191009/markus_testdata_trace_160945 HTTP/1.1\" 200 0\n",
      "2019-10-09T18:09:52.686 DEBUG iotfunctions.pipeline.save Saved trace to cos AnalyticsServiceDev/markus_testdata/20191009/markus_testdata_trace_160945\n",
      "2019-10-09T18:09:52.688 DEBUG iotfunctions.pipeline.save wrote trace to file auto_trace_markus_testdata_20191009160945.json\n",
      "2019-10-09T18:09:53.108 DEBUG iotfunctions.pipeline.update Updated job log (markus_testdata,5min): 2019-10-09 16:09:45.295410\n",
      "2019-10-09T18:09:53.111 DEBUG iotfunctions.pipeline.get_next_future_execution Next scheduled execution date is 2019-10-09 16:14:45.295410\n",
      "2019-10-09T18:09:53.111 DEBUG iotfunctions.pipeline.execute Ending job normally as there are no scheduled executions  due before execution end time\n",
      "2019-10-09T18:09:53.138 DEBUG iotfunctions.pipeline.run_auto_save auto_trace_markus_testdata_20191009160945 autosave thread has stopped\n",
      "2019-10-09T18:09:53.141 DEBUG iotfunctions.pipeline.stop Stopping autosave on trace auto_trace_markus_testdata_20191009160945\n"
     ]
    }
   ],
   "source": [
    "# dm_markus_testdate MUST exist, so run the following sql statment in DBeaver\n",
    "\n",
    "#  for db2\n",
    "#CREATE TABLE BLUADMIN.DM_MARKUS_TESTDATA_DAILY (\n",
    "#  entity_id varchar(255),\n",
    "#  key varchar(255),\n",
    "#  value_n double,\n",
    "#  value_b boolean,\n",
    "#  value_s varchar(255),\n",
    "#  value_t timestamp,\n",
    "#  timestamp timestamp,\n",
    "#  last_update timestamp\n",
    "# );\n",
    "\n",
    "#   for postgres\n",
    "#CREATE TABLE public.DM_MARKUS_TESTDATA_DAILY (\n",
    "#  entity_id varchar(255),\n",
    "#  key varchar(255),\n",
    "#  value_n double precision,\n",
    "#  value_b boolean,\n",
    "#  value_s varchar(255),\n",
    "#  value_t timestamp,\n",
    "#  timestamp timestamp,\n",
    "#  last_update timestamp\n",
    "# );\n",
    "\n",
    "from iotfunctions.metadata import Granularity\n",
    "from iotfunctions.pipeline import AggregateItems\n",
    "daily = Granularity(\n",
    "    name = 'daily',\n",
    "    freq = '1D',                 # pandas frequency string\n",
    "    timestamp= 'evt_timestamp',      # build time aggregations using this datetime col\n",
    "    entity_id = 'deviceid',            # aggregate by id\n",
    "    dimensions = None,\n",
    "    entity_name = None\n",
    ")\n",
    "\n",
    "#myAgg = bif.AggregateWithExpression(['TestData'],'x.max()','TestMax')\n",
    "myAgg = AggregateItems(['TestData'], 'max')\n",
    "myAgg.granularity = daily\n",
    "\n",
    "et2._functions = [myAgg]\n",
    "et2.grains = [daily]\n",
    "#et2._granularities_dict['daily'] = daily\n",
    "\n",
    "#jobsettings = {'writer_name' : SqlAlchemyDataWriter, '_db_schema': 'BLUADMIN', 'save_trace_to_file' : True}\n",
    "jobsettings = {'writer_name' : SqlAlchemyDataWriter, '_db_schema': 'public', 'save_trace_to_file' : True}\n",
    "job = pp.JobController(et2, **jobsettings)\n",
    "#job.data_writer = DataWriterFile\n",
    "job.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-01T16:25:51.201 DEBUG iotfunctions.metadata.index_df Indexed dataframe on id, evt_timestamp\n",
      "                                 deviceid       devicetype  \\\n",
      "id    evt_timestamp                                          \n",
      "73004 2019-10-01 13:06:25.341693    73004  markus_testdata   \n",
      "73000 2019-10-01 13:07:25.341693    73000  markus_testdata   \n",
      "      2019-10-01 13:08:25.341693    73000  markus_testdata   \n",
      "73002 2019-10-01 13:09:25.341693    73002  markus_testdata   \n",
      "73004 2019-10-01 13:10:25.341693    73004  markus_testdata   \n",
      "73000 2019-10-01 13:11:25.341693    73000  markus_testdata   \n",
      "73003 2019-10-01 13:12:27.737110    73003  markus_testdata   \n",
      "73004 2019-10-01 13:13:27.737110    73004  markus_testdata   \n",
      "73003 2019-10-01 13:14:27.737110    73003  markus_testdata   \n",
      "      2019-10-01 13:15:27.737110    73003  markus_testdata   \n",
      "73000 2019-10-01 13:16:27.737110    73000  markus_testdata   \n",
      "      2019-10-01 13:17:27.737110    73000  markus_testdata   \n",
      "      2019-10-01 14:03:59.994715    73000  markus_testdata   \n",
      "73001 2019-10-01 14:04:59.994715    73001  markus_testdata   \n",
      "      2019-10-01 14:05:59.994715    73001  markus_testdata   \n",
      "      2019-10-01 14:06:59.994715    73001  markus_testdata   \n",
      "73002 2019-10-01 14:07:59.994715    73002  markus_testdata   \n",
      "73001 2019-10-01 14:08:59.994715    73001  markus_testdata   \n",
      "      2019-10-01 14:07:29.501986    73001  markus_testdata   \n",
      "      2019-10-01 14:08:29.501986    73001  markus_testdata   \n",
      "73002 2019-10-01 14:09:29.501986    73002  markus_testdata   \n",
      "73003 2019-10-01 14:10:29.501986    73003  markus_testdata   \n",
      "73000 2019-10-01 14:11:29.501986    73000  markus_testdata   \n",
      "73003 2019-10-01 14:12:29.501986    73003  markus_testdata   \n",
      "73002 2019-10-01 14:11:56.729650    73002  markus_testdata   \n",
      "73001 2019-10-01 14:12:56.729650    73001  markus_testdata   \n",
      "73004 2019-10-01 14:13:56.729650    73004  markus_testdata   \n",
      "73002 2019-10-01 14:14:56.729650    73002  markus_testdata   \n",
      "73004 2019-10-01 14:15:56.729650    73004  markus_testdata   \n",
      "      2019-10-01 14:16:56.729650    73004  markus_testdata   \n",
      "73003 2019-10-01 14:13:53.168118    73003  markus_testdata   \n",
      "73004 2019-10-01 14:14:53.168118    73004  markus_testdata   \n",
      "73003 2019-10-01 14:15:53.168118    73003  markus_testdata   \n",
      "73004 2019-10-01 14:16:53.168118    73004  markus_testdata   \n",
      "      2019-10-01 14:17:53.168118    73004  markus_testdata   \n",
      "73002 2019-10-01 14:18:53.168118    73002  markus_testdata   \n",
      "\n",
      "                                 logicalinterface_id eventtype format  \\\n",
      "id    evt_timestamp                                                     \n",
      "73004 2019-10-01 13:06:25.341693                            en          \n",
      "73000 2019-10-01 13:07:25.341693                            en          \n",
      "      2019-10-01 13:08:25.341693                            yt          \n",
      "73002 2019-10-01 13:09:25.341693                            tt          \n",
      "73004 2019-10-01 13:10:25.341693                            ne          \n",
      "73000 2019-10-01 13:11:25.341693                            tt          \n",
      "73003 2019-10-01 13:12:27.737110                            ep          \n",
      "73004 2019-10-01 13:13:27.737110                            te          \n",
      "73003 2019-10-01 13:14:27.737110                            en          \n",
      "      2019-10-01 13:15:27.737110                            vt          \n",
      "73000 2019-10-01 13:16:27.737110                            et          \n",
      "      2019-10-01 13:17:27.737110                            ty          \n",
      "      2019-10-01 14:03:59.994715                            ey          \n",
      "73001 2019-10-01 14:04:59.994715                            et          \n",
      "      2019-10-01 14:05:59.994715                            et          \n",
      "      2019-10-01 14:06:59.994715                            tt          \n",
      "73002 2019-10-01 14:07:59.994715                            ye          \n",
      "73001 2019-10-01 14:08:59.994715                            ey          \n",
      "      2019-10-01 14:07:29.501986                            en          \n",
      "      2019-10-01 14:08:29.501986                            en          \n",
      "73002 2019-10-01 14:09:29.501986                            et          \n",
      "73003 2019-10-01 14:10:29.501986                            tn          \n",
      "73000 2019-10-01 14:11:29.501986                            et          \n",
      "73003 2019-10-01 14:12:29.501986                            ye          \n",
      "73002 2019-10-01 14:11:56.729650                            ny          \n",
      "73001 2019-10-01 14:12:56.729650                            ne          \n",
      "73004 2019-10-01 14:13:56.729650                            ee          \n",
      "73002 2019-10-01 14:14:56.729650                            tt          \n",
      "73004 2019-10-01 14:15:56.729650                            ny          \n",
      "      2019-10-01 14:16:56.729650                            pt          \n",
      "73003 2019-10-01 14:13:53.168118                            te          \n",
      "73004 2019-10-01 14:14:53.168118                            en          \n",
      "73003 2019-10-01 14:15:53.168118                            en          \n",
      "73004 2019-10-01 14:16:53.168118                            ee          \n",
      "      2019-10-01 14:17:53.168118                            nv          \n",
      "73002 2019-10-01 14:18:53.168118                            pe          \n",
      "\n",
      "                                 updated_utc  TestData  \\\n",
      "id    evt_timestamp                                      \n",
      "73004 2019-10-01 13:06:25.341693        None -1.012887   \n",
      "73000 2019-10-01 13:07:25.341693        None  0.640831   \n",
      "      2019-10-01 13:08:25.341693        None  0.657694   \n",
      "73002 2019-10-01 13:09:25.341693        None -2.390681   \n",
      "73004 2019-10-01 13:10:25.341693        None -0.234181   \n",
      "73000 2019-10-01 13:11:25.341693        None  0.952477   \n",
      "73003 2019-10-01 13:12:27.737110        None -0.107018   \n",
      "73004 2019-10-01 13:13:27.737110        None -0.203514   \n",
      "73003 2019-10-01 13:14:27.737110        None  0.720092   \n",
      "      2019-10-01 13:15:27.737110        None -1.720990   \n",
      "73000 2019-10-01 13:16:27.737110        None  1.520943   \n",
      "      2019-10-01 13:17:27.737110        None  0.097358   \n",
      "      2019-10-01 14:03:59.994715        None  0.521085   \n",
      "73001 2019-10-01 14:04:59.994715        None  1.059213   \n",
      "      2019-10-01 14:05:59.994715        None  0.064902   \n",
      "      2019-10-01 14:06:59.994715        None  0.762488   \n",
      "73002 2019-10-01 14:07:59.994715        None -0.715103   \n",
      "73001 2019-10-01 14:08:59.994715        None  2.103902   \n",
      "      2019-10-01 14:07:29.501986        None  0.124766   \n",
      "      2019-10-01 14:08:29.501986        None  0.014392   \n",
      "73002 2019-10-01 14:09:29.501986        None  0.359201   \n",
      "73003 2019-10-01 14:10:29.501986        None  1.391642   \n",
      "73000 2019-10-01 14:11:29.501986        None  0.431859   \n",
      "73003 2019-10-01 14:12:29.501986        None  0.889539   \n",
      "73002 2019-10-01 14:11:56.729650        None -0.150771   \n",
      "73001 2019-10-01 14:12:56.729650        None  1.145375   \n",
      "73004 2019-10-01 14:13:56.729650        None  0.144240   \n",
      "73002 2019-10-01 14:14:56.729650        None  2.126168   \n",
      "73004 2019-10-01 14:15:56.729650        None -0.211232   \n",
      "      2019-10-01 14:16:56.729650        None  0.753428   \n",
      "73003 2019-10-01 14:13:53.168118        None  0.392042   \n",
      "73004 2019-10-01 14:14:53.168118        None -0.288730   \n",
      "73003 2019-10-01 14:15:53.168118        None  1.062992   \n",
      "73004 2019-10-01 14:16:53.168118        None  0.635863   \n",
      "      2019-10-01 14:17:53.168118        None  0.953396   \n",
      "73002 2019-10-01 14:18:53.168118        None -0.189876   \n",
      "\n",
      "                                                 _timestamp  \n",
      "id    evt_timestamp                                          \n",
      "73004 2019-10-01 13:06:25.341693 2019-10-01 13:06:25.341693  \n",
      "73000 2019-10-01 13:07:25.341693 2019-10-01 13:07:25.341693  \n",
      "      2019-10-01 13:08:25.341693 2019-10-01 13:08:25.341693  \n",
      "73002 2019-10-01 13:09:25.341693 2019-10-01 13:09:25.341693  \n",
      "73004 2019-10-01 13:10:25.341693 2019-10-01 13:10:25.341693  \n",
      "73000 2019-10-01 13:11:25.341693 2019-10-01 13:11:25.341693  \n",
      "73003 2019-10-01 13:12:27.737110 2019-10-01 13:12:27.737110  \n",
      "73004 2019-10-01 13:13:27.737110 2019-10-01 13:13:27.737110  \n",
      "73003 2019-10-01 13:14:27.737110 2019-10-01 13:14:27.737110  \n",
      "      2019-10-01 13:15:27.737110 2019-10-01 13:15:27.737110  \n",
      "73000 2019-10-01 13:16:27.737110 2019-10-01 13:16:27.737110  \n",
      "      2019-10-01 13:17:27.737110 2019-10-01 13:17:27.737110  \n",
      "      2019-10-01 14:03:59.994715 2019-10-01 14:03:59.994715  \n",
      "73001 2019-10-01 14:04:59.994715 2019-10-01 14:04:59.994715  \n",
      "      2019-10-01 14:05:59.994715 2019-10-01 14:05:59.994715  \n",
      "      2019-10-01 14:06:59.994715 2019-10-01 14:06:59.994715  \n",
      "73002 2019-10-01 14:07:59.994715 2019-10-01 14:07:59.994715  \n",
      "73001 2019-10-01 14:08:59.994715 2019-10-01 14:08:59.994715  \n",
      "      2019-10-01 14:07:29.501986 2019-10-01 14:07:29.501986  \n",
      "      2019-10-01 14:08:29.501986 2019-10-01 14:08:29.501986  \n",
      "73002 2019-10-01 14:09:29.501986 2019-10-01 14:09:29.501986  \n",
      "73003 2019-10-01 14:10:29.501986 2019-10-01 14:10:29.501986  \n",
      "73000 2019-10-01 14:11:29.501986 2019-10-01 14:11:29.501986  \n",
      "73003 2019-10-01 14:12:29.501986 2019-10-01 14:12:29.501986  \n",
      "73002 2019-10-01 14:11:56.729650 2019-10-01 14:11:56.729650  \n",
      "73001 2019-10-01 14:12:56.729650 2019-10-01 14:12:56.729650  \n",
      "73004 2019-10-01 14:13:56.729650 2019-10-01 14:13:56.729650  \n",
      "73002 2019-10-01 14:14:56.729650 2019-10-01 14:14:56.729650  \n",
      "73004 2019-10-01 14:15:56.729650 2019-10-01 14:15:56.729650  \n",
      "      2019-10-01 14:16:56.729650 2019-10-01 14:16:56.729650  \n",
      "73003 2019-10-01 14:13:53.168118 2019-10-01 14:13:53.168118  \n",
      "73004 2019-10-01 14:14:53.168118 2019-10-01 14:14:53.168118  \n",
      "73003 2019-10-01 14:15:53.168118 2019-10-01 14:15:53.168118  \n",
      "73004 2019-10-01 14:16:53.168118 2019-10-01 14:16:53.168118  \n",
      "      2019-10-01 14:17:53.168118 2019-10-01 14:17:53.168118  \n",
      "73002 2019-10-01 14:18:53.168118 2019-10-01 14:18:53.168118  \n"
     ]
    }
   ],
   "source": [
    "print (et2.get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
