{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/.local/lib/python3.7/site-packages/iotfunctions/bif.py:1899: UserWarning: IoTCalcSettings is deprecated. Use entity type constants instead of a metadata provider to set entity type properties\n",
      "  warnings.warn(('IoTCalcSettings is deprecated. Use entity type constants'\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SqlAlchemyDataWriter' from 'iotfunctions.pipeline' (/home/markus/.local/lib/python3.7/site-packages/iotfunctions/pipeline.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a1d3c33abe08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0miotfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menginelog\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEngineLogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0miotfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0miotfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSqlAlchemyDataWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJobController\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataWriterFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataAggregator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'SqlAlchemyDataWriter' from 'iotfunctions.pipeline' (/home/markus/.local/lib/python3.7/site-packages/iotfunctions/pipeline.py)"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import threading\n",
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as seabornInstance\n",
    "from sqlalchemy import Column, Integer, String, Float, DateTime, Boolean, func\n",
    "from iotfunctions import base\n",
    "from iotfunctions import bif\n",
    "from iotfunctions import entity\n",
    "from iotfunctions import metadata\n",
    "from iotfunctions.metadata import EntityType\n",
    "from iotfunctions.db import Database\n",
    "from iotfunctions.enginelog import EngineLogging\n",
    "from iotfunctions import estimator, ui, base, bif\n",
    "from iotfunctions.base import BaseTransformer\n",
    "from iotfunctions.ui import (UISingle, UIMultiItem, UIFunctionOutSingle,\n",
    "                 UISingleItem, UIFunctionOutMulti, UIMulti, UIExpression,\n",
    "                 UIText, UIStatusFlag, UIParameters)\n",
    "from iotfunctions.enginelog import EngineLogging\n",
    "from iotfunctions import pipeline as pp\n",
    "from iotfunctions.pipeline import SqlAlchemyDataWriter, JobController, DataWriterFile, DataAggregator\n",
    "\n",
    "import datetime as dt\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-15T14:50:56.254 DEBUG iotfunctions.enginelog.configure_console_logging Console logging has been configured. Level = 10\n"
     ]
    }
   ],
   "source": [
    "credentials = {\n",
    "  \"tenantId\": \"AnalyticsServiceDev\",\n",
    "  \"as_api_host\": \"https://api-dev.connectedproducts.internetofthings.ibmcloud.com\",\n",
    "  \"as_api_key\": \"a-69xgm4-8bdgtvnsv4\",\n",
    "  \"as_api_token\": \"9X_tMKdupOiJ!mzaPV\",\n",
    "  \"config\" : {\n",
    "      \"objectStorageEndpoint\" : \"https://s3-api.us-geo.objectstorage.softlayer.net\",\n",
    "      \"bos_runtime_bucket\" : \"analytics-runtime-analyticsservicedev-799d2008b460\",\n",
    "      \"bos_logs_bucket\" : \"analytics-logs-analyticsservicedev-32703c52ec8b\"\n",
    "  },\n",
    "  \"objectStorage\": {\n",
    "      \"username\" : \"58ddd86b5de8468b819d385046f17033\",\n",
    "      \"password\" : \"ee0d6c5521ce9ff100f91b0e37d4eb8cc1a038b5a6d05b38\",\n",
    "      \"region\" : \"us\",\n",
    "      \"endpoint\" : \"https://s3-api.us-geo.objectstorage.softlayer.net\"\n",
    "  },\n",
    "  \"db2-nada\": {\n",
    "    \"username\": \"bluadmin\",\n",
    "    \"password\": \"ZmM5MmE5NmZkZGZl\",\n",
    "    \"databaseName\": \"BLUDB\",\n",
    "    \"port\": 50000,\n",
    "    \"httpsUrl\": \"https://dashdb-enterprise-yp-dal13-74.services.dal.bluemix.net:50000\",\n",
    "    \"host\": \"dashdb-enterprise-yp-dal13-74.services.dal.bluemix.net\"\n",
    "  },\n",
    "  \"postgresql\": {\n",
    "      \"username\": \"ibm_cloud_7d201f19_ffd0_475b_b058_26a76cec9905\",\n",
    "      \"password\": \"04cdf453585baa96c19b5e7f65c7e2762288c3c2a6043ac059283fe38a3761f1\",\n",
    "      \"region\": \"us\",\n",
    "      \"host\": \"0e899846-39a1-4b58-9b60-67cb5a0aada4.bkvfvtld0lmh0umkfi70.databases.appdomain.cloud\",\n",
    "      \"port\": 32698,\n",
    "      \"databaseName\": \"ibmclouddb\"\n",
    "  }\n",
    "}\n",
    "EngineLogging.configure_console_logging(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-15T14:50:57.283 DEBUG iotfunctions.db.__init__ Unable to locate message_hub credentials. Database object created, but it will not be able interact with message hub.\n",
      "2019-10-15T14:50:57.284 INFO iotfunctions.db.__init__ Connection string for SqlAlchemy => postgresql): postgresql+psycopg2://ibm_cloud_7d201f19_ffd0_475b_b058_26a76cec9905:04cdf453585baa96c19b5e7f65c7e2762288c3c2a6043ac059283fe38a3761f1@0e899846-39a1-4b58-9b60-67cb5a0aada4.bkvfvtld0lmh0umkfi70.databases.appdomain.cloud:32698/ibmclouddb\n",
      "2019-10-15T14:50:57.285 DEBUG iotfunctions.db.__init__ created a CosClient object\n",
      "2019-10-15T14:50:57.307 DEBUG iotfunctions.db.__init__ Db connection established\n",
      "2019-10-15T14:50:57.308 DEBUG urllib3.connectionpool._new_conn Starting new HTTPS connection (1): api-dev.connectedproducts.internetofthings.ibmcloud.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/.local/lib/python3.7/site-packages/urllib3/connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-15T14:51:02.090 DEBUG urllib3.connectionpool._make_request https://api-dev.connectedproducts.internetofthings.ibmcloud.com:443 \"GET /api/meta/v1/AnalyticsServiceDev/entityType HTTP/1.1\" 200 None\n",
      "2019-10-15T14:51:02.705 DEBUG iotfunctions.db.http_request http request successful. status 200\n",
      "<iotfunctions.db.Database object at 0x7fec3cd4ae48>\n"
     ]
    }
   ],
   "source": [
    "db_schema = None\n",
    "db = Database(credentials=credentials)\n",
    "print (db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetaData(bind=Engine(postgresql+psycopg2://ibm_cloud_7d201f19_ffd0_475b_b058_26a76cec9905:***@0e899846-39a1-4b58-9b60-67cb5a0aada4.bkvfvtld0lmh0umkfi70.databases.appdomain.cloud:32698/ibmclouddb))\n"
     ]
    }
   ],
   "source": [
    "print (db.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOT_TYPE_7EQAJ\n"
     ]
    }
   ],
   "source": [
    "table = db.get_table(\"IOT_TYPE_7EQAJ\")\n",
    "start_ts = dt.datetime.utcnow() - dt.timedelta(days=40)\n",
    "end_ts = dt.datetime.utcnow()\n",
    "df = db.read_table(table, None, None, None, \"rcv_timestamp_utc\", start_ts, end_ts)\n",
    "print (table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field1</th>\n",
       "      <th>field3</th>\n",
       "      <th>field2</th>\n",
       "      <th>devicetype</th>\n",
       "      <th>deviceid</th>\n",
       "      <th>logicalinterface_id</th>\n",
       "      <th>eventtype</th>\n",
       "      <th>format</th>\n",
       "      <th>rcv_timestamp_utc</th>\n",
       "      <th>updated_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>True</td>\n",
       "      <td>field2 description</td>\n",
       "      <td>TYPE_7EQAJ</td>\n",
       "      <td>DEVICE_7EQAJ</td>\n",
       "      <td>5d82799ce24d7e0022912865</td>\n",
       "      <td>tree</td>\n",
       "      <td>json</td>\n",
       "      <td>2019-09-18 18:38:36.875</td>\n",
       "      <td>2019-09-18 18:38:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>172.0</td>\n",
       "      <td>False</td>\n",
       "      <td>field2 description</td>\n",
       "      <td>TYPE_7EQAJ</td>\n",
       "      <td>DEVICE_7EQAJ</td>\n",
       "      <td>5d82799ce24d7e0022912865</td>\n",
       "      <td>tree</td>\n",
       "      <td>json</td>\n",
       "      <td>2019-09-18 18:38:37.614</td>\n",
       "      <td>2019-09-18 18:38:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>771.0</td>\n",
       "      <td>False</td>\n",
       "      <td>field2 description</td>\n",
       "      <td>TYPE_7EQAJ</td>\n",
       "      <td>DEVICE_7EQAJ</td>\n",
       "      <td>5d82799ce24d7e0022912865</td>\n",
       "      <td>tree</td>\n",
       "      <td>json</td>\n",
       "      <td>2019-09-18 18:38:39.745</td>\n",
       "      <td>2019-09-18 18:38:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>940.0</td>\n",
       "      <td>True</td>\n",
       "      <td>field2 description</td>\n",
       "      <td>TYPE_7EQAJ</td>\n",
       "      <td>DEVICE_7EQAJ</td>\n",
       "      <td>5d82799ce24d7e0022912865</td>\n",
       "      <td>tree</td>\n",
       "      <td>json</td>\n",
       "      <td>2019-09-18 18:38:38.355</td>\n",
       "      <td>2019-09-18 18:38:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>910.0</td>\n",
       "      <td>True</td>\n",
       "      <td>field2 description</td>\n",
       "      <td>TYPE_7EQAJ</td>\n",
       "      <td>DEVICE_7EQAJ</td>\n",
       "      <td>5d82799ce24d7e0022912865</td>\n",
       "      <td>tree</td>\n",
       "      <td>json</td>\n",
       "      <td>2019-09-18 18:38:38.989</td>\n",
       "      <td>2019-09-18 18:38:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>580.0</td>\n",
       "      <td>True</td>\n",
       "      <td>field2 description</td>\n",
       "      <td>TYPE_7EQAJ</td>\n",
       "      <td>DEVICE_7EQAJ</td>\n",
       "      <td>5d82799ce24d7e0022912865</td>\n",
       "      <td>tree</td>\n",
       "      <td>json</td>\n",
       "      <td>2019-09-18 18:38:40.695</td>\n",
       "      <td>2019-09-18 18:38:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>710.0</td>\n",
       "      <td>False</td>\n",
       "      <td>field2 description</td>\n",
       "      <td>TYPE_7EQAJ</td>\n",
       "      <td>DEVICE_7EQAJ</td>\n",
       "      <td>5d82799ce24d7e0022912865</td>\n",
       "      <td>tree</td>\n",
       "      <td>json</td>\n",
       "      <td>2019-09-18 18:38:41.355</td>\n",
       "      <td>2019-09-18 18:38:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>240.0</td>\n",
       "      <td>False</td>\n",
       "      <td>field2 description</td>\n",
       "      <td>TYPE_7EQAJ</td>\n",
       "      <td>DEVICE_7EQAJ</td>\n",
       "      <td>5d82799ce24d7e0022912865</td>\n",
       "      <td>tree</td>\n",
       "      <td>json</td>\n",
       "      <td>2019-09-18 18:38:42.001</td>\n",
       "      <td>2019-09-18 18:38:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>921.0</td>\n",
       "      <td>False</td>\n",
       "      <td>field2 description</td>\n",
       "      <td>TYPE_7EQAJ</td>\n",
       "      <td>DEVICE_7EQAJ</td>\n",
       "      <td>5d82799ce24d7e0022912865</td>\n",
       "      <td>tree</td>\n",
       "      <td>json</td>\n",
       "      <td>2019-09-18 18:38:42.706</td>\n",
       "      <td>2019-09-18 18:38:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>319.0</td>\n",
       "      <td>False</td>\n",
       "      <td>field2 description</td>\n",
       "      <td>TYPE_7EQAJ</td>\n",
       "      <td>DEVICE_7EQAJ</td>\n",
       "      <td>5d82799ce24d7e0022912865</td>\n",
       "      <td>tree</td>\n",
       "      <td>json</td>\n",
       "      <td>2019-09-18 18:38:43.468</td>\n",
       "      <td>2019-09-18 18:38:43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   field1  field3              field2  devicetype      deviceid  \\\n",
       "0   540.0    True  field2 description  TYPE_7EQAJ  DEVICE_7EQAJ   \n",
       "1   172.0   False  field2 description  TYPE_7EQAJ  DEVICE_7EQAJ   \n",
       "2   771.0   False  field2 description  TYPE_7EQAJ  DEVICE_7EQAJ   \n",
       "3   940.0    True  field2 description  TYPE_7EQAJ  DEVICE_7EQAJ   \n",
       "4   910.0    True  field2 description  TYPE_7EQAJ  DEVICE_7EQAJ   \n",
       "5   580.0    True  field2 description  TYPE_7EQAJ  DEVICE_7EQAJ   \n",
       "6   710.0   False  field2 description  TYPE_7EQAJ  DEVICE_7EQAJ   \n",
       "7   240.0   False  field2 description  TYPE_7EQAJ  DEVICE_7EQAJ   \n",
       "8   921.0   False  field2 description  TYPE_7EQAJ  DEVICE_7EQAJ   \n",
       "9   319.0   False  field2 description  TYPE_7EQAJ  DEVICE_7EQAJ   \n",
       "\n",
       "        logicalinterface_id eventtype format       rcv_timestamp_utc  \\\n",
       "0  5d82799ce24d7e0022912865      tree   json 2019-09-18 18:38:36.875   \n",
       "1  5d82799ce24d7e0022912865      tree   json 2019-09-18 18:38:37.614   \n",
       "2  5d82799ce24d7e0022912865      tree   json 2019-09-18 18:38:39.745   \n",
       "3  5d82799ce24d7e0022912865      tree   json 2019-09-18 18:38:38.355   \n",
       "4  5d82799ce24d7e0022912865      tree   json 2019-09-18 18:38:38.989   \n",
       "5  5d82799ce24d7e0022912865      tree   json 2019-09-18 18:38:40.695   \n",
       "6  5d82799ce24d7e0022912865      tree   json 2019-09-18 18:38:41.355   \n",
       "7  5d82799ce24d7e0022912865      tree   json 2019-09-18 18:38:42.001   \n",
       "8  5d82799ce24d7e0022912865      tree   json 2019-09-18 18:38:42.706   \n",
       "9  5d82799ce24d7e0022912865      tree   json 2019-09-18 18:38:43.468   \n",
       "\n",
       "          updated_utc  \n",
       "0 2019-09-18 18:38:36  \n",
       "1 2019-09-18 18:38:37  \n",
       "2 2019-09-18 18:38:39  \n",
       "3 2019-09-18 18:38:38  \n",
       "4 2019-09-18 18:38:38  \n",
       "5 2019-09-18 18:38:40  \n",
       "6 2019-09-18 18:38:41  \n",
       "7 2019-09-18 18:38:41  \n",
       "8 2019-09-18 18:38:42  \n",
       "9 2019-09-18 18:38:43  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deviceid</th>\n",
       "      <th>evt_timestamp</th>\n",
       "      <th>DEVICETYPE</th>\n",
       "      <th>LOGICALINTERFACE_ID</th>\n",
       "      <th>EVENTTYPE</th>\n",
       "      <th>FORMAT</th>\n",
       "      <th>UPDATED_UTC</th>\n",
       "      <th>pressure</th>\n",
       "      <th>temperature</th>\n",
       "      <th>TURBINE_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A101</td>\n",
       "      <td>2019-09-28 23:33:07.676973</td>\n",
       "      <td>EnergySystemModel02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-28 23:33:07.677001</td>\n",
       "      <td>33.498815</td>\n",
       "      <td>34.190390</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B102</td>\n",
       "      <td>2019-09-28 23:33:07.676973</td>\n",
       "      <td>EnergySystemModel02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-28 23:33:07.677001</td>\n",
       "      <td>33.498815</td>\n",
       "      <td>26.799052</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A101</td>\n",
       "      <td>2019-09-28 23:38:07.860557</td>\n",
       "      <td>EnergySystemModel02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-28 23:38:07.860528</td>\n",
       "      <td>29.552021</td>\n",
       "      <td>15.893546</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  deviceid              evt_timestamp           DEVICETYPE  \\\n",
       "0     A101 2019-09-28 23:33:07.676973  EnergySystemModel02   \n",
       "1     B102 2019-09-28 23:33:07.676973  EnergySystemModel02   \n",
       "2     A101 2019-09-28 23:38:07.860557  EnergySystemModel02   \n",
       "\n",
       "   LOGICALINTERFACE_ID  EVENTTYPE  FORMAT                UPDATED_UTC  \\\n",
       "0                  NaN        NaN     NaN 2019-09-28 23:33:07.677001   \n",
       "1                  NaN        NaN     NaN 2019-09-28 23:33:07.677001   \n",
       "2                  NaN        NaN     NaN 2019-09-28 23:38:07.860528   \n",
       "\n",
       "    pressure  temperature TURBINE_ID  \n",
       "0  33.498815    34.190390        NaN  \n",
       "1  33.498815    26.799052        NaN  \n",
       "2  29.552021    15.893546        NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get stuff in\n",
    "df_input = pd.read_csv('./Anomaly_Sample_data.csv',\n",
    "                  parse_dates=['EVT_TIMESTAMP','UPDATED_UTC'])\n",
    "df_input.rename(columns={'TEMPERATURE':'temperature',\n",
    "                 'PRESSURE':'pressure','DEVICEID':'deviceid',\n",
    "                 'EVT_TIMESTAMP':'evt_timestamp'}, inplace = True)\n",
    "df_input.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-15T14:52:48.022 DEBUG iotfunctions.enginelog.configure_console_logging Console logging has been configured. Level = 10\n",
      "2019-10-15T14:52:48.023 DEBUG iotfunctions.metadata.__init__ Initializing new entity type using iotfunctions 2.0.3\n",
      "2019-10-15T14:52:48.024 DEBUG iotfunctions.pipeline.__init__ Starting trace\n",
      "2019-10-15T14:52:48.025 DEBUG iotfunctions.pipeline.__init__ Trace name: auto_trace_markus_testdata_20191015125248\n",
      "2019-10-15T14:52:48.025 DEBUG iotfunctions.pipeline.__init__ auto_save None\n",
      "2019-10-15T14:52:48.026 WARNING iotfunctions.metadata.__init__ No _db_schema specified in **kwargs. Usingdefault database schema.\n",
      "2019-10-15T14:52:48.026 DEBUG iotfunctions.util.categorize_args categorizing arguments\n",
      "2019-10-15T14:52:48.027 DEBUG iotfunctions.metadata.__init__ Initialized entity type \n",
      "EntityType:markus_testdata\n",
      "Functions:\n",
      "EntityDataGenerator at granularity None required inputs not evaluated yet outputs produced not evaluated yet on schedule None\n",
      "Granularities:\n",
      "No schedules metadata\n",
      "2019-10-15T14:52:48.028 DEBUG iotfunctions.metadata.generate_data Generating data for markus_testdata with metrics ['Pressure', 'TestData', 'Temperature'] and dimensions ['eventtype'] and dates []\n",
      "2019-10-15T14:52:48.042 DEBUG iotfunctions.automation.get_data Generated 6 rows of time series data from 2019-10-15 12:47:48.028680 to 2019-10-15 12:52:48.028680\n",
      "2019-10-15T14:52:49.669 INFO iotfunctions.db.write_frame Wrote data to table markus_testdata \n"
     ]
    }
   ],
   "source": [
    "# Generate 5 mins of data in table 'testdata' with a single additional column of TestData\n",
    "EngineLogging.configure_console_logging(logging.DEBUG)\n",
    "jobsettings = {}\n",
    "#jobsettings = {'_timestamp' : 'TIMESTAMP'}\n",
    "et = metadata.EntityType('markus_testdata', db, \n",
    "                         bif.EntityDataGenerator(output_item='my_test_gen'),\n",
    "                         \n",
    "                         Column('TestData',Float()),\n",
    "                         Column('Temperature',Float()),\n",
    "                         Column('Pressure',Float()),\n",
    "                         **jobsettings)\n",
    "\n",
    "#start_date = dt.datetime.utcnow() - dt.timedelta(days=1)\n",
    "#et.exec_local_pipeline(start_ts = start_date)\n",
    "df = et.generate_data(datasource=df_input,datasourcemetrics = ['temperature','pressure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-15T14:52:56.406 DEBUG iotfunctions.metadata.__init__ Initializing new entity type using iotfunctions 2.0.3\n",
      "2019-10-15T14:52:56.408 DEBUG iotfunctions.pipeline.__init__ Starting trace\n",
      "2019-10-15T14:52:56.409 DEBUG iotfunctions.pipeline.__init__ Trace name: auto_trace_markus_testdata_20191015125256\n",
      "2019-10-15T14:52:56.409 DEBUG iotfunctions.pipeline.__init__ auto_save None\n",
      "2019-10-15T14:52:56.410 WARNING iotfunctions.metadata.__init__ No _db_schema specified in **kwargs. Usingdefault database schema.\n",
      "2019-10-15T14:52:56.410 DEBUG iotfunctions.util.categorize_args categorizing arguments\n",
      "2019-10-15T14:52:56.411 DEBUG iotfunctions.metadata.__init__ Initialized entity type \n",
      "EntityType:markus_testdata\n",
      "Functions:\n",
      "Granularities:\n",
      "No schedules metadata\n",
      "2019-10-15T14:52:56.871 DEBUG iotfunctions.metadata.index_df Indexed dataframe on id, evt_timestamp\n",
      "\n",
      "EntityType:markus_testdata\n",
      "Functions:\n",
      "Granularities:\n",
      "No schedules metadata\n"
     ]
    }
   ],
   "source": [
    "#jobsettings = {'_timestamp' : 'TIMESTAMP'}\n",
    "jobsettings = {}\n",
    "et2 = metadata.EntityType('markus_testdata', db, \n",
    "                          Column('TestData',Float()),\n",
    "                          Column('Temperature',Float()),\n",
    "                          Column('Pressure',Float()),\n",
    "                          **jobsettings)\n",
    "et2.get_data()\n",
    "print (et2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iotfunctions import pipeline as pp\n",
    "#job = pp.JobController(et)\n",
    "#job.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pressure</th>\n",
       "      <th>TestData</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>deviceid</th>\n",
       "      <th>evt_timestamp</th>\n",
       "      <th>eventtype</th>\n",
       "      <th>logicalinterface_id</th>\n",
       "      <th>devicetype</th>\n",
       "      <th>format</th>\n",
       "      <th>updated_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.655470</td>\n",
       "      <td>0.346964</td>\n",
       "      <td>0.191594</td>\n",
       "      <td>73004</td>\n",
       "      <td>2019-10-15 12:47:48.028680</td>\n",
       "      <td>yv</td>\n",
       "      <td></td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.640191</td>\n",
       "      <td>1.227744</td>\n",
       "      <td>-0.960800</td>\n",
       "      <td>73002</td>\n",
       "      <td>2019-10-15 12:48:48.028680</td>\n",
       "      <td>yt</td>\n",
       "      <td></td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.421616</td>\n",
       "      <td>1.327502</td>\n",
       "      <td>-0.138373</td>\n",
       "      <td>73002</td>\n",
       "      <td>2019-10-15 12:49:48.028680</td>\n",
       "      <td>pe</td>\n",
       "      <td></td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.678974</td>\n",
       "      <td>0.513737</td>\n",
       "      <td>-0.939611</td>\n",
       "      <td>73001</td>\n",
       "      <td>2019-10-15 12:50:48.028680</td>\n",
       "      <td>et</td>\n",
       "      <td></td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.075767</td>\n",
       "      <td>-1.138563</td>\n",
       "      <td>-0.950398</td>\n",
       "      <td>73004</td>\n",
       "      <td>2019-10-15 12:51:48.028680</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.244050</td>\n",
       "      <td>0.555476</td>\n",
       "      <td>1.762586</td>\n",
       "      <td>73002</td>\n",
       "      <td>2019-10-15 12:52:48.028680</td>\n",
       "      <td>nt</td>\n",
       "      <td></td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pressure  TestData  Temperature deviceid              evt_timestamp  \\\n",
       "0 -0.655470  0.346964     0.191594    73004 2019-10-15 12:47:48.028680   \n",
       "1  0.640191  1.227744    -0.960800    73002 2019-10-15 12:48:48.028680   \n",
       "2  0.421616  1.327502    -0.138373    73002 2019-10-15 12:49:48.028680   \n",
       "3  1.678974  0.513737    -0.939611    73001 2019-10-15 12:50:48.028680   \n",
       "4  1.075767 -1.138563    -0.950398    73004 2019-10-15 12:51:48.028680   \n",
       "5 -0.244050  0.555476     1.762586    73002 2019-10-15 12:52:48.028680   \n",
       "\n",
       "  eventtype logicalinterface_id       devicetype format updated_utc  \n",
       "0        yv                      markus_testdata               None  \n",
       "1        yt                      markus_testdata               None  \n",
       "2        pe                      markus_testdata               None  \n",
       "3        et                      markus_testdata               None  \n",
       "4        en                      markus_testdata               None  \n",
       "5        nt                      markus_testdata               None  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "markus_testdata\n",
      "(18, 10)\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "# read it back\n",
    "table = db.get_table(\"markus_testdata\")\n",
    "start_ts = dt.datetime.utcnow() - dt.timedelta(days=1)\n",
    "end_ts = dt.datetime.utcnow()\n",
    "df_in = db.read_table(table, None, None, None, \"evt_timestamp\", start_ts, end_ts)\n",
    "print (table)\n",
    "print (df_in.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deviceid</th>\n",
       "      <th>evt_timestamp</th>\n",
       "      <th>devicetype</th>\n",
       "      <th>logicalinterface_id</th>\n",
       "      <th>eventtype</th>\n",
       "      <th>format</th>\n",
       "      <th>updated_utc</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>TestData</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73001</td>\n",
       "      <td>2019-10-15 09:46:35.644241</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>ne</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>-0.939658</td>\n",
       "      <td>0.245004</td>\n",
       "      <td>-0.698251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73001</td>\n",
       "      <td>2019-10-15 09:47:35.644241</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>et</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>0.477806</td>\n",
       "      <td>0.086230</td>\n",
       "      <td>1.864483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73003</td>\n",
       "      <td>2019-10-15 09:48:35.644241</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>ye</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>0.620221</td>\n",
       "      <td>1.822393</td>\n",
       "      <td>-0.151731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73000</td>\n",
       "      <td>2019-10-15 09:49:35.644241</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>vy</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>-1.071366</td>\n",
       "      <td>0.204113</td>\n",
       "      <td>1.654859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73004</td>\n",
       "      <td>2019-10-15 09:50:35.644241</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>yn</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>-0.203016</td>\n",
       "      <td>1.746947</td>\n",
       "      <td>-0.268171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>73001</td>\n",
       "      <td>2019-10-15 09:51:35.644241</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>ye</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>2.582269</td>\n",
       "      <td>-0.179135</td>\n",
       "      <td>-1.314995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>73003</td>\n",
       "      <td>2019-10-15 12:46:21.301233</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>ey</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>-1.032872</td>\n",
       "      <td>0.995867</td>\n",
       "      <td>0.324136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>73001</td>\n",
       "      <td>2019-10-15 12:47:21.301233</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>ty</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>-0.233836</td>\n",
       "      <td>-0.956949</td>\n",
       "      <td>1.163183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>73001</td>\n",
       "      <td>2019-10-15 12:48:21.301233</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>et</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.103121</td>\n",
       "      <td>1.234914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>73001</td>\n",
       "      <td>2019-10-15 12:49:21.301233</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>nt</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>-0.179647</td>\n",
       "      <td>-0.499635</td>\n",
       "      <td>1.344584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>73002</td>\n",
       "      <td>2019-10-15 12:50:21.301233</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>vt</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>-1.433747</td>\n",
       "      <td>0.076875</td>\n",
       "      <td>0.340413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>73002</td>\n",
       "      <td>2019-10-15 12:51:21.301233</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>-1.091711</td>\n",
       "      <td>1.675762</td>\n",
       "      <td>0.074701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>73004</td>\n",
       "      <td>2019-10-15 12:47:48.028680</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>yv</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>-0.655470</td>\n",
       "      <td>0.191594</td>\n",
       "      <td>0.346964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>73002</td>\n",
       "      <td>2019-10-15 12:48:48.028680</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>yt</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>0.640191</td>\n",
       "      <td>-0.960800</td>\n",
       "      <td>1.227744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>73002</td>\n",
       "      <td>2019-10-15 12:49:48.028680</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>pe</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>0.421616</td>\n",
       "      <td>-0.138373</td>\n",
       "      <td>1.327502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>73001</td>\n",
       "      <td>2019-10-15 12:50:48.028680</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>et</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>1.678974</td>\n",
       "      <td>-0.939611</td>\n",
       "      <td>0.513737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>73004</td>\n",
       "      <td>2019-10-15 12:51:48.028680</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>1.075767</td>\n",
       "      <td>-0.950398</td>\n",
       "      <td>-1.138563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>73002</td>\n",
       "      <td>2019-10-15 12:52:48.028680</td>\n",
       "      <td>markus_testdata</td>\n",
       "      <td></td>\n",
       "      <td>nt</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>-0.244050</td>\n",
       "      <td>1.762586</td>\n",
       "      <td>0.555476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   deviceid              evt_timestamp       devicetype logicalinterface_id  \\\n",
       "0     73001 2019-10-15 09:46:35.644241  markus_testdata                       \n",
       "1     73001 2019-10-15 09:47:35.644241  markus_testdata                       \n",
       "2     73003 2019-10-15 09:48:35.644241  markus_testdata                       \n",
       "3     73000 2019-10-15 09:49:35.644241  markus_testdata                       \n",
       "4     73004 2019-10-15 09:50:35.644241  markus_testdata                       \n",
       "5     73001 2019-10-15 09:51:35.644241  markus_testdata                       \n",
       "6     73003 2019-10-15 12:46:21.301233  markus_testdata                       \n",
       "7     73001 2019-10-15 12:47:21.301233  markus_testdata                       \n",
       "8     73001 2019-10-15 12:48:21.301233  markus_testdata                       \n",
       "9     73001 2019-10-15 12:49:21.301233  markus_testdata                       \n",
       "10    73002 2019-10-15 12:50:21.301233  markus_testdata                       \n",
       "11    73002 2019-10-15 12:51:21.301233  markus_testdata                       \n",
       "12    73004 2019-10-15 12:47:48.028680  markus_testdata                       \n",
       "13    73002 2019-10-15 12:48:48.028680  markus_testdata                       \n",
       "14    73002 2019-10-15 12:49:48.028680  markus_testdata                       \n",
       "15    73001 2019-10-15 12:50:48.028680  markus_testdata                       \n",
       "16    73004 2019-10-15 12:51:48.028680  markus_testdata                       \n",
       "17    73002 2019-10-15 12:52:48.028680  markus_testdata                       \n",
       "\n",
       "   eventtype format updated_utc  Pressure  Temperature  TestData  \n",
       "0         ne               None -0.939658     0.245004 -0.698251  \n",
       "1         et               None  0.477806     0.086230  1.864483  \n",
       "2         ye               None  0.620221     1.822393 -0.151731  \n",
       "3         vy               None -1.071366     0.204113  1.654859  \n",
       "4         yn               None -0.203016     1.746947 -0.268171  \n",
       "5         ye               None  2.582269    -0.179135 -1.314995  \n",
       "6         ey               None -1.032872     0.995867  0.324136  \n",
       "7         ty               None -0.233836    -0.956949  1.163183  \n",
       "8         et               None  0.081633     0.103121  1.234914  \n",
       "9         nt               None -0.179647    -0.499635  1.344584  \n",
       "10        vt               None -1.433747     0.076875  0.340413  \n",
       "11        en               None -1.091711     1.675762  0.074701  \n",
       "12        yv               None -0.655470     0.191594  0.346964  \n",
       "13        yt               None  0.640191    -0.960800  1.227744  \n",
       "14        pe               None  0.421616    -0.138373  1.327502  \n",
       "15        et               None  1.678974    -0.939611  0.513737  \n",
       "16        en               None  1.075767    -0.950398 -1.138563  \n",
       "17        nt               None -0.244050     1.762586  0.555476  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# check whether the data for the last 5 minutes is the same - must return True\n",
    "print (np.array_equal(df['TestData'].tail(5), df_in['TestData'].tail(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<iotfunctions.bif.EntityDataGenerator object at 0x7fec3b907b00>]\n"
     ]
    }
   ],
   "source": [
    "print (et._functions)\n",
    "#del (et2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#define SpectralFeatureExtract\n",
    "class SpectralFeatureExtract(BaseTransformer):\n",
    "    '''\n",
    "    Employs spectral analysis to extract features from the time series data\n",
    "    '''\n",
    "    def __init__(self, input_item, windowsize, zscore, output_item):\n",
    "        super().__init__()\n",
    "        print (input_item)\n",
    "        self.input_item = input_item\n",
    "\n",
    "        # zscore - 3 deviation above mean\n",
    "        self.zscore = zscore\n",
    "\n",
    "        # use 24 by default - must be larger than 12\n",
    "        self.windowsize = windowsize\n",
    "\n",
    "        # overlap \n",
    "        self.windowoverlap = self.windowsize - self.windowsize // 12\n",
    "\n",
    "        # assume 1 per sec for now\n",
    "        self.frame_rate = 1\n",
    "\n",
    "        self.output_item = output_item\n",
    "        \n",
    "        \n",
    "    def execute(self, df):\n",
    "\n",
    "        print (df.index.levels[0])\n",
    "        entities = np.unique(df.index.levels[0])\n",
    "        logger.info (entities)\n",
    "        \n",
    "        df[self.output_item] = 0\n",
    "        \n",
    "        for entity in entities: \n",
    "            # per entity\n",
    "            dfe = df.loc[[entity]].dropna(how='all')\n",
    "            \n",
    "            # interpolate gaps - data imputation\n",
    "            #dfe.set_index('timestamp')\n",
    "            dfe = dfe.reset_index(level=[0])\n",
    "            Size = dfe[[self.input_item]].fillna(0).to_numpy().size\n",
    "            dfe = dfe.interpolate(method='time')\n",
    "            \n",
    "            # one dimensional time series - named temperature for catchyness\n",
    "            temperature = dfe[[self.input_item]].fillna(0).to_numpy().reshape(-1,)\n",
    "            \n",
    "            print (entity, self.input_item, self.windowsize, self.zscore, self.output_item, self.windowoverlap, temperature.size)\n",
    "            \n",
    "            if temperature.size > self.windowsize:\n",
    "                print (temperature.size, self.windowsize)\n",
    "                # Fourier transform:\n",
    "                #   frequency, time, spectral density\n",
    "                freqsTS, timesTS, SxTS = signal.spectrogram(temperature, fs = self.frame_rate, window = 'hanning',\n",
    "                                                        nperseg = self.windowsize, noverlap = self.windowoverlap,\n",
    "                                                        detrend = False, scaling='spectrum')\n",
    "\n",
    "                # cut off freqencies too low to fit into the window\n",
    "                freqsTSb = (freqsTS > 2/self.windowsize).astype(int)\n",
    "                freqsTS = freqsTS * freqsTSb\n",
    "                freqsTS[freqsTS == 0] = 1 / self.windowsize\n",
    "\n",
    "                # Compute energy = frequency * spectral density over time in decibel\n",
    "                ETS = np.log10(np.dot(SxTS.T, freqsTS))\n",
    "                print (entity, ETS)\n",
    "\n",
    "                # compute zscore over the energy\n",
    "                ets_zscore = (ETS - ETS.mean())/ETS.std(ddof=0)\n",
    "                print (entity, ets_zscore)\n",
    "\n",
    "                # length of timesTS, ETS and ets_zscore is smaller than half the original\n",
    "                #   extend it to cover the full original length \n",
    "                #timesI = np.linspace(0, temperature.size-1, temperature.size)\n",
    "                timesI = np.linspace(0, Size - 1, Size)\n",
    "                zscoreI = np.interp(timesI, timesTS, ets_zscore)\n",
    "\n",
    "                # absolute zscore > 3 ---> anomaly\n",
    "                ets_zscoreb = (abs(zscoreI) > self.zscore).astype(float)\n",
    "                df.loc[[entity]][self.output_item] = zscoreI #ets_zscoreb\n",
    "\n",
    "        msg = 'SpectralAnalysisFeatureExtract'\n",
    "        self.trace_append(msg)\n",
    "        return (df)\n",
    "\n",
    "    @classmethod\n",
    "    def build_ui(cls):\n",
    "        #define arguments that behave as function inputs\n",
    "        inputs = []\n",
    "        inputs.append(ui.UISingleItem(\n",
    "                name = 'input_item',\n",
    "                datatype=float,\n",
    "                description = 'Column for feature extraction'\n",
    "                                              ))\n",
    "        inputs.append(ui.UISingle(\n",
    "                name = 'windowsize',\n",
    "                datatype=int,\n",
    "                description = 'Window size for spectral analysis - default 24'\n",
    "                                              ))\n",
    "        inputs.append(ui.UISingle(\n",
    "                name = 'zscore',\n",
    "                datatype=float,\n",
    "                description = 'Zscore to be interpreted as anomaly'\n",
    "                                              ))\n",
    "        #define arguments that behave as function outputs\n",
    "        outputs = []\n",
    "        outputs.append(ui.UIFunctionOutSingle(\n",
    "                name = 'output_item',\n",
    "                datatype=float,\n",
    "                description='zscore'\n",
    "                ))\n",
    "        return (inputs,outputs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-15T14:54:26.990 DEBUG iotfunctions.metadata.__init__ Initializing new entity type using iotfunctions 2.0.3\n",
      "2019-10-15T14:54:26.991 DEBUG iotfunctions.pipeline.__init__ Starting trace\n",
      "2019-10-15T14:54:26.991 DEBUG iotfunctions.pipeline.__init__ Trace name: auto_trace_markus_testdata_20191015125426\n",
      "2019-10-15T14:54:26.992 DEBUG iotfunctions.pipeline.__init__ auto_save None\n",
      "2019-10-15T14:54:26.992 WARNING iotfunctions.metadata.__init__ No _db_schema specified in **kwargs. Usingdefault database schema.\n",
      "2019-10-15T14:54:26.992 DEBUG iotfunctions.util.categorize_args categorizing arguments\n",
      "2019-10-15T14:54:26.993 DEBUG iotfunctions.metadata.__init__ Initialized entity type \n",
      "EntityType:markus_testdata\n",
      "Functions:\n",
      "Granularities:\n",
      "No schedules metadata\n",
      "TestData\n"
     ]
    }
   ],
   "source": [
    "\n",
    "jobsettings = {}\n",
    "et2 = metadata.EntityType('markus_testdata', db, \n",
    "                          Column('TestData',Float()),\n",
    "                          **jobsettings)\n",
    "#et2._functions = [bif.PythonExpression('5*df[\"TestData\"]','TestOut')]\n",
    "et2._functions = [SpectralFeatureExtract('TestData',12, 2.4, 'TestOut')]\n",
    "\n",
    "\n",
    "# make sure the results of the python expression is saved to the derived metrics table\n",
    "et2._data_items.append({'columnName': 'TestOut', 'columnType': 'NUMBER', 'kpiFunctionId': 22856, \n",
    "                         'kpiFunctionDto': {'output': {'name': 'TestOut'}},\n",
    "                        'name': 'TestOut', 'parentDataItemName': None, 'sourceTableName': 'dm_markus_testdata',\n",
    "                        'transient': False,'type': 'DERIVED_METRIC'})\n",
    "# map device id to entity id for the derived metrics table\n",
    "et2._data_items.append({'columnName': 'deviceid', 'columnType': 'LITERAL', 'kpiFunctionId': None,\n",
    "                         'kpiFunctionDto': {},\n",
    "                         'name': 'ENTITY_ID', 'parentDataItemName': None,'sourceTableName': 'dm_markus_testdata',\n",
    "                         'transient': False,'type': 'METRIC'})\n",
    "\n",
    "# make sure the results of the python expression is saved to the derived metrics daily table\n",
    "et2._data_items.append({'columnName': 'TestData_max', 'columnType': 'NUMBER', 'kpiFunctionId': 22856, \n",
    "                         'kpiFunctionDto': {'output': {'name': 'TestData_max'}},\n",
    "                        'name': 'TestData_max', 'parentDataItemName': None, 'sourceTableName': 'dm_markus_testdata_daily',\n",
    "                        'transient': False,'type': 'DERIVED_METRIC'})\n",
    "# map device id to entity id for the derived metrics daily table\n",
    "et2._data_items.append({'columnName': 'deviceid', 'columnType': 'LITERAL', 'kpiFunctionId': None,\n",
    "                         'kpiFunctionDto': {},\n",
    "                         'name': 'ENTITY_ID', 'parentDataItemName': None,'sourceTableName': 'dm_markus_testdata_daily',\n",
    "                         'transient': False,'type': 'METRIC'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-01 13:06:25.341693\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "dt = datetime.datetime.strptime('2019-10-01 13:06:25.341693','%Y-%m-%d %H:%M:%S.%f')\n",
    "print (dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-15T14:57:25.370 DEBUG iotfunctions.pipeline.set_payload_params Setting param writer_name on payload to <class 'iotfunctions.pipeline.DataWriterSqlAlchemy'>\n",
      "2019-10-15T14:57:25.370 DEBUG iotfunctions.pipeline.set_payload_params Setting param db on payload to <iotfunctions.db.Database object at 0x7fec3cd4ae48>\n",
      "2019-10-15T14:57:25.371 DEBUG iotfunctions.pipeline.set_payload_params Setting param _db_schema on payload to public\n",
      "2019-10-15T14:57:25.371 DEBUG iotfunctions.pipeline.set_payload_params Setting param save_trace_to_file on payload to True\n",
      "2019-10-15T14:57:25.372 DEBUG iotfunctions.pipeline.set_payload_params Setting param tenant_id on payload to AnalyticsServiceDev\n",
      "2019-10-15T14:57:25.959 DEBUG iotfunctions.pipeline.get_output_list The payload has candidate data items ['deviceid', 'evt_timestamp', 'Pressure', 'Temperature', 'TestData', 'deviceid', 'deviceid']. The DataReader has no projection list\n",
      "2019-10-15T14:57:25.960 DEBUG iotfunctions.metadata.build_arg_metadata Using input items TestData for input_item\n",
      "2019-10-15T14:57:25.960 DEBUG iotfunctions.metadata.build_arg_metadata Using output items TestOut for output_item\n",
      "2019-10-15T14:57:25.961 INFO iotfunctions.pipeline.__init__ Initialized job.\n",
      "\n",
      "2019-10-15T14:57:25.962 INFO iotfunctions.pipeline.__init__ \n",
      "Default schedule 5min \n",
      "    Schedule 5min start None:None backtracks: None \n",
      "Stages of type: get_data at grain None: \n",
      "   System generated DataReader stage: read_entity_data. Reads data from objects: markus_testdata\n",
      "Stages of type: transform at grain None: \n",
      "   SpectralFeatureExtract at granularity None requires inputs {'TestData'} produces outputs ['TestOut'] on schedule None\n",
      "\n",
      "\n",
      "2019-10-15T14:57:25.963 INFO iotfunctions.enginelog.start_run_log Started logging into file run.log. Object Store path will be AnalyticsServiceDev/markus_testdata/20191015/125725_run.gz\n",
      "2019-10-15T14:57:25.964 DEBUG iotfunctions.pipeline.execute Starting execution number: 0 with execution date: 2019-10-15 12:57:25.962703\n",
      "2019-10-15T14:57:26.402 DEBUG iotfunctions.pipeline.get_next_execution_date Last execution of schedule 5min was 2019-10-14 17:27:30.812478. Next execution due 2019-10-14 17:32:30.812478.\n",
      "2019-10-15T14:57:26.403 DEBUG iotfunctions.pipeline.evaluate_schedules Schedule 5min will execute\n",
      "2019-10-15T14:57:26.404 DEBUG iotfunctions.pipeline.reset Started a new trace auto_trace_markus_testdata_20191015125725 \n",
      "2019-10-15T14:57:26.405 DEBUG iotfunctions.pipeline.reset Initiating auto save for trace\n",
      "2019-10-15T14:57:27.308 DEBUG iotfunctions.pipeline.insert Created job log entry (markus_testdata,5min): 2019-10-15 12:57:25.962703\n",
      "2019-10-15T14:57:27.310 DEBUG iotfunctions.pipeline.build_job_spec Building a job spec for schedule 5min with subsumbed schedules ['5min']\n",
      "2019-10-15T14:57:27.311 DEBUG iotfunctions.pipeline.build_stages_of_type Gathered stages of type get_data. Iteration 0: ['System generated DataReader stage: read_entity_data. Reads data from objects: markus_testdata']\n",
      "2019-10-15T14:57:27.313 DEBUG iotfunctions.pipeline.build_stages_of_type Built stages of type get_data\n",
      "2019-10-15T14:57:27.314 DEBUG iotfunctions.pipeline.build_stages_of_type Available columns: {'Pressure', 'TestData', 'evt_timestamp', 'deviceid', 'Temperature'}\n",
      "2019-10-15T14:57:27.315 DEBUG iotfunctions.pipeline.build_stages_of_type Gathered stages of type transform. Iteration 0: [\"SpectralFeatureExtract at granularity None requires inputs {'TestData'} produces outputs ['TestOut'] on schedule 5min\"]\n",
      "2019-10-15T14:57:27.316 DEBUG iotfunctions.pipeline.build_stages_of_type Built stages of type transform\n",
      "2019-10-15T14:57:27.317 DEBUG iotfunctions.pipeline.build_stages_of_type Available columns: {'Pressure', 'TestData', 'evt_timestamp', 'TestOut', 'deviceid', 'Temperature'}\n",
      "2019-10-15T14:57:27.318 DEBUG iotfunctions.pipeline.build_job_spec Evaluating data source read_entity_data. Data items required from this source for this execution are {'TestData', 'evt_timestamp', 'deviceid'}\n",
      "2019-10-15T14:57:27.320 INFO iotfunctions.pipeline.write Trimmed data source read_entity_data down to columns ['TestData', 'evt_timestamp', 'deviceid'] as remaining columns {'Pressure', 'Temperature'} are not used\n",
      "2019-10-15T14:57:27.321 DEBUG iotfunctions.pipeline.build_job_spec Build of job spec is complete.\n",
      "2019-10-15T14:57:27.322 DEBUG iotfunctions.pipeline.build_job_spec -------------------------------\n",
      "2019-10-15T14:57:27.323 DEBUG iotfunctions.pipeline.build_job_spec skipped_stages: [None]\n",
      "2019-10-15T14:57:27.324 DEBUG iotfunctions.pipeline.build_job_spec input_level:\n",
      "2019-10-15T14:57:27.325 INFO iotfunctions.pipeline.build_job_spec   System generated DataReader stage: read_entity_data. Reads data from objects: markus_testdata\n",
      "2019-10-15T14:57:27.326 INFO iotfunctions.pipeline.build_job_spec   System generated DropNull stage\n",
      "2019-10-15T14:57:27.327 INFO iotfunctions.pipeline.build_job_spec   SpectralFeatureExtract at granularity None requires inputs {'TestData'} produces outputs ['TestOut'] on schedule 5min\n",
      "2019-10-15T14:57:27.327 INFO iotfunctions.pipeline.build_job_spec   System generated DataWriterSqlAlchemy stage: markus_testdata_input_level\n",
      "2019-10-15T14:57:27.328 DEBUG iotfunctions.pipeline.build_job_spec -------------------------------\n",
      "TBD ***** - Add stages for usage stats and write to MessageHub\n",
      "2019-10-15T14:57:27.328 DEBUG iotfunctions.pipeline.exec_payload_method Returned default output for get_early_timestamp() on payload EntityType markus_testdata.  Default value is: None\n",
      "2019-10-15T14:57:27.329 DEBUG iotfunctions.pipeline.get_chunks The payload does not have an get_early_timestamp method or the method did not retrieve an early timestamp. Data will be retrieved in a single chunk\n",
      "2019-10-15T14:57:27.330 DEBUG iotfunctions.pipeline.write Processing as a single chunk\n",
      "2019-10-15T14:57:27.331 DEBUG iotfunctions.pipeline.write Executing stage read_entity_data.\n",
      "2019-10-15T14:57:29.698 DEBUG iotfunctions.metadata.index_df Indexed dataframe on id, evt_timestamp\n",
      "2019-10-15T14:57:29.701 DEBUG iotfunctions.pipeline.exec_stage_method No method get_column_map on read_entity_data returning default None. 'DataReader' object has no attribute 'get_column_map'\n",
      "2019-10-15T14:57:29.702 DEBUG iotfunctions.pipeline.merge_dataframe Merging dataframe with columns [] and index []\n",
      "2019-10-15T14:57:29.704 DEBUG iotfunctions.metadata.index_df Found existing index on id, evt_timestamp.No need to recreate index\n",
      "2019-10-15T14:57:29.706 INFO iotfunctions.pipeline.update_last_entry Trace message:  Completed stage.\n",
      "2019-10-15T14:57:29.706 INFO iotfunctions.pipeline.update_last_entry Trace payload: {'produces_output_items': True, 'output_items': ['TestData', 'evt_timestamp', 'deviceid'], 'discard_prior_data': False, 'merge_result': 'existing empty df with new DataFrame', 'usage': 54, 'index': ['id', 'evt_timestamp'], 'can_proceed': True, 'updated': datetime.datetime(2019, 10, 15, 12, 57, 29, 706081), 'cumulative_usage': 54}\n",
      "2019-10-15T14:57:29.707 DEBUG iotfunctions.pipeline.write Executing stage drop_null.\n",
      "2019-10-15T14:57:29.707 DEBUG iotfunctions.pipeline.execute columns excluded when dropping null rows ['deviceid', '_timestamp', 'logicalinterface_id', 'devicetype', 'format', 'updated_utc', 'evt_timestamp']\n",
      "2019-10-15T14:57:29.708 DEBUG iotfunctions.pipeline.execute columns considered when dropping null rows ['TestData']\n",
      "2019-10-15T14:57:29.709 DEBUG iotfunctions.pipeline.execute TestData count not null: 18\n",
      "2019-10-15T14:57:29.712 DEBUG iotfunctions.pipeline.exec_stage_method No method get_column_map on drop_null returning default None. 'DropNull' object has no attribute 'get_column_map'\n",
      "2019-10-15T14:57:29.712 INFO iotfunctions.pipeline.update_last_entry Trace message:  Completed stage.\n",
      "2019-10-15T14:57:29.713 INFO iotfunctions.pipeline.update_last_entry Trace payload: {'produces_output_items': False, 'output_items': None, 'discard_prior_data': False, 'new_data_items_info': 'Function is configured not to produce any new data items  during execution', 'can_proceed': True, 'updated': datetime.datetime(2019, 10, 15, 12, 57, 29, 712855), 'cumulative_usage': 54}\n",
      "2019-10-15T14:57:29.713 DEBUG iotfunctions.pipeline.write Executing stage SpectralFeatureExtract.\n",
      "Index(['73000', '73001', '73002', '73003', '73004'], dtype='object', name='id')\n",
      "2019-10-15T14:57:29.714 INFO __main__.execute ['73000' '73001' '73002' '73003' '73004']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73000 TestData 12 2.4 TestOut 11 1\n",
      "73001 TestData 12 2.4 TestOut 11 7\n",
      "73002 TestData 12 2.4 TestOut 11 5\n",
      "73003 TestData 12 2.4 TestOut 11 2\n",
      "73004 TestData 12 2.4 TestOut 11 3\n",
      "2019-10-15T14:57:29.767 DEBUG iotfunctions.pipeline.execute Input dataframe has columns ['TestData', 'deviceid', '_timestamp', 'TestOut'] and index ['id', 'evt_timestamp']\n",
      "2019-10-15T14:57:29.767 DEBUG iotfunctions.pipeline.merge_dataframe Merging dataframe with columns ['TestData', 'deviceid', '_timestamp', 'TestOut'] and index ['id', 'evt_timestamp']\n",
      "2019-10-15T14:57:29.768 DEBUG iotfunctions.pipeline.merge_dataframe Skipping df merge as it looks like the merge has already taken place. To bypass this check and merge set force_overwrite = True\n",
      "2019-10-15T14:57:29.768 DEBUG iotfunctions.pipeline.merge_dataframe Merging dataframe with the same index\n",
      "2019-10-15T14:57:29.769 INFO iotfunctions.pipeline.update_last_entry Trace message:  Completed stage.\n",
      "2019-10-15T14:57:29.769 INFO iotfunctions.pipeline.update_last_entry Trace payload: {'produces_output_items': True, 'output_items': ['TestOut'], 'discard_prior_data': False, 'merge_result': 'existing df with new DataFrame', 'usage': 18, 'can_proceed': True, 'updated': datetime.datetime(2019, 10, 15, 12, 57, 29, 769128), 'cumulative_usage': 72}\n",
      "2019-10-15T14:57:29.770 DEBUG iotfunctions.pipeline.write Executing stage markus_testdata_input_level.\n",
      "2019-10-15T14:57:29.770 DEBUG iotfunctions.pipeline.execute Data items will be written to database for interval (None, 2019-10-15 12:57:25.962703)\n",
      "2019-10-15T14:57:29.771 INFO iotfunctions.pipeline._get_active_cols_properties The column TestData in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-10-15T14:57:29.771 INFO iotfunctions.pipeline._get_active_cols_properties The column deviceid in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-10-15T14:57:29.772 INFO iotfunctions.pipeline._get_active_cols_properties The column _timestamp in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-10-15T14:57:29.772 INFO iotfunctions.pipeline.execute The following data items will be written to the database: TestOut (dm_markus_testdata, NUMBER)\n",
      "2019-10-15T14:57:29.773 DEBUG iotfunctions.pipeline._get_table_properties Mapping between index name and index position: id -> 0, evt_timestamp -> 1\n",
      "2019-10-15T14:57:31.716 DEBUG iotfunctions.pipeline._get_table_properties For table dm_markus_testdata: delete statement: DELETE FROM dm_markus_testdata, insert statement: INSERT INTO dm_markus_testdata (entity_id, key, value_n, value_b, value_s, value_t, timestamp, last_update) VALUES (:entity_id, :key, :value_n, :value_b, :value_s, :value_t, :timestamp, :last_update)\n",
      "2019-10-15T14:57:31.718 DEBUG iotfunctions.pipeline.get_col_name_map_for_sa Columns of table dm_markus_testdata: value_n, key, timestamp, value_b, entity_id, last_update, value_t, value_s\n",
      "2019-10-15T14:57:31.720 DEBUG iotfunctions.pipeline.get_col_name_map_for_sa Column name mapping for table dm_markus_testdata: key ==> key, value_n ==> value_n, value_s ==> value_s, value_b ==> value_b, value_t ==> value_t, timestamp ==> timestamp, entity_id ==> entity_id\n",
      "2019-10-15T14:57:31.721 DEBUG iotfunctions.pipeline._get_table_properties For table dm_markus_testdata: Mapping between column name and dataframe index position: entity_id ==> 0, timestamp ==> 1\n",
      "2019-10-15T14:57:31.723 INFO iotfunctions.pipeline.execute The data items will be written into the following tables: dm_markus_testdata\n",
      "2019-10-15T14:57:31.724 DEBUG iotfunctions.pipeline._delete_old_data Deleting old data items from table dm_markus_testdata for time range [None, 2019-10-15 12:57:25.962703]\n",
      "2019-10-15T14:57:31.725 DEBUG iotfunctions.pipeline._delete_old_data Executing delete statement: DELETE FROM dm_markus_testdata WHERE dm_markus_testdata.timestamp < :timestamp_1\n",
      "2019-10-15T14:57:32.172 INFO iotfunctions.pipeline._delete_old_data 66 data items have been deleted from table dm_markus_testdata. Elapsed time in sec: 0.447\n",
      "2019-10-15T14:57:35.090 INFO iotfunctions.pipeline._persist_data Number of data item values persisted so far: 18 (dm_markus_testdata)\n",
      "2019-10-15T14:57:35.092 INFO iotfunctions.pipeline._persist_data Total number of persisted data item values: 18, Elapsed time in sec: 2.919, SqlAlchemy time in sec: 2.914\n",
      "2019-10-15T14:57:35.094 DEBUG iotfunctions.pipeline.exec_stage_method No method get_column_map on markus_testdata_input_level returning default None. 'DataWriterSqlAlchemy' object has no attribute 'get_column_map'\n",
      "2019-10-15T14:57:35.095 INFO iotfunctions.pipeline.update_last_entry Trace message:  Completed stage.\n",
      "2019-10-15T14:57:35.097 INFO iotfunctions.pipeline.update_last_entry Trace payload: {'produces_output_items': False, 'output_items': None, 'discard_prior_data': False, 'new_data_items_info': 'Function is configured not to produce any new data items  during execution', 'can_proceed': True, 'updated': datetime.datetime(2019, 10, 15, 12, 57, 35, 95489), 'cumulative_usage': 72}\n",
      "2019-10-15T14:57:35.102 INFO iotfunctions.pipeline.write Execution complete\n",
      "2019-10-15T14:57:35.110 DEBUG urllib3.connectionpool._new_conn Starting new HTTPS connection (1): s3-api.us-geo.objectstorage.softlayer.net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/.local/lib/python3.7/site-packages/urllib3/connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-15T14:57:36.434 DEBUG urllib3.connectionpool._make_request https://s3-api.us-geo.objectstorage.softlayer.net:443 \"PUT /analytics-runtime-analyticsservicedev-799d2008b460/AnalyticsServiceDev/markus_testdata/20191015/markus_testdata_trace_125725 HTTP/1.1\" 200 0\n",
      "2019-10-15T14:57:36.436 DEBUG iotfunctions.pipeline.save Saved trace to cos AnalyticsServiceDev/markus_testdata/20191015/markus_testdata_trace_125725\n",
      "2019-10-15T14:57:36.437 DEBUG iotfunctions.pipeline.save wrote trace to file auto_trace_markus_testdata_20191015125725.json\n",
      "2019-10-15T14:57:36.890 DEBUG iotfunctions.pipeline.update Updated job log (markus_testdata,5min): 2019-10-15 12:57:25.962703\n",
      "2019-10-15T14:57:36.892 DEBUG iotfunctions.pipeline.get_next_future_execution Next scheduled execution date is 2019-10-15 13:02:25.962703\n",
      "2019-10-15T14:57:36.894 DEBUG iotfunctions.pipeline.execute Ending job normally as there are no scheduled executions  due before execution end time\n",
      "2019-10-15T14:57:36.928 DEBUG iotfunctions.pipeline.run_auto_save auto_trace_markus_testdata_20191015125725 autosave thread has stopped\n",
      "2019-10-15T14:57:36.929 DEBUG iotfunctions.pipeline.stop Stopping autosave on trace auto_trace_markus_testdata_20191015125725\n"
     ]
    }
   ],
   "source": [
    "# dm_markus_testdate MUST exist, so run the following sql statment in DBeaver\n",
    "#     - Db2 ----\n",
    "#CREATE TABLE BLUADMIN.DM_MARKUS_TESTDATA (\n",
    "#  entity_id varchar(255),\n",
    "#  key varchar(255),\n",
    "#  value_n double,\n",
    "#  value_b boolean,\n",
    "#  value_s varchar(255),\n",
    "#  value_t timestamp,\n",
    "#  timestamp timestamp,\n",
    "#  last_update timestamp\n",
    "# );\n",
    "\n",
    "#    - Postgres ---\n",
    "#CREATE TABLE public.dm_markus_testdata (\n",
    "#  entity_id varchar(255),\n",
    "#  key varchar(255),\n",
    "#  value_n double precision,\n",
    "#  value_b boolean,\n",
    "#  value_s varchar(255),\n",
    "#  value_t timestamp,\n",
    "#  timestamp timestamp,\n",
    "#  last_update timestamp\n",
    "# );\n",
    "\n",
    "\n",
    "# The commented out version just dumps the job spec\n",
    "#jobsettings = {'writer_name' : SqlAlchemyDataWriter, 'db': db, '_db_schema': 'BLUADMIN', 'save_trace_to_file' : True}\n",
    "jobsettings = {'writer_name' : pp.DataWriterSqlAlchemy, 'db': db, '_db_schema': 'public', 'save_trace_to_file' : True}\n",
    "job = pp.JobController(et2, **jobsettings)\n",
    "job.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (et2.get_data_items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-09T18:09:44.444 DEBUG iotfunctions.pipeline.set_payload_params Setting param writer_name on payload to <class 'iotfunctions.pipeline.SqlAlchemyDataWriter'>\n",
      "2019-10-09T18:09:44.446 DEBUG iotfunctions.pipeline.set_payload_params Setting param _db_schema on payload to public\n",
      "2019-10-09T18:09:44.447 DEBUG iotfunctions.pipeline.set_payload_params Setting param save_trace_to_file on payload to True\n",
      "2019-10-09T18:09:44.449 DEBUG iotfunctions.pipeline.set_payload_params Setting param tenant_id on payload to AnalyticsServiceDev\n",
      "2019-10-09T18:09:45.292 DEBUG iotfunctions.pipeline.get_output_list The payload has candidate data items ['deviceid', 'evt_timestamp', 'TestData', 'deviceid', 'deviceid']. The DataReader has no projection list\n",
      "2019-10-09T18:09:45.293 DEBUG iotfunctions.metadata.classify_stages Output list set was preset for function AggregateItems\n",
      "2019-10-09T18:09:45.293 DEBUG iotfunctions.metadata.classify_stages Function AggregateItems has no _metadata_params property. This property allows the stage to add properties to the entity type. Using default of {}\n",
      "2019-10-09T18:09:45.294 INFO iotfunctions.pipeline.__init__ Initialized job.\n",
      "\n",
      "2019-10-09T18:09:45.294 INFO iotfunctions.pipeline.__init__ \n",
      "Default schedule 5min \n",
      "    Schedule 5min start None:None backtracks: None \n",
      "Stages of type: get_data at grain None: \n",
      "   System generated DataReader stage: read_entity_data. Reads data from objects: markus_testdata\n",
      "Stages of type: simple_aggregate at grain daily: \n",
      "   <iotfunctions.pipeline.AggregateItems object at 0x7f0f9d396668>\n",
      "\n",
      "\n",
      "2019-10-09T18:09:45.295 INFO iotfunctions.enginelog.start_run_log Started logging into file run.log. Object Store path will be AnalyticsServiceDev/markus_testdata/20191009/160945_run.gz\n",
      "2019-10-09T18:09:45.296 DEBUG iotfunctions.pipeline.execute Starting execution number: 0 with execution date: 2019-10-09 16:09:45.295410\n",
      "2019-10-09T18:09:45.719 DEBUG iotfunctions.pipeline.get_next_execution_date Last execution of schedule 5min was 2019-10-09 15:41:12.476106. Next execution due 2019-10-09 15:46:12.476106.\n",
      "2019-10-09T18:09:45.720 DEBUG iotfunctions.pipeline.evaluate_schedules Schedule 5min will execute\n",
      "2019-10-09T18:09:45.721 DEBUG iotfunctions.pipeline.reset Started a new trace auto_trace_markus_testdata_20191009160945 \n",
      "2019-10-09T18:09:45.723 DEBUG iotfunctions.pipeline.reset Initiating auto save for trace\n",
      "2019-10-09T18:09:46.570 DEBUG iotfunctions.pipeline.insert Created job log entry (markus_testdata,5min): 2019-10-09 16:09:45.295410\n",
      "2019-10-09T18:09:46.571 DEBUG iotfunctions.pipeline.build_job_spec Building a job spec for schedule 5min with subsumbed schedules ['5min']\n",
      "2019-10-09T18:09:46.572 DEBUG iotfunctions.pipeline.build_stages_of_type Gathered stages of type get_data. Iteration 0: ['System generated DataReader stage: read_entity_data. Reads data from objects: markus_testdata']\n",
      "2019-10-09T18:09:46.573 DEBUG iotfunctions.pipeline.build_stages_of_type Built stages of type get_data\n",
      "2019-10-09T18:09:46.574 DEBUG iotfunctions.pipeline.build_stages_of_type Available columns: {'deviceid', 'evt_timestamp', 'TestData'}\n",
      "2019-10-09T18:09:46.575 DEBUG iotfunctions.pipeline.build_stages_of_type Built stages of type transform\n",
      "2019-10-09T18:09:46.576 DEBUG iotfunctions.pipeline.build_stages_of_type Available columns: {'deviceid', 'evt_timestamp', 'TestData'}\n",
      "2019-10-09T18:09:46.577 DEBUG iotfunctions.pipeline.build_job_spec Building job spec for aggregation to grain: daily\n",
      "2019-10-09T18:09:46.578 DEBUG iotfunctions.pipeline.build_job_spec Collapsed aggregation stages ['AggregateItems'] down to a single\"\n",
      "2019-10-09T18:09:46.579 DEBUG iotfunctions.pipeline.build_job_spec OrderedDict([('TestData', ['max'])])\n",
      "2019-10-09T18:09:46.581 DEBUG iotfunctions.pipeline.build_job_spec Added aggregregator to job spec: Aggregator: auto_aggregate with granularity: daily.  Aggregates TestData using ['max'] .\n",
      "2019-10-09T18:09:46.582 DEBUG iotfunctions.pipeline.build_stages_of_type Built stages of type transform\n",
      "2019-10-09T18:09:46.583 DEBUG iotfunctions.pipeline.build_stages_of_type Available columns: {'TestData_max', 'deviceid', 'TestData', 'evt_timestamp'}\n",
      "2019-10-09T18:09:46.585 DEBUG iotfunctions.pipeline.build_job_spec Completed job spec build for grain: daily\n",
      "2019-10-09T18:09:46.585 DEBUG iotfunctions.pipeline.build_job_spec Evaluating data source read_entity_data. Data items required from this source for this execution are {'deviceid', 'evt_timestamp', 'TestData'}\n",
      "2019-10-09T18:09:46.586 DEBUG iotfunctions.pipeline.build_job_spec Build of job spec is complete.\n",
      "2019-10-09T18:09:46.587 DEBUG iotfunctions.pipeline.build_job_spec -------------------------------\n",
      "2019-10-09T18:09:46.588 DEBUG iotfunctions.pipeline.build_job_spec skipped_stages: [None]\n",
      "2019-10-09T18:09:46.589 DEBUG iotfunctions.pipeline.build_job_spec input_level:\n",
      "2019-10-09T18:09:46.589 INFO iotfunctions.pipeline.build_job_spec   System generated DataReader stage: read_entity_data. Reads data from objects: markus_testdata\n",
      "2019-10-09T18:09:46.590 INFO iotfunctions.pipeline.build_job_spec   System generated DropNull stage\n",
      "2019-10-09T18:09:46.591 INFO iotfunctions.pipeline.build_job_spec   System generated SqlAlchemyDataWriter stage: markus_testdata_input_level\n",
      "2019-10-09T18:09:46.592 DEBUG iotfunctions.pipeline.build_job_spec daily:\n",
      "2019-10-09T18:09:46.594 INFO iotfunctions.pipeline.build_job_spec   Aggregator: auto_aggregate with granularity: daily.  Aggregates TestData using ['max'] .\n",
      "2019-10-09T18:09:46.596 INFO iotfunctions.pipeline.build_job_spec   System generated SqlAlchemyDataWriter stage: markus_testdata_daily\n",
      "2019-10-09T18:09:46.598 DEBUG iotfunctions.pipeline.build_job_spec -------------------------------\n",
      "TBD ***** - Add stages for usage stats and write to MessageHub\n",
      "2019-10-09T18:09:46.600 DEBUG iotfunctions.pipeline.exec_payload_method Returned default output for get_early_timestamp() on payload EntityType markus_testdata.  Default value is: None\n",
      "2019-10-09T18:09:46.602 DEBUG iotfunctions.pipeline.get_chunks The payload does not have an get_early_timestamp method or the method did not retrieve an early timestamp. Data will be retrieved in a single chunk\n",
      "2019-10-09T18:09:46.604 DEBUG iotfunctions.pipeline.write Processing as a single chunk\n",
      "2019-10-09T18:09:46.607 DEBUG iotfunctions.pipeline.write Executing stage read_entity_data.\n",
      "2019-10-09T18:09:47.034 DEBUG iotfunctions.metadata.index_df Indexed dataframe on id, evt_timestamp\n",
      "2019-10-09T18:09:47.036 DEBUG iotfunctions.pipeline.exec_stage_method No method get_column_map on read_entity_data returning default None. 'DataReader' object has no attribute 'get_column_map'\n",
      "2019-10-09T18:09:47.037 DEBUG iotfunctions.pipeline.merge_dataframe Merging dataframe with columns [] and index []\n",
      "2019-10-09T18:09:47.038 DEBUG iotfunctions.metadata.index_df Found existing index on id, evt_timestamp.No need to recreate index\n",
      "2019-10-09T18:09:47.040 INFO iotfunctions.pipeline.update_last_entry Trace message:  Completed stage.\n",
      "2019-10-09T18:09:47.041 INFO iotfunctions.pipeline.update_last_entry Trace payload: {'produces_output_items': True, 'output_items': ['deviceid', 'evt_timestamp', 'TestData', 'deviceid', 'deviceid'], 'discard_prior_data': False, 'merge_result': 'existing empty df with new DataFrame', 'usage': 270, 'index': ['id', 'evt_timestamp'], 'can_proceed': True, 'updated': datetime.datetime(2019, 10, 9, 16, 9, 47, 40200), 'cumulative_usage': 270}\n",
      "2019-10-09T18:09:47.042 DEBUG iotfunctions.pipeline.write Executing stage drop_null.\n",
      "2019-10-09T18:09:47.042 DEBUG iotfunctions.pipeline.execute columns excluded when dropping null rows ['deviceid', '_timestamp', 'logicalinterface_id', 'devicetype', 'format', 'updated_utc', 'evt_timestamp']\n",
      "2019-10-09T18:09:47.043 DEBUG iotfunctions.pipeline.execute columns considered when dropping null rows ['eventtype', 'TestData']\n",
      "2019-10-09T18:09:47.044 DEBUG iotfunctions.pipeline.execute eventtype count not null: 54\n",
      "2019-10-09T18:09:47.045 DEBUG iotfunctions.pipeline.execute TestData count not null: 54\n",
      "2019-10-09T18:09:47.053 DEBUG iotfunctions.pipeline.exec_stage_method No method get_column_map on drop_null returning default None. 'DropNull' object has no attribute 'get_column_map'\n",
      "2019-10-09T18:09:47.054 INFO iotfunctions.pipeline.update_last_entry Trace message:  Completed stage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-09T18:09:47.055 INFO iotfunctions.pipeline.update_last_entry Trace payload: {'produces_output_items': False, 'output_items': None, 'discard_prior_data': False, 'new_data_items_info': 'Function is configured not to produce any new data items  during execution', 'can_proceed': True, 'updated': datetime.datetime(2019, 10, 9, 16, 9, 47, 54561), 'cumulative_usage': 270}\n",
      "2019-10-09T18:09:47.056 DEBUG iotfunctions.pipeline.write Executing stage markus_testdata_input_level.\n",
      "2019-10-09T18:09:47.057 DEBUG iotfunctions.pipeline.execute Data items will be written to database for interval (None, 2019-10-09 16:09:45.295410)\n",
      "2019-10-09T18:09:47.058 INFO iotfunctions.pipeline._get_active_cols_properties The column deviceid in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-10-09T18:09:47.059 INFO iotfunctions.pipeline._get_active_cols_properties The column devicetype in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-10-09T18:09:47.060 INFO iotfunctions.pipeline._get_active_cols_properties The column logicalinterface_id in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-10-09T18:09:47.062 INFO iotfunctions.pipeline._get_active_cols_properties The column eventtype in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-10-09T18:09:47.067 INFO iotfunctions.pipeline._get_active_cols_properties The column format in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-10-09T18:09:47.068 INFO iotfunctions.pipeline._get_active_cols_properties The column updated_utc in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-10-09T18:09:47.074 INFO iotfunctions.pipeline._get_active_cols_properties The column TestData in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-10-09T18:09:47.076 INFO iotfunctions.pipeline._get_active_cols_properties The column _timestamp in data frame does not correspond to a data item. Therefore it is not written to the database.\n",
      "2019-10-09T18:09:47.078 INFO iotfunctions.pipeline.execute The following data items will be written to the database: \n",
      "2019-10-09T18:09:47.081 DEBUG iotfunctions.pipeline._get_table_properties Mapping between index name and index position: id -> 0, evt_timestamp -> 1\n",
      "2019-10-09T18:09:47.084 INFO iotfunctions.pipeline.execute The data items will be written into the following tables: \n",
      "2019-10-09T18:09:47.086 WARNING iotfunctions.pipeline.execute There are no data items that have to be written to the database.\n",
      "2019-10-09T18:09:47.087 DEBUG iotfunctions.pipeline.exec_stage_method No method get_column_map on markus_testdata_input_level returning default None. 'SqlAlchemyDataWriter' object has no attribute 'get_column_map'\n",
      "2019-10-09T18:09:47.088 INFO iotfunctions.pipeline.update_last_entry Trace message:  Completed stage.\n",
      "2019-10-09T18:09:47.090 INFO iotfunctions.pipeline.update_last_entry Trace payload: {'produces_output_items': False, 'output_items': None, 'discard_prior_data': False, 'new_data_items_info': 'Function is configured not to produce any new data items  during execution', 'can_proceed': True, 'updated': datetime.datetime(2019, 10, 9, 16, 9, 47, 87988), 'cumulative_usage': 270}\n",
      "2019-10-09T18:09:47.094 WARNING iotfunctions.pipeline.write Error aligning input data to granularity daily\n",
      "2019-10-09T18:09:47.096 DEBUG iotfunctions.pipeline.write Executing stage auto_aggregate.\n",
      "2019-10-09T18:09:47.130 INFO iotfunctions.pipeline.execute Completed aggregation: daily\n",
      "2019-10-09T18:09:47.131 DEBUG iotfunctions.pipeline.exec_stage_method No method get_column_map on auto_aggregate returning default None. 'DataAggregator' object has no attribute 'get_column_map'\n",
      "2019-10-09T18:09:47.132 INFO iotfunctions.pipeline.update_last_entry Trace message:  Completed stage.\n",
      "2019-10-09T18:09:47.133 INFO iotfunctions.pipeline.update_last_entry Trace payload: {'produces_output_items': True, 'output_items': ['TestData_max'], 'discard_prior_data': True, 'merge_result': 'replaced prior data', 'index': [], 'can_proceed': True, 'updated': datetime.datetime(2019, 10, 9, 16, 9, 47, 132509), 'cumulative_usage': 270}\n",
      "2019-10-09T18:09:47.143 DEBUG iotfunctions.pipeline.write Executing stage markus_testdata_daily.\n",
      "2019-10-09T18:09:47.144 DEBUG iotfunctions.pipeline.execute Data items will be written to database for interval (None, 2019-10-09 16:09:45.295410)\n",
      "2019-10-09T18:09:47.145 INFO iotfunctions.pipeline.execute The following data items will be written to the database: TestData_max (dm_markus_testdata_daily, NUMBER)\n",
      "2019-10-09T18:09:47.146 DEBUG iotfunctions.pipeline._get_table_properties Mapping between index name and index position: deviceid -> 0, evt_timestamp -> 1\n",
      "2019-10-09T18:09:48.998 DEBUG iotfunctions.pipeline._get_table_properties For table dm_markus_testdata_daily: delete statement: DELETE FROM dm_markus_testdata_daily, insert statement: INSERT INTO dm_markus_testdata_daily (entity_id, key, value_n, value_b, value_s, value_t, timestamp, last_update) VALUES (:entity_id, :key, :value_n, :value_b, :value_s, :value_t, :timestamp, :last_update)\n",
      "2019-10-09T18:09:48.999 DEBUG iotfunctions.pipeline._get_table_properties For table dm_markus_testdata_daily: Mapping between column name and dataframe index position: ('entity_id', 0), ('timestamp', 1)\n",
      "2019-10-09T18:09:49.000 INFO iotfunctions.pipeline.execute The data items will be written into the following tables: dm_markus_testdata_daily\n",
      "2019-10-09T18:09:49.001 DEBUG iotfunctions.pipeline._delete_old_data Deleting old data items from table dm_markus_testdata_daily for time range [None, 2019-10-09 16:09:45.295410]\n",
      "2019-10-09T18:09:49.428 INFO iotfunctions.pipeline._delete_old_data 9 data items have been deleted from table dm_markus_testdata_daily. Elapsed time in sec: 0.426\n",
      "entity_id 0 73004\n",
      "timestamp 1 2019-10-01 00:00:00\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73004', 'timestamp': Timestamp('2019-10-01 00:00:00', freq='D'), 'value_b': None, 'value_n': 0.953396136664403, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73004\n",
      "timestamp 1 2019-10-08 00:00:00\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73004', 'timestamp': Timestamp('2019-10-08 00:00:00', freq='D'), 'value_b': None, 'value_n': 0.927310831853966, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73004\n",
      "timestamp 1 2019-10-09 00:00:00\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73004', 'timestamp': Timestamp('2019-10-09 00:00:00', freq='D'), 'value_b': None, 'value_n': 0.01851680376576, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73000\n",
      "timestamp 1 2019-10-01 00:00:00\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73000', 'timestamp': Timestamp('2019-10-01 00:00:00', freq='D'), 'value_b': None, 'value_n': 1.52094274992196, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73000\n",
      "timestamp 1 2019-10-09 00:00:00\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73000', 'timestamp': Timestamp('2019-10-09 00:00:00', freq='D'), 'value_b': None, 'value_n': 1.13576418326954, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73002\n",
      "timestamp 1 2019-10-01 00:00:00\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73002', 'timestamp': Timestamp('2019-10-01 00:00:00', freq='D'), 'value_b': None, 'value_n': 2.12616774515587, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73002\n",
      "timestamp 1 2019-10-08 00:00:00\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73002', 'timestamp': Timestamp('2019-10-08 00:00:00', freq='D'), 'value_b': None, 'value_n': 1.19209188983732, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73002\n",
      "timestamp 1 2019-10-09 00:00:00\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73002', 'timestamp': Timestamp('2019-10-09 00:00:00', freq='D'), 'value_b': None, 'value_n': -0.163200762338566, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73003\n",
      "timestamp 1 2019-10-01 00:00:00\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73003', 'timestamp': Timestamp('2019-10-01 00:00:00', freq='D'), 'value_b': None, 'value_n': 1.39164198020907, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73003\n",
      "timestamp 1 2019-10-08 00:00:00\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73003', 'timestamp': Timestamp('2019-10-08 00:00:00', freq='D'), 'value_b': None, 'value_n': 0.0336939599395834, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73003\n",
      "timestamp 1 2019-10-09 00:00:00\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73003', 'timestamp': Timestamp('2019-10-09 00:00:00', freq='D'), 'value_b': None, 'value_n': 1.55894318586124, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73001\n",
      "timestamp 1 2019-10-01 00:00:00\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73001', 'timestamp': Timestamp('2019-10-01 00:00:00', freq='D'), 'value_b': None, 'value_n': 2.1039020307689, 'value_s': None, 'value_t': None})\n",
      "entity_id 0 73001\n",
      "timestamp 1 2019-10-08 00:00:00\n",
      "Persist row: ({'KEY': 'TestData_max', 'entity_id': '73001', 'timestamp': Timestamp('2019-10-08 00:00:00', freq='D'), 'value_b': None, 'value_n': 1.14430471557052, 'value_s': None, 'value_t': None})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-09T18:09:51.558 INFO iotfunctions.pipeline._persist_data Number of data item values persisted so far: 13 (dm_markus_testdata_daily)\n",
      "2019-10-09T18:09:51.559 INFO iotfunctions.pipeline._persist_data Total number of persisted data item values: 13, Elapsed time in sec: 2.130, SqlAlchemy time in sec: 2.123\n",
      "2019-10-09T18:09:51.561 DEBUG iotfunctions.pipeline.exec_stage_method No method get_column_map on markus_testdata_daily returning default None. 'SqlAlchemyDataWriter' object has no attribute 'get_column_map'\n",
      "[]\n",
      "2019-10-09T18:09:51.563 INFO iotfunctions.pipeline.update_last_entry Trace message:  Completed stage.\n",
      "2019-10-09T18:09:51.564 INFO iotfunctions.pipeline.update_last_entry Trace payload: {'produces_output_items': False, 'output_items': None, 'discard_prior_data': False, 'new_data_items_info': 'Function is configured not to produce any new data items  during execution', 'can_proceed': True, 'updated': datetime.datetime(2019, 10, 9, 16, 9, 51, 563062), 'cumulative_usage': 270}\n",
      "2019-10-09T18:09:51.565 INFO iotfunctions.pipeline.write Execution complete\n",
      "2019-10-09T18:09:51.570 DEBUG urllib3.connectionpool._new_conn Starting new HTTPS connection (1): s3-api.us-geo.objectstorage.softlayer.net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/.local/lib/python3.7/site-packages/urllib3/connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-09T18:09:52.683 DEBUG urllib3.connectionpool._make_request https://s3-api.us-geo.objectstorage.softlayer.net:443 \"PUT /analytics-runtime-analyticsservicedev-799d2008b460/AnalyticsServiceDev/markus_testdata/20191009/markus_testdata_trace_160945 HTTP/1.1\" 200 0\n",
      "2019-10-09T18:09:52.686 DEBUG iotfunctions.pipeline.save Saved trace to cos AnalyticsServiceDev/markus_testdata/20191009/markus_testdata_trace_160945\n",
      "2019-10-09T18:09:52.688 DEBUG iotfunctions.pipeline.save wrote trace to file auto_trace_markus_testdata_20191009160945.json\n",
      "2019-10-09T18:09:53.108 DEBUG iotfunctions.pipeline.update Updated job log (markus_testdata,5min): 2019-10-09 16:09:45.295410\n",
      "2019-10-09T18:09:53.111 DEBUG iotfunctions.pipeline.get_next_future_execution Next scheduled execution date is 2019-10-09 16:14:45.295410\n",
      "2019-10-09T18:09:53.111 DEBUG iotfunctions.pipeline.execute Ending job normally as there are no scheduled executions  due before execution end time\n",
      "2019-10-09T18:09:53.138 DEBUG iotfunctions.pipeline.run_auto_save auto_trace_markus_testdata_20191009160945 autosave thread has stopped\n",
      "2019-10-09T18:09:53.141 DEBUG iotfunctions.pipeline.stop Stopping autosave on trace auto_trace_markus_testdata_20191009160945\n"
     ]
    }
   ],
   "source": [
    "# dm_markus_testdate MUST exist, so run the following sql statment in DBeaver\n",
    "\n",
    "#  for db2\n",
    "#CREATE TABLE BLUADMIN.DM_MARKUS_TESTDATA_DAILY (\n",
    "#  entity_id varchar(255),\n",
    "#  key varchar(255),\n",
    "#  value_n double,\n",
    "#  value_b boolean,\n",
    "#  value_s varchar(255),\n",
    "#  value_t timestamp,\n",
    "#  timestamp timestamp,\n",
    "#  last_update timestamp\n",
    "# );\n",
    "\n",
    "#   for postgres\n",
    "#CREATE TABLE public.DM_MARKUS_TESTDATA_DAILY (\n",
    "#  entity_id varchar(255),\n",
    "#  key varchar(255),\n",
    "#  value_n double precision,\n",
    "#  value_b boolean,\n",
    "#  value_s varchar(255),\n",
    "#  value_t timestamp,\n",
    "#  timestamp timestamp,\n",
    "#  last_update timestamp\n",
    "# );\n",
    "\n",
    "from iotfunctions.metadata import Granularity\n",
    "from iotfunctions.pipeline import AggregateItems\n",
    "daily = Granularity(\n",
    "    name = 'daily',\n",
    "    freq = '1D',                 # pandas frequency string\n",
    "    timestamp= 'evt_timestamp',      # build time aggregations using this datetime col\n",
    "    entity_id = 'deviceid',            # aggregate by id\n",
    "    dimensions = None,\n",
    "    entity_name = None\n",
    ")\n",
    "\n",
    "#myAgg = bif.AggregateWithExpression(['TestData'],'x.max()','TestMax')\n",
    "myAgg = AggregateItems(['TestData'], 'max')\n",
    "myAgg.granularity = daily\n",
    "\n",
    "et2._functions = [myAgg]\n",
    "et2.grains = [daily]\n",
    "#et2._granularities_dict['daily'] = daily\n",
    "\n",
    "#jobsettings = {'writer_name' : SqlAlchemyDataWriter, '_db_schema': 'BLUADMIN', 'save_trace_to_file' : True}\n",
    "jobsettings = {'writer_name' : pp.DataWriterSqlAlchemy, '_db_schema': 'public', 'save_trace_to_file' : True}\n",
    "job = pp.JobController(et2, **jobsettings)\n",
    "#job.data_writer = DataWriterFile\n",
    "job.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-01T16:25:51.201 DEBUG iotfunctions.metadata.index_df Indexed dataframe on id, evt_timestamp\n",
      "                                 deviceid       devicetype  \\\n",
      "id    evt_timestamp                                          \n",
      "73004 2019-10-01 13:06:25.341693    73004  markus_testdata   \n",
      "73000 2019-10-01 13:07:25.341693    73000  markus_testdata   \n",
      "      2019-10-01 13:08:25.341693    73000  markus_testdata   \n",
      "73002 2019-10-01 13:09:25.341693    73002  markus_testdata   \n",
      "73004 2019-10-01 13:10:25.341693    73004  markus_testdata   \n",
      "73000 2019-10-01 13:11:25.341693    73000  markus_testdata   \n",
      "73003 2019-10-01 13:12:27.737110    73003  markus_testdata   \n",
      "73004 2019-10-01 13:13:27.737110    73004  markus_testdata   \n",
      "73003 2019-10-01 13:14:27.737110    73003  markus_testdata   \n",
      "      2019-10-01 13:15:27.737110    73003  markus_testdata   \n",
      "73000 2019-10-01 13:16:27.737110    73000  markus_testdata   \n",
      "      2019-10-01 13:17:27.737110    73000  markus_testdata   \n",
      "      2019-10-01 14:03:59.994715    73000  markus_testdata   \n",
      "73001 2019-10-01 14:04:59.994715    73001  markus_testdata   \n",
      "      2019-10-01 14:05:59.994715    73001  markus_testdata   \n",
      "      2019-10-01 14:06:59.994715    73001  markus_testdata   \n",
      "73002 2019-10-01 14:07:59.994715    73002  markus_testdata   \n",
      "73001 2019-10-01 14:08:59.994715    73001  markus_testdata   \n",
      "      2019-10-01 14:07:29.501986    73001  markus_testdata   \n",
      "      2019-10-01 14:08:29.501986    73001  markus_testdata   \n",
      "73002 2019-10-01 14:09:29.501986    73002  markus_testdata   \n",
      "73003 2019-10-01 14:10:29.501986    73003  markus_testdata   \n",
      "73000 2019-10-01 14:11:29.501986    73000  markus_testdata   \n",
      "73003 2019-10-01 14:12:29.501986    73003  markus_testdata   \n",
      "73002 2019-10-01 14:11:56.729650    73002  markus_testdata   \n",
      "73001 2019-10-01 14:12:56.729650    73001  markus_testdata   \n",
      "73004 2019-10-01 14:13:56.729650    73004  markus_testdata   \n",
      "73002 2019-10-01 14:14:56.729650    73002  markus_testdata   \n",
      "73004 2019-10-01 14:15:56.729650    73004  markus_testdata   \n",
      "      2019-10-01 14:16:56.729650    73004  markus_testdata   \n",
      "73003 2019-10-01 14:13:53.168118    73003  markus_testdata   \n",
      "73004 2019-10-01 14:14:53.168118    73004  markus_testdata   \n",
      "73003 2019-10-01 14:15:53.168118    73003  markus_testdata   \n",
      "73004 2019-10-01 14:16:53.168118    73004  markus_testdata   \n",
      "      2019-10-01 14:17:53.168118    73004  markus_testdata   \n",
      "73002 2019-10-01 14:18:53.168118    73002  markus_testdata   \n",
      "\n",
      "                                 logicalinterface_id eventtype format  \\\n",
      "id    evt_timestamp                                                     \n",
      "73004 2019-10-01 13:06:25.341693                            en          \n",
      "73000 2019-10-01 13:07:25.341693                            en          \n",
      "      2019-10-01 13:08:25.341693                            yt          \n",
      "73002 2019-10-01 13:09:25.341693                            tt          \n",
      "73004 2019-10-01 13:10:25.341693                            ne          \n",
      "73000 2019-10-01 13:11:25.341693                            tt          \n",
      "73003 2019-10-01 13:12:27.737110                            ep          \n",
      "73004 2019-10-01 13:13:27.737110                            te          \n",
      "73003 2019-10-01 13:14:27.737110                            en          \n",
      "      2019-10-01 13:15:27.737110                            vt          \n",
      "73000 2019-10-01 13:16:27.737110                            et          \n",
      "      2019-10-01 13:17:27.737110                            ty          \n",
      "      2019-10-01 14:03:59.994715                            ey          \n",
      "73001 2019-10-01 14:04:59.994715                            et          \n",
      "      2019-10-01 14:05:59.994715                            et          \n",
      "      2019-10-01 14:06:59.994715                            tt          \n",
      "73002 2019-10-01 14:07:59.994715                            ye          \n",
      "73001 2019-10-01 14:08:59.994715                            ey          \n",
      "      2019-10-01 14:07:29.501986                            en          \n",
      "      2019-10-01 14:08:29.501986                            en          \n",
      "73002 2019-10-01 14:09:29.501986                            et          \n",
      "73003 2019-10-01 14:10:29.501986                            tn          \n",
      "73000 2019-10-01 14:11:29.501986                            et          \n",
      "73003 2019-10-01 14:12:29.501986                            ye          \n",
      "73002 2019-10-01 14:11:56.729650                            ny          \n",
      "73001 2019-10-01 14:12:56.729650                            ne          \n",
      "73004 2019-10-01 14:13:56.729650                            ee          \n",
      "73002 2019-10-01 14:14:56.729650                            tt          \n",
      "73004 2019-10-01 14:15:56.729650                            ny          \n",
      "      2019-10-01 14:16:56.729650                            pt          \n",
      "73003 2019-10-01 14:13:53.168118                            te          \n",
      "73004 2019-10-01 14:14:53.168118                            en          \n",
      "73003 2019-10-01 14:15:53.168118                            en          \n",
      "73004 2019-10-01 14:16:53.168118                            ee          \n",
      "      2019-10-01 14:17:53.168118                            nv          \n",
      "73002 2019-10-01 14:18:53.168118                            pe          \n",
      "\n",
      "                                 updated_utc  TestData  \\\n",
      "id    evt_timestamp                                      \n",
      "73004 2019-10-01 13:06:25.341693        None -1.012887   \n",
      "73000 2019-10-01 13:07:25.341693        None  0.640831   \n",
      "      2019-10-01 13:08:25.341693        None  0.657694   \n",
      "73002 2019-10-01 13:09:25.341693        None -2.390681   \n",
      "73004 2019-10-01 13:10:25.341693        None -0.234181   \n",
      "73000 2019-10-01 13:11:25.341693        None  0.952477   \n",
      "73003 2019-10-01 13:12:27.737110        None -0.107018   \n",
      "73004 2019-10-01 13:13:27.737110        None -0.203514   \n",
      "73003 2019-10-01 13:14:27.737110        None  0.720092   \n",
      "      2019-10-01 13:15:27.737110        None -1.720990   \n",
      "73000 2019-10-01 13:16:27.737110        None  1.520943   \n",
      "      2019-10-01 13:17:27.737110        None  0.097358   \n",
      "      2019-10-01 14:03:59.994715        None  0.521085   \n",
      "73001 2019-10-01 14:04:59.994715        None  1.059213   \n",
      "      2019-10-01 14:05:59.994715        None  0.064902   \n",
      "      2019-10-01 14:06:59.994715        None  0.762488   \n",
      "73002 2019-10-01 14:07:59.994715        None -0.715103   \n",
      "73001 2019-10-01 14:08:59.994715        None  2.103902   \n",
      "      2019-10-01 14:07:29.501986        None  0.124766   \n",
      "      2019-10-01 14:08:29.501986        None  0.014392   \n",
      "73002 2019-10-01 14:09:29.501986        None  0.359201   \n",
      "73003 2019-10-01 14:10:29.501986        None  1.391642   \n",
      "73000 2019-10-01 14:11:29.501986        None  0.431859   \n",
      "73003 2019-10-01 14:12:29.501986        None  0.889539   \n",
      "73002 2019-10-01 14:11:56.729650        None -0.150771   \n",
      "73001 2019-10-01 14:12:56.729650        None  1.145375   \n",
      "73004 2019-10-01 14:13:56.729650        None  0.144240   \n",
      "73002 2019-10-01 14:14:56.729650        None  2.126168   \n",
      "73004 2019-10-01 14:15:56.729650        None -0.211232   \n",
      "      2019-10-01 14:16:56.729650        None  0.753428   \n",
      "73003 2019-10-01 14:13:53.168118        None  0.392042   \n",
      "73004 2019-10-01 14:14:53.168118        None -0.288730   \n",
      "73003 2019-10-01 14:15:53.168118        None  1.062992   \n",
      "73004 2019-10-01 14:16:53.168118        None  0.635863   \n",
      "      2019-10-01 14:17:53.168118        None  0.953396   \n",
      "73002 2019-10-01 14:18:53.168118        None -0.189876   \n",
      "\n",
      "                                                 _timestamp  \n",
      "id    evt_timestamp                                          \n",
      "73004 2019-10-01 13:06:25.341693 2019-10-01 13:06:25.341693  \n",
      "73000 2019-10-01 13:07:25.341693 2019-10-01 13:07:25.341693  \n",
      "      2019-10-01 13:08:25.341693 2019-10-01 13:08:25.341693  \n",
      "73002 2019-10-01 13:09:25.341693 2019-10-01 13:09:25.341693  \n",
      "73004 2019-10-01 13:10:25.341693 2019-10-01 13:10:25.341693  \n",
      "73000 2019-10-01 13:11:25.341693 2019-10-01 13:11:25.341693  \n",
      "73003 2019-10-01 13:12:27.737110 2019-10-01 13:12:27.737110  \n",
      "73004 2019-10-01 13:13:27.737110 2019-10-01 13:13:27.737110  \n",
      "73003 2019-10-01 13:14:27.737110 2019-10-01 13:14:27.737110  \n",
      "      2019-10-01 13:15:27.737110 2019-10-01 13:15:27.737110  \n",
      "73000 2019-10-01 13:16:27.737110 2019-10-01 13:16:27.737110  \n",
      "      2019-10-01 13:17:27.737110 2019-10-01 13:17:27.737110  \n",
      "      2019-10-01 14:03:59.994715 2019-10-01 14:03:59.994715  \n",
      "73001 2019-10-01 14:04:59.994715 2019-10-01 14:04:59.994715  \n",
      "      2019-10-01 14:05:59.994715 2019-10-01 14:05:59.994715  \n",
      "      2019-10-01 14:06:59.994715 2019-10-01 14:06:59.994715  \n",
      "73002 2019-10-01 14:07:59.994715 2019-10-01 14:07:59.994715  \n",
      "73001 2019-10-01 14:08:59.994715 2019-10-01 14:08:59.994715  \n",
      "      2019-10-01 14:07:29.501986 2019-10-01 14:07:29.501986  \n",
      "      2019-10-01 14:08:29.501986 2019-10-01 14:08:29.501986  \n",
      "73002 2019-10-01 14:09:29.501986 2019-10-01 14:09:29.501986  \n",
      "73003 2019-10-01 14:10:29.501986 2019-10-01 14:10:29.501986  \n",
      "73000 2019-10-01 14:11:29.501986 2019-10-01 14:11:29.501986  \n",
      "73003 2019-10-01 14:12:29.501986 2019-10-01 14:12:29.501986  \n",
      "73002 2019-10-01 14:11:56.729650 2019-10-01 14:11:56.729650  \n",
      "73001 2019-10-01 14:12:56.729650 2019-10-01 14:12:56.729650  \n",
      "73004 2019-10-01 14:13:56.729650 2019-10-01 14:13:56.729650  \n",
      "73002 2019-10-01 14:14:56.729650 2019-10-01 14:14:56.729650  \n",
      "73004 2019-10-01 14:15:56.729650 2019-10-01 14:15:56.729650  \n",
      "      2019-10-01 14:16:56.729650 2019-10-01 14:16:56.729650  \n",
      "73003 2019-10-01 14:13:53.168118 2019-10-01 14:13:53.168118  \n",
      "73004 2019-10-01 14:14:53.168118 2019-10-01 14:14:53.168118  \n",
      "73003 2019-10-01 14:15:53.168118 2019-10-01 14:15:53.168118  \n",
      "73004 2019-10-01 14:16:53.168118 2019-10-01 14:16:53.168118  \n",
      "      2019-10-01 14:17:53.168118 2019-10-01 14:17:53.168118  \n",
      "73002 2019-10-01 14:18:53.168118 2019-10-01 14:18:53.168118  \n"
     ]
    }
   ],
   "source": [
    "print (et2.get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
